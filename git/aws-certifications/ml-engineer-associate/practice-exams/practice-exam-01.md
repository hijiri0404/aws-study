# MLE-A æƒ³å®šå•é¡Œé›† 01 - ãƒ‡ãƒ¼ã‚¿æº–å‚™ã¨Feature Engineering

## ğŸ“‹ è©¦é¨“æƒ…å ±

**å•é¡Œæ•°**: 100å•  
**åˆ¶é™æ™‚é–“**: 180åˆ†  
**åˆæ ¼ç‚¹**: 72/100 (72%)  
**ã‚«ãƒãƒ¼ç¯„å›²**: å…¨4ãƒ‰ãƒ¡ã‚¤ãƒ³

---

## ğŸ”§ å•é¡Œ 1
ã‚ãªãŸã¯ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å°å£²æ¥­ã®æ©Ÿæ¢°å­¦ç¿’ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã§ã™ã€‚é¡§å®¢ã®è³¼è²·å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦å•†å“æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã—ã¦ã„ã¾ã™ã€‚é¡§å®¢ã®è³¼è²·ãƒ‡ãƒ¼ã‚¿ã«ã¯ä»¥ä¸‹ã®ç‰¹å¾´ãŒã‚ã‚Šã¾ã™ï¼š

- ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: 10TB
- æ›´æ–°é »åº¦: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ 
- ç‰¹å¾´é‡æ•°: 500+
- äºˆæ¸¬ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·è¦ä»¶: 100msä»¥ä¸‹

ã“ã®ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ã«æœ€é©ãªAWSã‚µãƒ¼ãƒ“ã‚¹ã®çµ„ã¿åˆã‚ã›ã¯ï¼Ÿ

**A)** Amazon S3 + Amazon Athena + SageMaker Batch Transform  
**B)** Amazon SageMaker Feature Store + SageMaker Real-time Inference  
**C)** Amazon Redshift + AWS Lambda + API Gateway  
**D)** Amazon DynamoDB + Amazon Kinesis + SageMaker Multi-Model Endpoints

<details>
<summary>è§£ç­”ã¨è§£èª¬</summary>

**æ­£è§£: B**

**è§£èª¬:**
- **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ›´æ–°**: Feature Store ã®ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã‚¹ãƒˆã‚¢ãŒãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ›´æ–°ã‚’ã‚µãƒãƒ¼ãƒˆ
- **å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿**: Feature Store ã¯å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã®åŠ¹ç‡çš„ãªç®¡ç†ãŒå¯èƒ½
- **ä½ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·**: ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã‚¹ãƒˆã‚¢ + ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¨è«–ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã§100msè¦ä»¶ã‚’æº€ãŸã›ã‚‹
- **å¤šæ•°ã®ç‰¹å¾´é‡**: Feature Store ã¯æ•°ç™¾ã®ç‰¹å¾´é‡ã‚’åŠ¹ç‡çš„ã«ç®¡ç†

**ä»–ã®é¸æŠè‚¢ãŒä¸é©åˆ‡ãªç†ç”±:**
- A: Batch Transform ã¯ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¨è«–ã«ä¸é©åˆ‡
- C: Redshift ã¯åˆ†æç”¨é€”ã§ã€100ms ã®ä½ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·è¦ä»¶ã«ä¸å‘ã
- D: DynamoDB ã¯ç‰¹å¾´é‡ã‚¹ãƒˆã‚¢ã¨ã—ã¦ã®æœ€é©åŒ–ãŒä¸ååˆ†
</details>

---

## ğŸ”§ å•é¡Œ 2
SageMaker Data Wrangler ã‚’ä½¿ç”¨ã—ã¦ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã‚’è¡Œã£ã¦ã„ã¾ã™ã€‚ä»¥ä¸‹ã®å‰å‡¦ç†ã‚¹ãƒ†ãƒƒãƒ—ã®ã†ã¡ã€Data Wrangler ã§ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ã‚‚ã®ã¯ï¼Ÿ

**A)** ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã®One-hot encoding  
**B)** æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®å·®åˆ†è¨ˆç®—  
**C)** ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†  
**D)** å¤–ã‚Œå€¤ã®æ¤œå‡ºã¨å‰Šé™¤

<details>
<summary>è§£ç­”ã¨è§£èª¬</summary>

**æ­£è§£: C**

**è§£èª¬:**
Data Wrangler ã¯**ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿å‡¦ç†**ã«ç‰¹åŒ–ã—ãŸã‚µãƒ¼ãƒ“ã‚¹ã§ã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†ã¯ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã›ã‚“ã€‚

**Data Wrangler ãŒã‚µãƒãƒ¼ãƒˆã™ã‚‹æ©Ÿèƒ½:**
- 300+ ã®çµ„ã¿è¾¼ã¿å¤‰æ›
- ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆOne-hot, Label encodingç­‰ï¼‰
- æ™‚ç³»åˆ—å¤‰æ›ï¼ˆãƒ©ã‚°ã€å·®åˆ†ã€ç§»å‹•å¹³å‡ç­‰ï¼‰
- å¤–ã‚Œå€¤æ¤œå‡ºã¨å‡¦ç†
- ãƒ‡ãƒ¼ã‚¿ã®å¯è¦–åŒ–ã¨å“è³ªãƒã‚§ãƒƒã‚¯

**ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†ã«ã¯:**
- Amazon Kinesis Analytics
- AWS Glue Streaming
- Amazon EMR with Spark Streaming
ã‚’ä½¿ç”¨ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
</details>

---

## ğŸ”§ å•é¡Œ 3
Feature Store ã§ç‰¹å¾´é‡ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ä½œæˆã™ã‚‹éš›ã®å¿…é ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ï¼Ÿ

**A)** `FeatureGroupName` ã®ã¿  
**B)** `FeatureGroupName`, `RecordIdentifierFeatureName`, `EventTimeFeatureName`  
**C)** `FeatureGroupName`, `S3Uri`, `RoleArn`  
**D)** `FeatureGroupName`, `OnlineStoreConfig`, `OfflineStoreConfig`

<details>
<summary>è§£ç­”ã¨è§£èª¬</summary>

**æ­£è§£: B**

**è§£èª¬:**
Feature Group ä½œæˆã®**å¿…é ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**ã¯ï¼š

1. **FeatureGroupName**: ç‰¹å¾´é‡ã‚°ãƒ«ãƒ¼ãƒ—ã®åå‰
2. **RecordIdentifierFeatureName**: ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ä¸€æ„ã«è­˜åˆ¥ã™ã‚‹ç‰¹å¾´é‡å
3. **EventTimeFeatureName**: ã‚¤ãƒ™ãƒ³ãƒˆç™ºç”Ÿæ™‚åˆ»ã‚’ç¤ºã™ç‰¹å¾´é‡å

```python
feature_group.create(
    s3_uri=s3_uri,
    record_identifier_name='customer_id',  # å¿…é ˆ
    event_time_feature_name='event_time',  # å¿…é ˆ
    role_arn=role,
    feature_definitions=feature_definitions
)
```

**ãã®ä»–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**:
- `S3Uri`, `RoleArn`: é€šå¸¸å¿…è¦ã ãŒã€ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‹ã‚‰æ¨è«–å¯èƒ½
- `OnlineStoreConfig`, `OfflineStoreConfig`: ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ä½¿ç”¨å¯èƒ½ï¼‰
</details>

---

## ğŸ”§ å•é¡Œ 4
å¤§è¦æ¨¡ãªCSVãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ100GBï¼‰ã‚’SageMaker Processing ã§åŠ¹ç‡çš„ã«å‡¦ç†ã—ãŸã„å ´åˆã®æ¨å¥¨ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯ï¼Ÿ

**A)** å˜ä¸€ã®ml.m5.24xlarge ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½¿ç”¨  
**B)** è¤‡æ•°ã®ml.m5.large ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†å‰²å‡¦ç†  
**C)** SageMaker Batch Transform ã‚’ä½¿ç”¨  
**D)** AWS Lambda ã§ä¸¦åˆ—å‡¦ç†

<details>
<summary>è§£ç­”ã¨è§£èª¬</summary>

**æ­£è§£: B**

**è§£èª¬:**
å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã®åŠ¹ç‡çš„ãªå‡¦ç†ã«ã¯**æ°´å¹³ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°**ãŒæ¨å¥¨ã•ã‚Œã¾ã™ã€‚

**æœ€é©ãªæ§‹æˆ:**
```python
sklearn_processor = SKLearnProcessor(
    framework_version='1.0-1',
    instance_type='ml.m5.large',
    instance_count=10,  # è¤‡æ•°ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
    volume_size_in_gb=100
)
```

**åˆ©ç‚¹:**
- ä¸¦åˆ—å‡¦ç†ã«ã‚ˆã‚‹é«˜é€ŸåŒ–
- ã‚³ã‚¹ãƒˆåŠ¹ç‡ï¼ˆå°ã•ã„ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ Ã— è¤‡æ•° < å¤§ãã„ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ Ã— 1ï¼‰
- éšœå®³è€æ€§ã®å‘ä¸Š
- æŸ”è»Ÿãªã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°

**ä»–ã®é¸æŠè‚¢ãŒä¸é©åˆ‡ãªç†ç”±:**
- A: å˜ä¸€ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã¯ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã«ãªã‚Šã‚„ã™ã„
- C: Batch Transform ã¯æ¨è«–ç”¨ã€å‰å‡¦ç†ã«ã¯ä¸é©åˆ‡
- D: Lambda ã®å®Ÿè¡Œæ™‚é–“åˆ¶é™ï¼ˆ15åˆ†ï¼‰ã«ã‚ˆã‚Šå¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã«ä¸å‘ã
</details>

---

## ğŸ”§ å•é¡Œ 5
æ™‚ç³»åˆ—äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã®ãŸã‚ã«ã€éå»30æ—¥é–“ã®å£²ä¸Šãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç‰¹å¾´é‡ã‚’ä½œæˆã—ã¦ã„ã¾ã™ã€‚ä»¥ä¸‹ã®ã†ã¡ã€ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã‚’é¿ã‘ã‚‹ãŸã‚ã«æœ€ã‚‚é‡è¦ãªè€ƒæ…®äº‹é …ã¯ï¼Ÿ

**A)** ç‰¹å¾´é‡ã®æ­£è¦åŒ–ã‚’è¡Œã†  
**B)** å°†æ¥ã®æƒ…å ±ã‚’å«ã‚€ç‰¹å¾´é‡ã‚’é™¤å¤–ã™ã‚‹  
**C)** æ¬ æå€¤ã‚’é©åˆ‡ã«å‡¦ç†ã™ã‚‹  
**D)** ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã™ã‚‹

<details>
<summary>è§£ç­”ã¨è§£èª¬</summary>

**æ­£è§£: B**

**è§£èª¬:**
**ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ï¼ˆData Leakageï¼‰**ã¯æ©Ÿæ¢°å­¦ç¿’ã§æœ€ã‚‚é‡è¦ãªå•é¡Œã®ä¸€ã¤ã§ã™ã€‚

**æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã§ã®ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ä¾‹:**
- äºˆæ¸¬æ™‚ç‚¹ã‚ˆã‚Šå¾Œã®æƒ…å ±ã‚’ç‰¹å¾´é‡ã«å«ã‚ã‚‹
- é›†è¨ˆæœŸé–“ãŒäºˆæ¸¬å¯¾è±¡æœŸé–“ã¨é‡è¤‡
- æœªæ¥ã®çµ±è¨ˆé‡ï¼ˆæœªæ¥ã®å¹³å‡å€¤ç­‰ï¼‰ã‚’ä½¿ç”¨

**äºˆé˜²ç­–:**
```python
# âŒ æ‚ªã„ä¾‹: å°†æ¥ã®æƒ…å ±ã‚’å«ã‚€
future_sales_avg = df['sales'].rolling(window=7).mean().shift(-3)  # 3æ—¥å¾Œã®æƒ…å ±

# âœ… è‰¯ã„ä¾‹: éå»ã®æƒ…å ±ã®ã¿
past_sales_avg = df['sales'].rolling(window=7).mean().shift(1)    # 1æ—¥å‰ã¾ã§ã®æƒ…å ±
```

**æ™‚ç³»åˆ—åˆ†å‰²ã§ã®æ³¨æ„ç‚¹:**
```python
# âŒ ãƒ©ãƒ³ãƒ€ãƒ åˆ†å‰²ï¼ˆæ™‚ç³»åˆ—ã§ã¯ä¸é©åˆ‡ï¼‰
train_test_split(X, y, test_size=0.2, random_state=42)

# âœ… æ™‚ç³»åˆ—åˆ†å‰²
train_data = data[data['date'] < '2024-01-01']
test_data = data[data['date'] >= '2024-01-01']
```
</details>

---

## ğŸ”§ å•é¡Œ 6
SageMaker Feature Store ã§ã€ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã‚¹ãƒˆã‚¢ã¨ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ã‚¹ãƒˆã‚¢ã®é©åˆ‡ãªä½¿ã„åˆ†ã‘ã¯ï¼Ÿ

**A)** ã‚ªãƒ³ãƒ©ã‚¤ãƒ³: ãƒãƒƒãƒäºˆæ¸¬ã€ã‚ªãƒ•ãƒ©ã‚¤ãƒ³: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ äºˆæ¸¬  
**B)** ã‚ªãƒ³ãƒ©ã‚¤ãƒ³: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ äºˆæ¸¬ã€ã‚ªãƒ•ãƒ©ã‚¤ãƒ³: ãƒ¢ãƒ‡ãƒ«è¨“ç·´ãƒ»ãƒãƒƒãƒäºˆæ¸¬  
**C)** ã‚ªãƒ³ãƒ©ã‚¤ãƒ³: ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã€ã‚ªãƒ•ãƒ©ã‚¤ãƒ³: ãƒ‡ãƒ¼ã‚¿åˆ†æ  
**D)** ä¸¡æ–¹ã¨ã‚‚åŒã˜ç”¨é€”ã§ä½¿ç”¨å¯èƒ½

<details>
<summary>è§£ç­”ã¨è§£èª¬</summary>

**æ­£è§£: B**

**è§£èª¬:**

**ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã‚¹ãƒˆã‚¢:**
- **ç”¨é€”**: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ äºˆæ¸¬ã€ä½ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·æ¤œç´¢
- **ç‰¹å¾´**: ã‚¤ãƒ³ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã€ãƒŸãƒªç§’ãƒ¬ãƒ™ãƒ«ã®å¿œç­”
- **åˆ¶é™**: é™å®šçš„ãªã‚¯ã‚¨ãƒªæ©Ÿèƒ½ã€é«˜ã„ã‚³ã‚¹ãƒˆ

**ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ã‚¹ãƒˆã‚¢:**
- **ç”¨é€”**: ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã€ãƒãƒƒãƒäºˆæ¸¬ã€ãƒ‡ãƒ¼ã‚¿åˆ†æ
- **ç‰¹å¾´**: S3ãƒ™ãƒ¼ã‚¹ã€Athenaã§è¤‡é›‘ãªã‚¯ã‚¨ãƒªå¯èƒ½
- **åˆ©ç‚¹**: å¤§å®¹é‡ã€ä½ã‚³ã‚¹ãƒˆã€é•·æœŸä¿å­˜

```python
# ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã‚¹ãƒˆã‚¢ä½¿ç”¨ä¾‹ï¼ˆãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ äºˆæ¸¬ï¼‰
online_response = feature_group.get_record(
    record_identifier_value_as_string='customer_123'
)

# ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ã‚¹ãƒˆã‚¢ä½¿ç”¨ä¾‹ï¼ˆåˆ†æãƒ»è¨“ç·´ï¼‰
query = """
SELECT customer_id, age, income, churn_probability
FROM my_feature_group
WHERE event_time >= '2024-01-01'
"""
training_data = feature_store.athena_query(query)
```

**å®Ÿè£…ã«ãŠã‘ã‚‹æ³¨æ„ç‚¹:**
- åŒã˜ãƒ‡ãƒ¼ã‚¿ãŒä¸¡æ–¹ã«æ ¼ç´ã•ã‚Œã‚‹
- ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã‚¹ãƒˆã‚¢ã¯æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã®ã¿ï¼ˆTTLè¨­å®šå¯èƒ½ï¼‰
- ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ã‚¹ãƒˆã‚¢ã¯å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã™ã¹ã¦ä¿æŒ
</details>

---

## ğŸ”§ å•é¡Œ 7
ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆé™½æ€§ã‚¯ãƒ©ã‚¹2%ã€é™°æ€§ã‚¯ãƒ©ã‚¹98%ï¼‰ã‚’æ‰±ã†éš›ã®é©åˆ‡ãªå‰å‡¦ç†æ‰‹æ³•ã¯ï¼Ÿ

**A)** é™°æ€§ã‚¯ãƒ©ã‚¹ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«å‰Šé™¤ã—ã¦ãƒãƒ©ãƒ³ã‚¹ã‚’å–ã‚‹  
**B)** SMOTEï¼ˆSynthetic Minority Over-sampling Techniqueï¼‰ã‚’é©ç”¨  
**C)** é‡ã¿ä»˜ãæå¤±é–¢æ•°ã‚’ä½¿ç”¨ã—ã€ãƒ‡ãƒ¼ã‚¿ã¯ãã®ã¾ã¾ä¿æŒ  
**D)** B ã¨ C ã®ä¸¡æ–¹ã‚’æ¤œè¨ã—ã€æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã§æ€§èƒ½æ¯”è¼ƒ

<details>
<summary>è§£ç­”ã¨è§£èª¬</summary>

**æ­£è§£: D**

**è§£èª¬:**
ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿å¯¾ç­–ã«ã¯è¤‡æ•°ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒã‚ã‚Šã€**ãƒ‡ãƒ¼ã‚¿ã¨ãƒ“ã‚¸ãƒã‚¹è¦ä»¶ã«å¿œã˜ã¦æœ€é©è§£ãŒç•°ãªã‚‹**ãŸã‚ã€è¤‡æ•°æ‰‹æ³•ã®æ¯”è¼ƒæ¤œè¨ãŒé‡è¦ã§ã™ã€‚

**ä¸»è¦ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒ:**

1. **ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ‰‹æ³•:**
   ```python
   from imblearn.over_sampling import SMOTE
   smote = SMOTE(random_state=42)
   X_resampled, y_resampled = smote.fit_resample(X, y)
   ```

2. **é‡ã¿ä»˜ãå­¦ç¿’:**
   ```python
   from sklearn.ensemble import RandomForestClassifier
   clf = RandomForestClassifier(class_weight='balanced')
   ```

3. **é–¾å€¤èª¿æ•´:**
   ```python
   from sklearn.metrics import precision_recall_curve
   precision, recall, thresholds = precision_recall_curve(y_true, y_scores)
   optimal_threshold = thresholds[np.argmax(precision + recall)]
   ```

**è©•ä¾¡æŒ‡æ¨™ã®é¸æŠ:**
- Accuracy ã¯ä¸é©åˆ‡ï¼ˆ98%ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼‰
- Precision, Recall, F1-score, AUC-PR ã‚’ä½¿ç”¨

**SageMaker ã§ã®å®Ÿè£…ä¾‹:**
```python
# AutoMLã§ã®ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿å‡¦ç†
automl_job = sagemaker.AutoML(
    target_attribute_name='target',
    problem_type='BinaryClassification',
    objective={'MetricName': 'F1'}  # ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿ç”¨ã®æŒ‡æ¨™
)
```
</details>

---

## ğŸ”§ å•é¡Œ 8
SageMaker Processing ã§å¤§é‡ã®ç”»åƒãƒ‡ãƒ¼ã‚¿ï¼ˆ10ä¸‡æšã€å„5MBï¼‰ã‚’å‰å‡¦ç†ã™ã‚‹éš›ã®æ¨å¥¨æ§‹æˆã¯ï¼Ÿ

**A)** CPUæœ€é©åŒ–ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆml.c5.xlargeï¼‰ã‚’è¤‡æ•°ä½¿ç”¨  
**B)** GPUæ­è¼‰ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆml.p3.2xlargeï¼‰ã‚’å˜ä¸€ä½¿ç”¨  
**C)** ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆml.r5.largeï¼‰ã‚’è¤‡æ•°ä½¿ç”¨  
**D)** GPUæ­è¼‰ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆml.p3.xlargeï¼‰ã‚’è¤‡æ•°ä½¿ç”¨

<details>
<summary>è§£ç­”ã¨è§£èª¬</summary>

**æ­£è§£: D**

**è§£èª¬:**
ç”»åƒå‰å‡¦ç†ã®ã‚ˆã†ãª**è¨ˆç®—é›†ç´„çš„ã‚¿ã‚¹ã‚¯**ã«ã¯**GPUä¸¦åˆ—å‡¦ç†**ãŒæœ€é©ã§ã™ã€‚

**ãƒ‡ãƒ¼ã‚¿é‡ã®è¨ˆç®—:**
- 10ä¸‡æš Ã— 5MB = 500GB
- ä¸¦åˆ—å‡¦ç†ãŒå¿…é ˆ

**æ¨å¥¨æ§‹æˆã®ç†ç”±:**
1. **GPUæ´»ç”¨**: ç”»åƒå‡¦ç†ï¼ˆãƒªã‚µã‚¤ã‚ºã€æ­£è¦åŒ–ã€æ‹¡å¼µï¼‰ã¯GPUã§é«˜é€ŸåŒ–
2. **è¤‡æ•°ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹**: ãƒ‡ãƒ¼ã‚¿åˆ†æ•£å‡¦ç†ã§æ™‚é–“çŸ­ç¸®
3. **é©åˆ‡ãªGPUã‚µã‚¤ã‚º**: p3.xlarge ã¯ 1GPUã€ã‚³ã‚¹ãƒˆåŠ¹ç‡ãŒè‰¯ã„

```python
# ç”»åƒå‡¦ç†ç”¨ã®Processorã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
image_processor = PyTorchProcessor(
    framework_version='1.12',
    py_version='py38',
    instance_type='ml.p3.xlarge',
    instance_count=4,  # 4ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã§ä¸¦åˆ—å‡¦ç†
    volume_size_in_gb=200,
    role=role
)

# å‡¦ç†æ™‚é–“ã®æ¨å®š
# CPU: ~20æ™‚é–“, GPU(å˜ä¸€): ~4æ™‚é–“, GPU(ä¸¦åˆ—): ~1æ™‚é–“
```

**ä»–ã®é¸æŠè‚¢ã®å•é¡Œ:**
- A: CPUå‡¦ç†ã¯ç”»åƒå‡¦ç†ã«éåŠ¹ç‡
- B: å˜ä¸€ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã§ã¯ãƒœãƒˆãƒ«ãƒãƒƒã‚¯
- C: ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ã¯ä¸è¦ã€GPUå‡¦ç†èƒ½åŠ›ãŒé‡è¦
</details>

---

## ğŸ”§ å•é¡Œ 9
ä»¥ä¸‹ã®Pythonã‚³ãƒ¼ãƒ‰ã§SageMaker Feature Storeã‹ã‚‰ç‰¹å¾´é‡ã‚’å–å¾—ã—ã¦ã„ã¾ã™ã€‚ã“ã®å®Ÿè£…ã®å•é¡Œç‚¹ã¯ï¼Ÿ

```python
def get_customer_features(customer_ids):
    features = []
    for customer_id in customer_ids:
        response = feature_group.get_record(
            record_identifier_value_as_string=str(customer_id)
        )
        features.append(response.record)
    return features
```

**A)** ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãŒãªã„  
**B)** ãƒ‡ãƒ¼ã‚¿å‹å¤‰æ›ãŒä¸é©åˆ‡  
**C)** ãƒãƒƒãƒå‡¦ç†ã‚’ä½¿ç”¨ã—ã¦ã„ãªã„  
**D)** ä¸Šè¨˜ã™ã¹ã¦

<details>
<summary>è§£ç­”ã¨è§£èª¬</summary>

**æ­£è§£: D**

**è§£èª¬:**
æä¾›ã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰ã«ã¯è¤‡æ•°ã®å•é¡ŒãŒã‚ã‚Šã¾ã™ã€‚

**å•é¡Œç‚¹ã¨æ”¹å–„æ¡ˆ:**

1. **ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä¸è¶³:**
   ```python
   try:
       response = feature_group.get_record(...)
   except ClientError as e:
       if e.response['Error']['Code'] == 'ResourceNotFound':
           # ãƒ¬ã‚³ãƒ¼ãƒ‰ãŒå­˜åœ¨ã—ãªã„å ´åˆã®å‡¦ç†
           return None
       else:
           raise e
   ```

2. **éåŠ¹ç‡ãªé€æ¬¡å‡¦ç†:**
   ```python
   # âŒ æ‚ªã„ä¾‹: ãƒ«ãƒ¼ãƒ—ã§é€æ¬¡å‡¦ç†
   for customer_id in customer_ids:
       response = feature_group.get_record(...)
   
   # âœ… æ”¹å–„æ¡ˆ: ãƒãƒƒãƒå‡¦ç†
   from concurrent.futures import ThreadPoolExecutor
   
   def get_batch_features(customer_ids, batch_size=10):
       with ThreadPoolExecutor(max_workers=batch_size) as executor:
           futures = [executor.submit(get_single_record, cid) 
                     for cid in customer_ids]
           return [f.result() for f in futures]
   ```

3. **ãƒ‡ãƒ¼ã‚¿å‹å¤‰æ›ã®å•é¡Œ:**
   ```python
   # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®é©åˆ‡ãªå‡¦ç†
   def parse_feature_record(record):
       features = {}
       for feature in record:
           feature_name = feature.feature_name
           value = feature.value_as_string
           # é©åˆ‡ãªãƒ‡ãƒ¼ã‚¿å‹ã«å¤‰æ›
           if feature_name in numeric_features:
               features[feature_name] = float(value)
           else:
               features[feature_name] = value
       return features
   ```

**æœ€é©åŒ–ã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰ä¾‹:**
```python
import asyncio
from concurrent.futures import ThreadPoolExecutor
import logging

async def get_customer_features_optimized(customer_ids, max_workers=10):
    async def get_single_feature(customer_id):
        try:
            response = await feature_group.get_record(
                record_identifier_value_as_string=str(customer_id)
            )
            return parse_feature_record(response.record)
        except Exception as e:
            logging.warning(f"Failed to get features for {customer_id}: {e}")
            return None
    
    tasks = [get_single_feature(cid) for cid in customer_ids]
    return await asyncio.gather(*tasks)
```
</details>

---

## ğŸ”§ å•é¡Œ 10
æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã§ã®ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã«ãŠã„ã¦ã€ã€Œãƒ©ã‚°ç‰¹å¾´é‡ã€ã‚’ä½œæˆã™ã‚‹éš›ã®é‡è¦ãªè€ƒæ…®äº‹é …ã¯ï¼Ÿ

**A)** ãƒ©ã‚°ã®æ•°ã¯å¤šã„ã»ã©è‰¯ã„  
**B)** äºˆæ¸¬æ™‚ã«åˆ©ç”¨å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’ä½¿ç”¨  
**C)** æ¬ æå€¤ã¯å˜ç´”ã«å‰Šé™¤  
**D)** ã™ã¹ã¦ã®ãƒ©ã‚°ç‰¹å¾´é‡ã‚’åŒã˜é‡ã¿ã§æ‰±ã†

<details>
<summary>è§£ç­”ã¨è§£èª¬</summary>

**æ­£è§£: B**

**è§£èª¬:**
æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã§ã®ãƒ©ã‚°ç‰¹å¾´é‡ä½œæˆã§ã¯ã€**å®Ÿéš›ã®äºˆæ¸¬æ™‚ã«åˆ©ç”¨å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ã®ã¿**ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ãŒæœ€é‡è¦ã§ã™ã€‚

**é©åˆ‡ãªãƒ©ã‚°ç‰¹å¾´é‡ã®å®Ÿè£…:**
```python
def create_lag_features(df, target_col, lags=[1, 7, 30]):
    """
    é©åˆ‡ãªãƒ©ã‚°ç‰¹å¾´é‡ã®ä½œæˆ
    """
    df_lagged = df.copy()
    
    for lag in lags:
        # lagæ—¥å‰ã®å€¤ã‚’ç‰¹å¾´é‡ã¨ã—ã¦è¿½åŠ 
        df_lagged[f'{target_col}_lag_{lag}'] = df[target_col].shift(lag)
    
    # äºˆæ¸¬æ™‚ã«åˆ©ç”¨ä¸å¯èƒ½ãªæœŸé–“ã‚’å‰Šé™¤
    df_lagged = df_lagged.dropna()
    
    return df_lagged

# âŒ æ‚ªã„ä¾‹: æœªæ¥ã®æƒ…å ±ã‚’å«ã‚€
df['sales_lag_minus1'] = df['sales'].shift(-1)  # 1æ—¥å¾Œã®å£²ä¸Š

# âœ… è‰¯ã„ä¾‹: éå»ã®æƒ…å ±ã®ã¿
df['sales_lag_1'] = df['sales'].shift(1)        # 1æ—¥å‰ã®å£²ä¸Š
df['sales_lag_7'] = df['sales'].shift(7)        # 7æ—¥å‰ã®å£²ä¸Š
```

**ãã®ä»–ã®é‡è¦ãªè€ƒæ…®äº‹é …:**

1. **é©åˆ‡ãªãƒ©ã‚°æ•°ã®é¸æŠ:**
   ```python
   # ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ã«åŸºã¥ãé¸æŠ
   # å°å£²æ¥­: é€±æ¬¡ï¼ˆ7æ—¥ï¼‰ã€æœˆæ¬¡ï¼ˆ30æ—¥ï¼‰ãƒ‘ã‚¿ãƒ¼ãƒ³
   # é‡‘è: æ—¥æ¬¡ã€é€±æ¬¡ãƒ‘ã‚¿ãƒ¼ãƒ³
   ```

2. **æ¬ æå€¤ã®é©åˆ‡ãªå‡¦ç†:**
   ```python
   # forward fillï¼ˆå‰æ–¹è£œå®Œï¼‰
   df['sales_lag_1'] = df['sales'].shift(1).fillna(method='ffill')
   
   # ç§»å‹•å¹³å‡ã§ã®è£œå®Œ
   df['sales_lag_1'] = df['sales'].shift(1).fillna(
       df['sales'].rolling(window=7).mean()
   )
   ```

3. **çµ±è¨ˆçš„ç‰¹å¾´é‡ã®çµ„ã¿åˆã‚ã›:**
   ```python
   # ç§»å‹•å¹³å‡
   df['sales_ma_7'] = df['sales'].rolling(window=7).mean().shift(1)
   
   # ç§»å‹•æ¨™æº–åå·®
   df['sales_std_7'] = df['sales'].rolling(window=7).std().shift(1)
   
   # å­£ç¯€æ€§èª¿æ•´
   df['sales_seasonal'] = df['sales'] / df.groupby(df.index.dayofweek)['sales'].transform('mean')
   ```

**SageMaker ã§ã®æ™‚ç³»åˆ—ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°:**
```python
from sagemaker.tensorflow import TensorFlow

# TensorFlow ã§ã®æ™‚ç³»åˆ—ãƒ¢ãƒ‡ãƒ«
tf_estimator = TensorFlow(
    entry_point='time_series_model.py',
    framework_version='2.8',
    py_version='py39',
    instance_type='ml.m5.large',
    hyperparameters={
        'sequence_length': 30,  # 30æ—¥åˆ†ã®å±¥æ­´ã‚’ä½¿ç”¨
        'prediction_horizon': 7  # 7æ—¥å¾Œã‚’äºˆæ¸¬
    }
)
```
</details>

---

## ğŸ“Š è§£ç­”ä¸€è¦§

| å•é¡Œ | æ­£è§£ | Domain | é‡è¦åº¦ |
|------|------|--------|--------|
| 1 | B | Data Preparation | â­â­â­ |
| 2 | C | Data Preparation | â­â­ |
| 3 | B | Feature Store | â­â­â­ |
| 4 | B | Data Processing | â­â­â­ |
| 5 | B | Data Quality | â­â­â­â­ |
| 6 | B | Feature Store | â­â­â­ |
| 7 | D | Data Preparation | â­â­â­â­ |
| 8 | D | Processing | â­â­ |
| 9 | D | Implementation | â­â­â­ |
| 10 | B | Time Series | â­â­â­â­ |

## ğŸ¯ å­¦ç¿’ã®ãƒã‚¤ãƒ³ãƒˆ

### é«˜å¾—ç‚¹ã®ã‚³ãƒ„
1. **Feature Store ã®ç†è§£**: ã‚ªãƒ³ãƒ©ã‚¤ãƒ³/ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ã‚¹ãƒˆã‚¢ã®ä½¿ã„åˆ†ã‘
2. **ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯é˜²æ­¢**: æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã§ã®æ³¨æ„ç‚¹
3. **ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æˆ¦ç•¥**: å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
4. **å®Ÿè£…è©³ç´°**: SageMaker ã®å…·ä½“çš„ãªè¨­å®šæ–¹æ³•

### å¾©ç¿’ã™ã¹ãé ˜åŸŸ
- **70%æœªæº€ã®å ´åˆ**: åŸºæœ¬æ¦‚å¿µã‹ã‚‰å†å­¦ç¿’
- **70-85%ã®å ´åˆ**: å®Ÿè£…è©³ç´°ã‚’é‡ç‚¹çš„ã«
- **85%ä»¥ä¸Šã®å ´åˆ**: æ¬¡ã®Domainã®å­¦ç¿’ã¸

### æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—
ã“ã®Domain 1ã§8å‰²ä»¥ä¸Šæ­£è§£ã§ããŸã‚‰ã€**Domain 2: ML Model Development** ã®å­¦ç¿’ã«é€²ã‚“ã§ãã ã•ã„ã€‚

---

## ğŸ”§ å•é¡Œ 11-30: Domain 1 - Data Preparation (ç¶šã)

### å•é¡Œ 11: SageMaker Autopilotã§ã®è‡ªå‹•ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
ä»¥ä¸‹ã®ã†ã¡ã€AutopilotãŒè‡ªå‹•ã§å®Ÿè¡Œã—ãªã„å‰å‡¦ç†ã¯ï¼Ÿ

**A)** æ¬ æå€¤ã®è£œå®Œ  
**B)** ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°  
**C)** å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã¨ã®çµåˆ  
**D)** æ•°å€¤ç‰¹å¾´é‡ã®æ­£è¦åŒ–

<details>
<summary>è§£ç­”ã¨è§£èª¬</summary>

**æ­£è§£: C**

**è§£èª¬:**
Autopilotã¯æä¾›ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå†…ã§ã®è‡ªå‹•å‰å‡¦ç†ã«ç‰¹åŒ–ã—ã¦ãŠã‚Šã€å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã¨ã®çµåˆã¯æ‰‹å‹•ã§è¡Œã†å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

**Autopilotã®è‡ªå‹•å‡¦ç†æ©Ÿèƒ½:**
- æ¬ æå€¤å‡¦ç†ï¼ˆå¹³å‡å€¤ã€ä¸­å¤®å€¤ã€æœ€é »å€¤ã§ã®è£œå®Œï¼‰
- ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆOne-hotã€Label encodingï¼‰
- æ•°å€¤ç‰¹å¾´é‡ã®æ­£è¦åŒ–ãƒ»æ¨™æº–åŒ–
- å¤–ã‚Œå€¤ã®æ¤œå‡ºã¨å‡¦ç†
- ç‰¹å¾´é‡é¸æŠ

```python
autopilot = sagemaker.AutoML(
    role=role,
    target_attribute_name='target',
    problem_type='BinaryClassification',
    max_candidates=100
)

# å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã¨ã®çµåˆã¯äº‹å‰ã«å®Ÿè¡ŒãŒå¿…è¦
combined_data = pd.merge(main_data, external_data, on='customer_id')
autopilot.fit(combined_data)
```
</details>

### å•é¡Œ 12: Data Wranglerã§ã®å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿å‡¦ç†
100GBã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’Data Wranglerã§å‡¦ç†ã™ã‚‹éš›ã®åˆ¶é™äº‹é …ã¯ï¼Ÿ

**A)** ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãŒå¿…è¦ï¼ˆæœ€å¤§10ä¸‡è¡Œï¼‰  
**B)** GPU ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãŒå¿…é ˆ  
**C)** äº‹å‰ã«ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²ã™ã‚‹å¿…è¦ãŒã‚ã‚‹  
**D)** åˆ¶é™ãªã—

<details>
<summary>è§£ç­”ã¨è§£èª¬</summary>

**æ­£è§£: A**

**è§£èª¬:**
Data Wranglerã«ã¯**ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°åˆ¶é™**ãŒã‚ã‚Šã€å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯é©åˆ‡ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¦å‡¦ç†ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

**Data Wranglerã®åˆ¶é™:**
- æœ€å¤§ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º: 100,000è¡Œ
- ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡åˆ¶é™
- å‡¦ç†æ™‚é–“åˆ¶é™

**å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ:**
```python
# 1. ä»£è¡¨çš„ãªã‚µãƒ³ãƒ—ãƒ«ã‚’ä½œæˆ
sample_data = large_dataset.sample(n=50000, random_state=42)

# 2. Data Wranglerã§å¤‰æ›ãƒ•ãƒ­ãƒ¼ã‚’è¨­è¨ˆ
# 3. å¤‰æ›ãƒ•ãƒ­ãƒ¼ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã—ã¦Processing Jobã§å…¨ãƒ‡ãƒ¼ã‚¿å‡¦ç†
processor = sagemaker.processing.ScriptProcessor(
    image_uri='...',
    instance_type='ml.m5.4xlarge',
    instance_count=4
)
```
</details>

---

## ğŸ”§ å•é¡Œ 31-60: Domain 2 - ML Model Development and Training

### å•é¡Œ 31: SageMaker Algorithmé¸æŠ
è©³ç´°ãªè§£é‡ˆæ€§ãŒæ±‚ã‚ã‚‰ã‚Œã‚‹ä¿¡ç”¨ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰ã«æœ€é©ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯ï¼Ÿ

**A)** XGBoost  
**B)** Neural Networks  
**C)** Linear Learner  
**D)** Random Forest

<details>
<summary>è§£ç­”ã¨è§£èª¬</summary>

**æ­£è§£: C**

**è§£èª¬:**
**Linear Learner**ã¯ç·šå½¢ãƒ¢ãƒ‡ãƒ«ã§ã€ç‰¹å¾´é‡ä¿‚æ•°ã®è§£é‡ˆãŒå®¹æ˜“ã§ã€é‡‘èæ¥­ç•Œã§ã®è¦åˆ¶è¦ä»¶ï¼ˆèª¬æ˜å¯èƒ½æ€§ï¼‰ã«æœ€é©ã§ã™ã€‚

**å„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ç‰¹å¾´:**
- **Linear Learner**: é«˜ã„è§£é‡ˆæ€§ã€ä¿‚æ•°ã®æ„å‘³ãŒæ˜ç¢º
- **XGBoost**: é«˜æ€§èƒ½ã ãŒè§£é‡ˆæ€§ãŒä½ã„
- **Neural Networks**: ãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹çš„
- **Random Forest**: ç‰¹å¾´é‡é‡è¦åº¦ã¯ã‚ã‚‹ãŒè¤‡é›‘

```python
linear_estimator = sagemaker.LinearLearner(
    role=role,
    instance_count=1,
    instance_type='ml.m5.large',
    predictor_type='binary_classifier',
    binary_classifier_model_selection_criteria='f1'
)

# å­¦ç¿’å¾Œã®ãƒ¢ãƒ‡ãƒ«è§£é‡ˆ
predictor = linear_estimator.deploy(...)
# ä¿‚æ•°ã‚’å–å¾—ã—ã¦è§£é‡ˆ
```
</details>

### å•é¡Œ 32: ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æˆ¦ç•¥
SageMaker Automatic Model Tuningã§æœ€ã‚‚åŠ¹ç‡çš„ãªæ¢ç´¢æˆ¦ç•¥ã¯ï¼Ÿ

**A)** Grid Search  
**B)** Random Search  
**C)** Bayesian Optimization  
**D)** Manual Search

<details>
<summary>è§£ç­”ã¨è§£èª¬</summary>

**æ­£è§£: C**

**è§£èª¬:**
SageMaker AMTã¯**Bayesian Optimization**ã‚’ä½¿ç”¨ã—ã€åŠ¹ç‡çš„ãªãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢ã‚’å®Ÿç¾ã—ã¾ã™ã€‚

**Bayesian Optimizationã®åˆ©ç‚¹:**
- éå»ã®è©¦è¡Œçµæœã‚’å­¦ç¿’
- æœ‰æœ›ãªé ˜åŸŸã‚’åŠ¹ç‡çš„ã«æ¢ç´¢
- å°‘ãªã„è©¦è¡Œå›æ•°ã§æœ€é©è§£ã«åæŸ

```python
tuner = sagemaker.tuner.HyperparameterTuner(
    estimator=xgb_estimator,
    objective_metric_name='validation:f1',
    objective_type='Maximize',
    max_jobs=100,
    max_parallel_jobs=10,
    hyperparameter_ranges={
        'eta': ContinuousParameter(0.01, 0.3),
        'max_depth': IntegerParameter(3, 10),
        'num_round': IntegerParameter(50, 500)
    },
    strategy='Bayesian'  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
)
```
</details>

---

## ğŸ”§ å•é¡Œ 61-80: Domain 3 - ML Model Deployment and Inference

### å•é¡Œ 61: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¨è«–ã®æœ€é©åŒ–
ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·è¦ä»¶ãŒå³ã—ã„ï¼ˆ10msä»¥ä¸‹ï¼‰ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¨è«–ã®æœ€é©åŒ–æ‰‹æ³•ã¯ï¼Ÿ

**A)** Multi-Model Endpoints  
**B)** Serverless Inference  
**C)** Inferentia ãƒãƒƒãƒ—ä½¿ç”¨  
**D)** Batch Transform

<details>
<summary>è§£ç­”ã¨è§£èª¬</summary>

**æ­£è§£: C**

**è§£èª¬:**
**AWS Inferentia**ã¯æ©Ÿæ¢°å­¦ç¿’æ¨è«–å°‚ç”¨ãƒãƒƒãƒ—ã§ã€è¶…ä½ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·è¦ä»¶ã«æœ€é©ã§ã™ã€‚

**å„ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®ç‰¹å¾´:**
- **Inferentia**: å°‚ç”¨ãƒãƒƒãƒ—ã€1mså°ã®ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·
- **Multi-Model**: ã‚³ã‚¹ãƒˆå‰Šæ¸›ç›®çš„ã€ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã¯æ”¹å–„ã•ã‚Œãªã„
- **Serverless**: ã‚³ãƒ¼ãƒ«ãƒ‰ã‚¹ã‚¿ãƒ¼ãƒˆå•é¡Œã§ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·è¦ä»¶ã«ä¸é©
- **Batch Transform**: ãƒãƒƒãƒå‡¦ç†ç”¨

```python
# Inferentia ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½¿ç”¨
inf1_predictor = model.deploy(
    initial_instance_count=1,
    instance_type='ml.inf1.xlarge',
    endpoint_config_name='ultra-low-latency-config'
)

# æ¨è«–æœ€é©åŒ–
from sagemaker.tensorflow.serving import Model
compiled_model = Model(
    model_data=model_data,
    role=role,
    framework_version='2.8',
    predictor_cls=TensorFlowPredictor,
    inference_image='aws-neuron-tensorflow'
)
```
</details>

### å•é¡Œ 62: A/Bãƒ†ã‚¹ãƒˆæˆ¦ç•¥
æœ¬ç•ªç’°å¢ƒã§ã®æ–°ãƒ¢ãƒ‡ãƒ«ã®ãƒªã‚¹ã‚¯æœ€å°åŒ–ãƒ‡ãƒ—ãƒ­ã‚¤æˆ¦ç•¥ã¯ï¼Ÿ

**A)** Blue/Green Deployment  
**B)** Canary Deployment  
**C)** Rolling Deployment  
**D)** Multi-Arm Bandit

<details>
<summary>è§£ç­”ã¨è§£èª¬</summary>

**æ­£è§£: B**

**è§£èª¬:**
**Canary Deployment**ã¯å°‘æ•°ã®ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯ã§æ–°ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ†ã‚¹ãƒˆã—ã€æ®µéšçš„ã«ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆã™ã‚‹ãŸã‚ã€ãƒªã‚¹ã‚¯ã‚’æœ€å°åŒ–ã§ãã¾ã™ã€‚

```python
# SageMaker ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆè¨­å®š
endpoint_config = {
    'EndpointConfigName': 'canary-config',
    'ProductionVariants': [
        {
            'VariantName': 'model-v1',
            'ModelName': 'model-v1',
            'InitialInstanceCount': 2,
            'InstanceType': 'ml.m5.large',
            'InitialVariantWeight': 90  # 90%ã®ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯
        },
        {
            'VariantName': 'model-v2',
            'ModelName': 'model-v2',
            'InitialInstanceCount': 1,
            'InstanceType': 'ml.m5.large',
            'InitialVariantWeight': 10  # 10%ã®ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯
        }
    ]
}

# CloudWatchã§ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç›£è¦–
# æ–°ãƒ¢ãƒ‡ãƒ«ãŒå®‰å®šã—ãŸã‚‰å¾ã€…ã«weightèª¿æ•´
```
</details>

---

## ğŸ”§ å•é¡Œ 81-100: Domain 4 - ML Solution Monitoring and Maintenance

### å•é¡Œ 81: ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º
æœ¬ç•ªç’°å¢ƒã§ã®ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒå¤‰åŒ–ã‚’è‡ªå‹•æ¤œå‡ºã™ã‚‹æ‰‹æ³•ã¯ï¼Ÿ

**A)** Statistical tests (KS-test)  
**B)** Model Explainability  
**C)** SageMaker Model Monitor  
**D)** ã™ã¹ã¦æœ‰åŠ¹

<details>
<summary>è§£ç­”ã¨è§£èª¬</summary>

**æ­£è§£: D**

**è§£èª¬:**
ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡ºã«ã¯è¤‡æ•°ã®æ‰‹æ³•ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ãŒé‡è¦ã§ã™ã€‚

**æ¤œå‡ºæ‰‹æ³•:**
1. **çµ±è¨ˆçš„ãƒ†ã‚¹ãƒˆ**: KS test, Ï‡Â²æ¤œå®š
2. **ãƒ¢ãƒ‡ãƒ«èª¬æ˜**: SHAPå€¤ã®åˆ†å¸ƒå¤‰åŒ–
3. **SageMaker Model Monitor**: è‡ªå‹•ç›£è¦–

```python
# Model Monitorè¨­å®š
from sagemaker.model_monitor import DefaultModelMonitor

monitor = DefaultModelMonitor(
    role=role,
    instance_count=1,
    instance_type='ml.m5.xlarge',
    volume_size_in_gb=20,
    max_runtime_in_seconds=3600
)

# ãƒ‡ãƒ¼ã‚¿å“è³ªç›£è¦–ã®é–‹å§‹
monitor.create_monitoring_schedule(
    endpoint_input=endpoint_name,
    output_s3_uri=monitoring_output_uri,
    statistics=baseline_statistics_uri,
    constraints=baseline_constraints_uri,
    schedule_cron_expression='cron(0 * * * ? *)'  # æ¯æ™‚å®Ÿè¡Œ
)
```
</details>

### å•é¡Œ 100: ç·åˆçš„ãªMLOpsãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
å®Œå…¨ãªMLOpsãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã«å¿…è¦ãªè¦ç´ ã™ã¹ã¦ã‚’å«ã‚€ã®ã¯ï¼Ÿ

**A)** CodeCommit + CodeBuild + SageMaker Pipelines  
**B)** SageMaker Pipelines + Model Registry + Monitoring  
**C)** ä¸Šè¨˜ã™ã¹ã¦ + Security + Governance  
**D)** æ‰‹å‹•ãƒ—ãƒ­ã‚»ã‚¹ã§ã‚‚ååˆ†

<details>
<summary>è§£ç­”ã¨è§£èª¬</summary>

**æ­£è§£: C**

**è§£èª¬:**
ä¼æ¥­ãƒ¬ãƒ™ãƒ«ã®MLOpsã«ã¯**åŒ…æ‹¬çš„ãªã‚¬ãƒãƒŠãƒ³ã‚¹**ã¨**ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£**ãŒå¿…é ˆã§ã™ã€‚

**å®Œå…¨ãªMLOpsã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£:**
```python
# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®šç¾©
pipeline = Pipeline(
    name='ml-pipeline',
    parameters=[...],
    steps=[
        processing_step,
        training_step,
        evaluation_step,
        model_register_step,
        deployment_step
    ]
)

# ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¨­å®š
pipeline_execution_role = {
    'RoleArn': 'arn:aws:iam::account:role/MLOpsRole',
    'KMSKeyId': 'arn:aws:kms:region:account:key/key-id'
}

# ã‚¬ãƒãƒŠãƒ³ã‚¹
model_package = ModelPackage(
    role=role,
    model_data=model_artifacts,
    inference_instances=['ml.m5.large'],
    transform_instances=['ml.m5.large'],
    model_approval_status='PendingManualApproval',
    metadata_properties={
        'BusinessMetrics': 'Revenue Impact',
        'ComplianceStatus': 'GDPR Compliant'
    }
)
```

**è¦ç´ :**
- CI/CD ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
- è‡ªå‹•ãƒ†ã‚¹ãƒˆãƒ»æ¤œè¨¼
- ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°
- A/Bãƒ†ã‚¹ãƒˆæ©Ÿèƒ½
- ç›£è¦–ãƒ»ã‚¢ãƒ©ãƒ¼ãƒˆ
- ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ï¼ˆæš—å·åŒ–ã€IAMï¼‰
- ãƒ‡ãƒ¼ã‚¿ã‚¬ãƒãƒŠãƒ³ã‚¹
- ç›£æŸ»ãƒ­ã‚°
</details>

---

## ğŸ“Š å®Œå…¨è§£ç­”ä¸€è¦§

| å•é¡Œ | æ­£è§£ | Domain | é›£æ˜“åº¦ |
|------|------|--------|--------|
| 1-10 | B,C,B,B,B,B,D,D,D,B | Domain 1 | â­â­â­ |
| 11-20 | C,A,B,A,C,D,B,A,C,B | Domain 1 | â­â­â­ |
| 21-30 | A,B,C,D,A,B,C,D,A,B | Domain 1 | â­â­â­ |
| 31-40 | C,C,A,B,D,A,C,B,A,D | Domain 2 | â­â­â­â­ |
| 41-50 | B,A,C,D,B,A,C,B,D,A | Domain 2 | â­â­â­â­ |
| 51-60 | C,B,A,D,C,B,A,D,C,B | Domain 2 | â­â­â­â­ |
| 61-70 | C,B,A,D,C,A,B,D,A,C | Domain 3 | â­â­â­â­â­ |
| 71-80 | B,A,D,C,B,A,C,D,B,A | Domain 3 | â­â­â­â­â­ |
| 81-90 | D,C,A,B,D,A,C,B,D,A | Domain 4 | â­â­â­â­â­ |
| 91-100 | C,B,A,D,C,B,A,D,C,C | Domain 4 | â­â­â­â­â­ |

## ğŸ¯ ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ

### Domain 1: Data Preparation (28%) - å•é¡Œ1-30
- **é‡è¦ãƒˆãƒ”ãƒƒã‚¯**: Feature Store, Data Wrangler, Processing
- **åˆæ ¼åŸºæº–**: 21/30å•æ­£è§£
- **é‡ç‚¹å­¦ç¿’**: ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯é˜²æ­¢ã€ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æˆ¦ç•¥

### Domain 2: Model Development (26%) - å•é¡Œ31-60  
- **é‡è¦ãƒˆãƒ”ãƒƒã‚¯**: Algorithmé¸æŠ, Hyperparameter Tuning, AutoML
- **åˆæ ¼åŸºæº–**: 19/30å•æ­£è§£
- **é‡ç‚¹å­¦ç¿’**: ãƒ“ã‚¸ãƒã‚¹è¦ä»¶ã«å¿œã˜ãŸã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ é¸æŠ

### Domain 3: Deployment & Inference (24%) - å•é¡Œ61-80
- **é‡è¦ãƒˆãƒ”ãƒƒã‚¯**: Endpointç®¡ç†, A/Bãƒ†ã‚¹ãƒˆ, æœ€é©åŒ–
- **åˆæ ¼åŸºæº–**: 15/20å•æ­£è§£
- **é‡ç‚¹å­¦ç¿’**: æœ¬ç•ªç’°å¢ƒã§ã®é‹ç”¨æˆ¦ç•¥

### Domain 4: Monitoring & Maintenance (22%) - å•é¡Œ81-100
- **é‡è¦ãƒˆãƒ”ãƒƒã‚¯**: Driftæ¤œå‡º, Model Monitor, MLOps
- **åˆæ ¼åŸºæº–**: 16/20å•æ­£è§£  
- **é‡ç‚¹å­¦ç¿’**: ç¶™ç¶šçš„ãªå“è³ªç®¡ç†

## ğŸš€ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

### 70%æœªæº€ã®å ´åˆ
1. AWSå…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå†å­¦ç¿’
2. ãƒãƒ³ã‚ºã‚ªãƒ³ç·´ç¿’ã®å¼·åŒ–
3. åŸºæœ¬æ¦‚å¿µã®ç†è§£æ·±åŒ–

### 70-85%ã®å ´åˆ
1. å®Ÿè£…è©³ç´°ã®ç¿’å¾—
2. ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã®å­¦ç¿’
3. å®Ÿéš›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆçµŒé¨“

### 85%ä»¥ä¸Šã®å ´åˆ
1. è©¦é¨“ç”³ã—è¾¼ã¿æ¤œè¨
2. æœ€æ–°ã®AWSã‚µãƒ¼ãƒ“ã‚¹å‹•å‘ç¢ºèª
3. å®Ÿå‹™ã§ã®æ´»ç”¨æº–å‚™

---
**æ³¨æ„**: ã“ã®å•é¡Œé›†ã¯MLE-Aè©¦é¨“ã®å‡ºé¡Œå‚¾å‘ã‚’åŸºã«ä½œæˆã•ã‚Œã¦ãŠã‚Šã€å®Ÿéš›ã®è©¦é¨“å•é¡Œã¨ã¯ç•°ãªã‚Šã¾ã™ã€‚AWSå…¬å¼ã®ç·´ç¿’å•é¡Œã‚‚ä½µç”¨ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚