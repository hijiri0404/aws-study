# Lab 2: SageMaker ã§ã®MLãƒ¢ãƒ‡ãƒ«é–‹ç™ºå®Ÿè·µ

## ğŸ¯ å­¦ç¿’ç›®æ¨™

ã“ã®ãƒ©ãƒœã§ã¯ã€SageMaker ã‚’ä½¿ç”¨ã—ã¦æœ¬æ ¼çš„ãªæ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«é–‹ç™ºã‚’è¡Œã„ã¾ã™ã€‚Built-in ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‹ã‚‰ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«ã¾ã§ã€æ§˜ã€…ãªé–‹ç™ºæ‰‹æ³•ã‚’ç¿’å¾—ã—ã¾ã™ã€‚

**ç¿’å¾—ã‚¹ã‚­ãƒ«**:
- SageMaker Built-in ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æ´»ç”¨
- ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«ã®ã‚³ãƒ³ãƒ†ãƒŠåŒ–
- ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
- AutoML (SageMaker Autopilot) ã®æ´»ç”¨
- ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒã¨é¸æŠ

**æ‰€è¦æ™‚é–“**: 6-8æ™‚é–“  
**æ¨å®šã‚³ã‚¹ãƒˆ**: $20-35

## ğŸ“‹ ã‚·ãƒŠãƒªã‚ª

**ä¼æ¥­**: ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å°å£²æ¥­  
**èª²é¡Œ**: é¡§å®¢ã®è³¼è²·è¡Œå‹•äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰  
**ãƒ‡ãƒ¼ã‚¿**: é¡§å®¢å±æ€§ã€è³¼è²·å±¥æ­´ã€Webã‚µã‚¤ãƒˆè¡Œå‹•ãƒ­ã‚°  
**ç›®æ¨™**: 30æ—¥ä»¥å†…ã®è³¼è²·ç¢ºç‡ã‚’äºˆæ¸¬ã™ã‚‹ãƒ¢ãƒ‡ãƒ«é–‹ç™º

## Phase 1: ãƒ‡ãƒ¼ã‚¿æº–å‚™ã¨æ¢ç´¢çš„åˆ†æ

### 1.1 å®Ÿè·µç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆ

```python
# ã‚¹ã‚¯ãƒªãƒ—ãƒˆ: create_customer_dataset.py

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import boto3
import sagemaker

def create_synthetic_customer_data():
    """
    é¡§å®¢è³¼è²·äºˆæ¸¬ç”¨ã®åˆæˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ
    """
    np.random.seed(42)
    n_customers = 10000
    
    print("ğŸ”„ åˆæˆé¡§å®¢ãƒ‡ãƒ¼ã‚¿ä½œæˆä¸­...")
    
    # åŸºæœ¬é¡§å®¢å±æ€§
    customers = pd.DataFrame({
        'customer_id': range(1, n_customers + 1),
        'age': np.random.normal(40, 15, n_customers).astype(int),
        'gender': np.random.choice(['M', 'F'], n_customers),
        'income': np.random.lognormal(10.5, 0.5, n_customers).astype(int),
        'membership_years': np.random.exponential(2, n_customers),
        'city_tier': np.random.choice([1, 2, 3], n_customers, p=[0.3, 0.5, 0.2])
    })
    
    # è¡Œå‹•ãƒ‡ãƒ¼ã‚¿
    customers['website_visits_30d'] = np.random.poisson(8, n_customers)
    customers['avg_session_duration'] = np.random.exponential(15, n_customers)  # minutes
    customers['cart_abandonment_rate'] = np.random.beta(2, 5, n_customers)
    customers['email_open_rate'] = np.random.beta(3, 7, n_customers)
    
    # éå»è³¼è²·ãƒ‡ãƒ¼ã‚¿
    customers['total_purchases'] = np.random.poisson(5, n_customers)
    customers['avg_order_value'] = np.random.lognormal(4, 0.8, n_customers)
    customers['days_since_last_purchase'] = np.random.exponential(45, n_customers)
    customers['favorite_category'] = np.random.choice(
        ['Electronics', 'Clothing', 'Books', 'Home', 'Sports'], 
        n_customers
    )
    
    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ï¼ˆ30æ—¥ä»¥å†…è³¼è²·ç¢ºç‡ã«å½±éŸ¿ã™ã‚‹ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ï¼‰
    purchase_probability = (
        0.3 * (customers['income'] / customers['income'].max()) +
        0.2 * (1 - customers['days_since_last_purchase'] / 365) +
        0.2 * (customers['website_visits_30d'] / customers['website_visits_30d'].max()) +
        0.15 * customers['email_open_rate'] +
        0.15 * (1 - customers['cart_abandonment_rate']) +
        np.random.normal(0, 0.1, n_customers)  # ãƒã‚¤ã‚º
    )
    
    # ç¢ºç‡ã‚’0-1ã«æ­£è¦åŒ–
    purchase_probability = np.clip(purchase_probability, 0, 1)
    
    # äºŒå€¤ã‚¿ãƒ¼ã‚²ãƒƒãƒˆä½œæˆ
    customers['will_purchase_30d'] = (np.random.random(n_customers) < purchase_probability).astype(int)
    
    print(f"âœ… ãƒ‡ãƒ¼ã‚¿ä½œæˆå®Œäº†: {len(customers)} é¡§å®¢")
    print(f"   è³¼è²·äºˆå®šé¡§å®¢: {customers['will_purchase_30d'].sum()} ({customers['will_purchase_30d'].mean():.1%})")
    
    return customers

def upload_to_s3(df, bucket_name, key):
    """ãƒ‡ãƒ¼ã‚¿ã‚’S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"""
    print(f"ğŸ“¤ S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­: s3://{bucket_name}/{key}")
    
    s3 = boto3.client('s3')
    
    # CSVã¨ã—ã¦ä¿å­˜
    csv_buffer = df.to_csv(index=False)
    
    try:
        s3.put_object(
            Bucket=bucket_name,
            Key=key,
            Body=csv_buffer,
            ContentType='text/csv'
        )
        print(f"âœ… ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†")
    except Exception as e:
        print(f"âŒ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")

def main():
    # SageMaker ã‚»ãƒƒã‚·ãƒ§ãƒ³åˆæœŸåŒ–
    sagemaker_session = sagemaker.Session()
    bucket = sagemaker_session.default_bucket()
    prefix = 'ml-lab02-customer-prediction'
    
    # ãƒ‡ãƒ¼ã‚¿ä½œæˆ
    customer_data = create_synthetic_customer_data()
    
    # å­¦ç¿’ãƒ»æ¤œè¨¼ãƒ»ãƒ†ã‚¹ãƒˆåˆ†å‰²
    from sklearn.model_selection import train_test_split
    
    train_data, temp_data = train_test_split(
        customer_data, test_size=0.4, random_state=42, 
        stratify=customer_data['will_purchase_30d']
    )
    
    val_data, test_data = train_test_split(
        temp_data, test_size=0.5, random_state=42,
        stratify=temp_data['will_purchase_30d']
    )
    
    print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿åˆ†å‰²:")
    print(f"   å­¦ç¿’: {len(train_data)} ä»¶")
    print(f"   æ¤œè¨¼: {len(val_data)} ä»¶")  
    print(f"   ãƒ†ã‚¹ãƒˆ: {len(test_data)} ä»¶")
    
    # S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    upload_to_s3(train_data, bucket, f'{prefix}/train/train.csv')
    upload_to_s3(val_data, bucket, f'{prefix}/validation/validation.csv')
    upload_to_s3(test_data, bucket, f'{prefix}/test/test.csv')
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜
    metadata = {
        'bucket': bucket,
        'prefix': prefix,
        'train_path': f's3://{bucket}/{prefix}/train/',
        'validation_path': f's3://{bucket}/{prefix}/validation/',
        'test_path': f's3://{bucket}/{prefix}/test/',
        'target_column': 'will_purchase_30d',
        'feature_columns': [col for col in customer_data.columns if col not in ['customer_id', 'will_purchase_30d']],
        'num_features': len(customer_data.columns) - 2,
        'total_samples': len(customer_data),
        'positive_rate': customer_data['will_purchase_30d'].mean()
    }
    
    import json
    with open('dataset_metadata.json', 'w') as f:
        json.dump(metadata, f, indent=2)
    
    print("ğŸ‰ ãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº†")
    return metadata

if __name__ == "__main__":
    metadata = main()
```

### 1.2 æ¢ç´¢çš„ãƒ‡ãƒ¼ã‚¿åˆ†æ (EDA)

```python
# ã‚¹ã‚¯ãƒªãƒ—ãƒˆ: exploratory_data_analysis.py

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import boto3
from sagemaker import get_execution_role
import warnings
warnings.filterwarnings('ignore')

def load_data_from_s3(bucket, key):
    """S3ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
    s3_path = f's3://{bucket}/{key}'
    print(f"ğŸ“¥ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: {s3_path}")
    
    df = pd.read_csv(s3_path)
    print(f"âœ… èª­ã¿è¾¼ã¿å®Œäº†: {df.shape}")
    return df

def perform_eda(df):
    """åŒ…æ‹¬çš„ãªæ¢ç´¢çš„ãƒ‡ãƒ¼ã‚¿åˆ†æ"""
    print("ğŸ” æ¢ç´¢çš„ãƒ‡ãƒ¼ã‚¿åˆ†æé–‹å§‹")
    
    # åŸºæœ¬çµ±è¨ˆæƒ…å ±
    print("\nğŸ“Š åŸºæœ¬çµ±è¨ˆæƒ…å ±:")
    print(df.describe())
    
    # æ¬ æå€¤ç¢ºèª
    print("\nğŸ” æ¬ æå€¤ç¢ºèª:")
    missing_data = df.isnull().sum()
    print(missing_data[missing_data > 0])
    
    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ†å¸ƒ
    print("\nğŸ¯ ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°åˆ†å¸ƒ:")
    target_dist = df['will_purchase_30d'].value_counts()
    print(target_dist)
    print(f"è³¼è²·äºˆå®šç‡: {df['will_purchase_30d'].mean():.2%}")
    
    # å¯è¦–åŒ–
    fig, axes = plt.subplots(3, 3, figsize=(18, 15))
    fig.suptitle('é¡§å®¢ãƒ‡ãƒ¼ã‚¿æ¢ç´¢çš„åˆ†æ', fontsize=16)
    
    # 1. ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ†å¸ƒ
    axes[0, 0].pie(target_dist.values, labels=['éè³¼è²·', 'è³¼è²·äºˆå®š'], autopct='%1.1f%%')
    axes[0, 0].set_title('è³¼è²·äºˆå®šåˆ†å¸ƒ')
    
    # 2. å¹´é½¢åˆ†å¸ƒ
    df['age'].hist(bins=30, ax=axes[0, 1], alpha=0.7)
    axes[0, 1].set_title('å¹´é½¢åˆ†å¸ƒ')
    axes[0, 1].set_xlabel('å¹´é½¢')
    
    # 3. åå…¥åˆ†å¸ƒ
    df['income'].hist(bins=30, ax=axes[0, 2], alpha=0.7)
    axes[0, 2].set_title('åå…¥åˆ†å¸ƒ')
    axes[0, 2].set_xlabel('åå…¥')
    
    # 4. å¹´é½¢vsè³¼è²·å‚¾å‘
    purchase_by_age = df.groupby(pd.cut(df['age'], bins=5))['will_purchase_30d'].mean()
    purchase_by_age.plot(kind='bar', ax=axes[1, 0])
    axes[1, 0].set_title('å¹´é½¢å±¤åˆ¥è³¼è²·ç‡')
    axes[1, 0].tick_params(axis='x', rotation=45)
    
    # 5. åå…¥vsè³¼è²·å‚¾å‘
    purchase_by_income = df.groupby(pd.cut(df['income'], bins=5))['will_purchase_30d'].mean()
    purchase_by_income.plot(kind='bar', ax=axes[1, 1])
    axes[1, 1].set_title('åå…¥å±¤åˆ¥è³¼è²·ç‡')
    axes[1, 1].tick_params(axis='x', rotation=45)
    
    # 6. éƒ½å¸‚å±¤åˆ¥è³¼è²·ç‡
    purchase_by_city = df.groupby('city_tier')['will_purchase_30d'].mean()
    purchase_by_city.plot(kind='bar', ax=axes[1, 2])
    axes[1, 2].set_title('éƒ½å¸‚å±¤åˆ¥è³¼è²·ç‡')
    
    # 7. Webã‚µã‚¤ãƒˆè¨ªå•vsè³¼è²·
    axes[2, 0].scatter(df['website_visits_30d'], df['will_purchase_30d'], alpha=0.3)
    axes[2, 0].set_title('Webã‚µã‚¤ãƒˆè¨ªå•æ•° vs è³¼è²·äºˆå®š')
    axes[2, 0].set_xlabel('30æ—¥é–“è¨ªå•æ•°')
    
    # 8. ç›¸é–¢è¡Œåˆ—
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    corr_matrix = df[numeric_cols].corr()
    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, ax=axes[2, 1])
    axes[2, 1].set_title('ç‰¹å¾´é‡ç›¸é–¢è¡Œåˆ—')
    
    # 9. ã‚«ãƒ†ã‚´ãƒªåˆ¥è³¼è²·ç‡
    category_purchase = df.groupby('favorite_category')['will_purchase_30d'].mean()
    category_purchase.plot(kind='bar', ax=axes[2, 2])
    axes[2, 2].set_title('ã‚«ãƒ†ã‚´ãƒªåˆ¥è³¼è²·ç‡')
    axes[2, 2].tick_params(axis='x', rotation=45)
    
    plt.tight_layout()
    plt.show()
    
    # ç‰¹å¾´é‡é‡è¦åº¦åˆ†æ
    analyze_feature_importance(df)

def analyze_feature_importance(df):
    """ç‰¹å¾´é‡é‡è¦åº¦ã®ç°¡æ˜“åˆ†æ"""
    print("\nğŸ” ç‰¹å¾´é‡é‡è¦åº¦åˆ†æ:")
    
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.preprocessing import LabelEncoder
    
    # ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
    df_encoded = df.copy()
    label_encoders = {}
    
    categorical_cols = ['gender', 'favorite_category']
    for col in categorical_cols:
        le = LabelEncoder()
        df_encoded[col] = le.fit_transform(df[col])
        label_encoders[col] = le
    
    # ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®åˆ†é›¢
    feature_cols = [col for col in df_encoded.columns if col not in ['customer_id', 'will_purchase_30d']]
    X = df_encoded[feature_cols]
    y = df_encoded['will_purchase_30d']
    
    # Random Forest ã§ç‰¹å¾´é‡é‡è¦åº¦è¨ˆç®—
    rf = RandomForestClassifier(n_estimators=100, random_state=42)
    rf.fit(X, y)
    
    # é‡è¦åº¦ã‚’é™é †ã§ã‚½ãƒ¼ãƒˆ
    feature_importance = pd.DataFrame({
        'feature': feature_cols,
        'importance': rf.feature_importances_
    }).sort_values('importance', ascending=False)
    
    print(feature_importance)
    
    # é‡è¦åº¦å¯è¦–åŒ–
    plt.figure(figsize=(10, 6))
    sns.barplot(data=feature_importance.head(10), x='importance', y='feature')
    plt.title('Top 10 ç‰¹å¾´é‡é‡è¦åº¦')
    plt.xlabel('é‡è¦åº¦')
    plt.tight_layout()
    plt.show()
    
    return feature_importance

def main():
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    import json
    with open('dataset_metadata.json', 'r') as f:
        metadata = json.load(f)
    
    # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    train_data = load_data_from_s3(metadata['bucket'], f"{metadata['prefix']}/train/train.csv")
    
    # EDAå®Ÿè¡Œ
    perform_eda(train_data)
    
    print("ğŸ‰ EDAå®Œäº†")

if __name__ == "__main__":
    main()
```

## Phase 2: Built-in ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã®ãƒ¢ãƒ‡ãƒ«å­¦ç¿’

### 2.1 XGBoost ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«ã‚ˆã‚‹å­¦ç¿’

```python
# ã‚¹ã‚¯ãƒªãƒ—ãƒˆ: train_xgboost_model.py

import sagemaker
from sagemaker.xgboost.estimator import XGBoost
from sagemaker.inputs import TrainingInput
from sagemaker.tuner import IntegerParameter, ContinuousParameter, HyperparameterTuner
import pandas as pd
import json

def prepare_data_for_xgboost(metadata):
    """XGBoostç”¨ã®ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†"""
    print("ğŸ”„ XGBoostç”¨ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ä¸­...")
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    train_data = pd.read_csv(f"s3://{metadata['bucket']}/{metadata['prefix']}/train/train.csv")
    val_data = pd.read_csv(f"s3://{metadata['bucket']}/{metadata['prefix']}/validation/validation.csv")
    
    # ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
    categorical_cols = ['gender', 'favorite_category']
    
    for col in categorical_cols:
        # One-hot encoding
        train_encoded = pd.get_dummies(train_data[col], prefix=col)
        val_encoded = pd.get_dummies(val_data[col], prefix=col)
        
        # ã‚«ãƒ©ãƒ ã®çµ±ä¸€ï¼ˆå­¦ç¿’ã¨ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã§åŒã˜ã‚«ãƒ©ãƒ ç¢ºä¿ï¼‰
        all_columns = set(train_encoded.columns) | set(val_encoded.columns)
        for c in all_columns:
            if c not in train_encoded.columns:
                train_encoded[c] = 0
            if c not in val_encoded.columns:
                val_encoded[c] = 0
        
        train_encoded = train_encoded[sorted(all_columns)]
        val_encoded = val_encoded[sorted(all_columns)]
        
        # å…ƒãƒ‡ãƒ¼ã‚¿ã¨çµåˆ
        train_data = pd.concat([train_data.drop(col, axis=1), train_encoded], axis=1)
        val_data = pd.concat([val_data.drop(col, axis=1), val_encoded], axis=1)
    
    # XGBoostå½¢å¼ï¼ˆã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ—ã‚’æœ€åˆã«ç§»å‹•ï¼‰
    target_col = 'will_purchase_30d'
    feature_cols = [col for col in train_data.columns if col not in ['customer_id', target_col]]
    
    train_final = train_data[[target_col] + feature_cols]
    val_final = val_data[[target_col] + feature_cols]
    
    # S3ã«ä¿å­˜
    train_path = f"s3://{metadata['bucket']}/{metadata['prefix']}/processed/train/"
    val_path = f"s3://{metadata['bucket']}/{metadata['prefix']}/processed/validation/"
    
    train_final.to_csv(train_path + 'train.csv', index=False, header=False)
    val_final.to_csv(val_path + 'validation.csv', index=False, header=False)
    
    print(f"âœ… å‰å‡¦ç†å®Œäº†")
    print(f"   å­¦ç¿’ãƒ‡ãƒ¼ã‚¿: {train_final.shape}")
    print(f"   æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {val_final.shape}")
    
    return train_path, val_path, feature_cols

def train_xgboost_model(train_path, val_path, role):
    """XGBoostãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’"""
    print("ğŸš€ XGBoostå­¦ç¿’é–‹å§‹...")
    
    # XGBoost estimator è¨­å®š
    xgb_estimator = XGBoost(
        entry_point="training_script.py",  # ã‚«ã‚¹ã‚¿ãƒ ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆå¾Œã§ä½œæˆï¼‰
        framework_version="1.5-1",
        py_version="py3",
        instance_type="ml.m5.large",
        instance_count=1,
        role=role,
        hyperparameters={
            'objective': 'binary:logistic',
            'eval_metric': 'auc',
            'num_round': 100,
            'max_depth': 6,
            'learning_rate': 0.1,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'min_child_weight': 1
        }
    )
    
    # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®è¨­å®š
    train_input = TrainingInput(train_path, content_type="text/csv")
    validation_input = TrainingInput(val_path, content_type="text/csv")
    
    # å­¦ç¿’å®Ÿè¡Œ
    xgb_estimator.fit({
        'train': train_input,
        'validation': validation_input
    })
    
    print("âœ… XGBoostå­¦ç¿’å®Œäº†")
    return xgb_estimator

def create_training_script():
    """XGBoostå­¦ç¿’ç”¨ã‚¹ã‚¯ãƒªãƒ—ãƒˆä½œæˆ"""
    training_script = '''
import argparse
import joblib
import pandas as pd
import xgboost as xgb
from sklearn.metrics import roc_auc_score, classification_report
import os

def main():
    parser = argparse.ArgumentParser()
    
    # SageMakerç’°å¢ƒå¤‰æ•°
    parser.add_argument("--model-dir", type=str, default=os.environ.get("SM_MODEL_DIR"))
    parser.add_argument("--train", type=str, default=os.environ.get("SM_CHANNEL_TRAIN"))
    parser.add_argument("--validation", type=str, default=os.environ.get("SM_CHANNEL_VALIDATION"))
    
    # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    parser.add_argument("--objective", type=str, default="binary:logistic")
    parser.add_argument("--eval_metric", type=str, default="auc")
    parser.add_argument("--num_round", type=int, default=100)
    parser.add_argument("--max_depth", type=int, default=6)
    parser.add_argument("--learning_rate", type=float, default=0.1)
    parser.add_argument("--subsample", type=float, default=0.8)
    parser.add_argument("--colsample_bytree", type=float, default=0.8)
    parser.add_argument("--min_child_weight", type=int, default=1)
    
    args = parser.parse_args()
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    train_data = pd.read_csv(f"{args.train}/train.csv", header=None)
    val_data = pd.read_csv(f"{args.validation}/validation.csv", header=None)
    
    # ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®åˆ†é›¢
    X_train = train_data.iloc[:, 1:]
    y_train = train_data.iloc[:, 0]
    X_val = val_data.iloc[:, 1:]
    y_val = val_data.iloc[:, 0]
    
    # XGBoostå½¢å¼ã«å¤‰æ›
    dtrain = xgb.DMatrix(X_train, label=y_train)
    dval = xgb.DMatrix(X_val, label=y_val)
    
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
    params = {
        'objective': args.objective,
        'eval_metric': args.eval_metric,
        'max_depth': args.max_depth,
        'learning_rate': args.learning_rate,
        'subsample': args.subsample,
        'colsample_bytree': args.colsample_bytree,
        'min_child_weight': args.min_child_weight,
        'verbosity': 1
    }
    
    # å­¦ç¿’å®Ÿè¡Œ
    watchlist = [(dtrain, 'train'), (dval, 'validation')]
    model = xgb.train(
        params=params,
        dtrain=dtrain,
        num_boost_round=args.num_round,
        evals=watchlist,
        early_stopping_rounds=20,
        verbose_eval=10
    )
    
    # äºˆæ¸¬ã¨è©•ä¾¡
    y_pred = model.predict(dval)
    auc_score = roc_auc_score(y_val, y_pred)
    
    print(f"\\nValidation AUC: {auc_score:.4f}")
    
    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
    joblib.dump(model, f"{args.model_dir}/xgboost-model")
    
    print("ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†")

if __name__ == "__main__":
    main()
'''
    
    with open('training_script.py', 'w') as f:
        f.write(training_script)
    
    print("âœ… å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆä½œæˆå®Œäº†")

def main():
    # ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
    role = sagemaker.get_execution_role()
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    with open('dataset_metadata.json', 'r') as f:
        metadata = json.load(f)
    
    # å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆä½œæˆ
    create_training_script()
    
    # ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
    train_path, val_path, feature_cols = prepare_data_for_xgboost(metadata)
    
    # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
    xgb_model = train_xgboost_model(train_path, val_path, role)
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ›´æ–°
    metadata['xgboost_model'] = {
        'model_name': xgb_model.model_name,
        'training_job_name': xgb_model.latest_training_job.name,
        'feature_columns': feature_cols
    }
    
    with open('dataset_metadata.json', 'w') as f:
        json.dump(metadata, f, indent=2)
    
    print("ğŸ‰ XGBoostãƒ¢ãƒ‡ãƒ«å­¦ç¿’å®Œäº†")
    return xgb_model

if __name__ == "__main__":
    model = main()
```

## Phase 3: ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°

### 3.1 è‡ªå‹•ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–

```python
# ã‚¹ã‚¯ãƒªãƒ—ãƒˆ: hyperparameter_tuning.py

from sagemaker.tuner import IntegerParameter, ContinuousParameter, HyperparameterTuner
from sagemaker.xgboost.estimator import XGBoost
import sagemaker
import json

def setup_hyperparameter_tuning(train_path, val_path, role):
    """ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®è¨­å®š"""
    print("ğŸ”§ ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®šä¸­...")
    
    # ãƒ™ãƒ¼ã‚¹Estimator
    xgb_estimator = XGBoost(
        entry_point="training_script.py",
        framework_version="1.5-1",
        py_version="py3",
        instance_type="ml.m5.large",
        instance_count=1,
        role=role,
        # å›ºå®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        hyperparameters={
            'objective': 'binary:logistic',
            'eval_metric': 'auc',
            'num_round': 200
        }
    )
    
    # ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å¯¾è±¡ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç¯„å›²
    hyperparameter_ranges = {
        'max_depth': IntegerParameter(3, 10),
        'learning_rate': ContinuousParameter(0.01, 0.3),
        'subsample': ContinuousParameter(0.6, 1.0),
        'colsample_bytree': ContinuousParameter(0.6, 1.0),
        'min_child_weight': IntegerParameter(1, 10),
    }
    
    # ç›®çš„ãƒ¡ãƒˆãƒªãƒƒã‚¯
    objective_metric_name = 'validation:auc'
    objective_type = 'Maximize'
    
    # ãƒãƒ¥ãƒ¼ãƒŠãƒ¼è¨­å®š
    tuner = HyperparameterTuner(
        estimator=xgb_estimator,
        objective_metric_name=objective_metric_name,
        objective_type=objective_type,
        hyperparameter_ranges=hyperparameter_ranges,
        max_jobs=20,           # æœ€å¤§20å›ã®è©¦è¡Œ
        max_parallel_jobs=3,   # ä¸¦åˆ—å®Ÿè¡Œæ•°
        strategy='Bayesian',   # ãƒ™ã‚¤ã‚¸ã‚¢ãƒ³æœ€é©åŒ–
        early_stopping_type='Auto'
    )
    
    print("âœ… ãƒãƒ¥ãƒ¼ãƒŠãƒ¼è¨­å®šå®Œäº†")
    return tuner

def run_hyperparameter_tuning(tuner, train_path, val_path):
    """ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ"""
    print("ğŸš€ ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹...")
    print("   â±ï¸  å®Ÿè¡Œæ™‚é–“: ç´„60-90åˆ†")
    
    from sagemaker.inputs import TrainingInput
    
    # ãƒ‡ãƒ¼ã‚¿å…¥åŠ›è¨­å®š
    train_input = TrainingInput(train_path, content_type="text/csv")
    val_input = TrainingInput(val_path, content_type="text/csv")
    
    # ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ
    tuner.fit({
        'train': train_input,
        'validation': val_input
    })
    
    print("âœ… ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†")
    return tuner

def analyze_tuning_results(tuner):
    """ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°çµæœã®åˆ†æ"""
    print("ğŸ“Š ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°çµæœåˆ†æä¸­...")
    
    # å…¨å®Ÿè¡Œã‚¸ãƒ§ãƒ–ã®å–å¾—
    tuning_job_name = tuner.latest_tuning_job.name
    
    import boto3
    sagemaker_client = boto3.client('sagemaker')
    
    response = sagemaker_client.describe_hyper_parameter_tuning_job(
        HyperParameterTuningJobName=tuning_job_name
    )
    
    # æœ€è‰¯ã‚¸ãƒ§ãƒ–ã®è©³ç´°å–å¾—
    best_job = response['BestTrainingJob']
    print(f"\\nğŸ† æœ€è‰¯ã‚¸ãƒ§ãƒ–:")
    print(f"   ã‚¸ãƒ§ãƒ–å: {best_job['TrainingJobName']}")
    print(f"   ãƒ¡ãƒˆãƒªãƒƒã‚¯å€¤: {best_job['FinalHyperParameterTuningJobObjectiveMetric']['Value']:.4f}")
    
    print(f"\\nğŸ”§ æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:")
    for param, value in best_job['TunedHyperParameters'].items():
        print(f"   {param}: {value}")
    
    # ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°çµ±è¨ˆã®å–å¾—
    training_jobs = []
    next_token = None
    
    while True:
        if next_token:
            response = sagemaker_client.list_training_jobs_for_hyper_parameter_tuning_job(
                HyperParameterTuningJobName=tuning_job_name,
                NextToken=next_token,
                MaxResults=100
            )
        else:
            response = sagemaker_client.list_training_jobs_for_hyper_parameter_tuning_job(
                HyperParameterTuningJobName=tuning_job_name,
                MaxResults=100
            )
        
        training_jobs.extend(response['TrainingJobSummaries'])
        
        if 'NextToken' not in response:
            break
        next_token = response['NextToken']
    
    # çµæœã®å¯è¦–åŒ–
    import pandas as pd
    import matplotlib.pyplot as plt
    import seaborn as sns
    
    results_data = []
    for job in training_jobs:
        if job['TrainingJobStatus'] == 'Completed':
            job_details = sagemaker_client.describe_training_job(
                TrainingJobName=job['TrainingJobName']
            )
            
            params = job_details['HyperParameters']
            metric_value = None
            
            if 'FinalMetricDataList' in job_details:
                for metric in job_details['FinalMetricDataList']:
                    if metric['MetricName'] == 'validation:auc':
                        metric_value = metric['Value']
                        break
            
            results_data.append({
                'job_name': job['TrainingJobName'],
                'auc': metric_value,
                'max_depth': int(params.get('max_depth', 0)),
                'learning_rate': float(params.get('learning_rate', 0)),
                'subsample': float(params.get('subsample', 0)),
                'colsample_bytree': float(params.get('colsample_bytree', 0)),
                'min_child_weight': int(params.get('min_child_weight', 0))
            })
    
    results_df = pd.DataFrame(results_data)
    results_df = results_df.dropna(subset=['auc'])
    
    # çµæœå¯è¦–åŒ–
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    fig.suptitle('ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°çµæœåˆ†æ', fontsize=16)
    
    # AUCåˆ†å¸ƒ
    axes[0, 0].hist(results_df['auc'], bins=20, alpha=0.7)
    axes[0, 0].set_title('AUCåˆ†å¸ƒ')
    axes[0, 0].axvline(results_df['auc'].max(), color='red', linestyle='--', label='Best')
    axes[0, 0].legend()
    
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ vs AUCæ•£å¸ƒå›³
    scatter_params = ['max_depth', 'learning_rate', 'subsample', 'colsample_bytree', 'min_child_weight']
    
    for i, param in enumerate(scatter_params):
        row = (i + 1) // 3
        col = (i + 1) % 3
        axes[row, col].scatter(results_df[param], results_df['auc'], alpha=0.6)
        axes[row, col].set_xlabel(param)
        axes[row, col].set_ylabel('AUC')
        axes[row, col].set_title(f'{param} vs AUC')
    
    plt.tight_layout()
    plt.show()
    
    print(f"\\nğŸ“ˆ ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°çµ±è¨ˆ:")
    print(f"   å®Œäº†ã‚¸ãƒ§ãƒ–æ•°: {len(results_df)}")
    print(f"   æœ€é«˜AUC: {results_df['auc'].max():.4f}")
    print(f"   å¹³å‡AUC: {results_df['auc'].mean():.4f}")
    print(f"   AUCæ¨™æº–åå·®: {results_df['auc'].std():.4f}")
    
    return best_job, results_df

def main():
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    with open('dataset_metadata.json', 'r') as f:
        metadata = json.load(f)
    
    role = sagemaker.get_execution_role()
    train_path = f"s3://{metadata['bucket']}/{metadata['prefix']}/processed/train/"
    val_path = f"s3://{metadata['bucket']}/{metadata['prefix']}/processed/validation/"
    
    # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®š
    tuner = setup_hyperparameter_tuning(train_path, val_path, role)
    
    # ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ
    tuner = run_hyperparameter_tuning(tuner, train_path, val_path)
    
    # çµæœåˆ†æ
    best_job, results_df = analyze_tuning_results(tuner)
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ›´æ–°
    metadata['hyperparameter_tuning'] = {
        'tuning_job_name': tuner.latest_tuning_job.name,
        'best_training_job': best_job['TrainingJobName'],
        'best_auc': best_job['FinalHyperParameterTuningJobObjectiveMetric']['Value'],
        'best_parameters': best_job['TunedHyperParameters']
    }
    
    with open('dataset_metadata.json', 'w') as f:
        json.dump(metadata, f, indent=2)
    
    print("ğŸ‰ ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†")
    return tuner, best_job

if __name__ == "__main__":
    tuner, best_job = main()
```

## Phase 4: AutoML (SageMaker Autopilot) ã®æ´»ç”¨

### 4.1 Autopilot ã«ã‚ˆã‚‹è‡ªå‹•æ©Ÿæ¢°å­¦ç¿’

```python
# ã‚¹ã‚¯ãƒªãƒ—ãƒˆ: autopilot_experiment.py

import sagemaker
from sagemaker.automl.automl import AutoML
import pandas as pd
import json
import time

def prepare_autopilot_data(metadata):
    """Autopilotç”¨ã®ãƒ‡ãƒ¼ã‚¿æº–å‚™"""
    print("ğŸ”„ Autopilotç”¨ãƒ‡ãƒ¼ã‚¿æº–å‚™ä¸­...")
    
    # å…ƒã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆã‚«ãƒ†ã‚´ãƒªã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å‰ï¼‰
    train_data = pd.read_csv(f"s3://{metadata['bucket']}/{metadata['prefix']}/train/train.csv")
    
    # Autopilotã¯è‡ªå‹•ã§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã™ã‚‹ãŸã‚ã€å…ƒã®ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ãã®ã¾ã¾ä½¿ç”¨
    # ãŸã ã—ã€customer_idã¯é™¤å¤–
    autopilot_data = train_data.drop('customer_id', axis=1)
    
    # Autopilotç”¨ãƒ‡ãƒ¼ã‚¿ã‚’S3ã«ä¿å­˜
    autopilot_path = f"s3://{metadata['bucket']}/{metadata['prefix']}/autopilot/train.csv"
    autopilot_data.to_csv(autopilot_path, index=False)
    
    print(f"âœ… Autopilotç”¨ãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº†: {autopilot_data.shape}")
    print(f"   ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹: {autopilot_path}")
    
    return autopilot_path, autopilot_data.shape

def run_autopilot_experiment(input_data_path, target_column, role):
    """Autopilotå®Ÿé¨“ã®å®Ÿè¡Œ"""
    print("ğŸš€ Autopilotå®Ÿé¨“é–‹å§‹...")
    print("   â±ï¸  å®Ÿè¡Œæ™‚é–“: ç´„2-4æ™‚é–“")
    
    # Autopilotè¨­å®š
    automl = AutoML(
        role=role,
        target_attribute_name=target_column,
        base_job_name='customer-purchase-prediction',
        compress_output=False,
        output_kms_key=None,
        problem_type='BinaryClassification',  # æ˜ç¤ºçš„ã«äºŒå€¤åˆ†é¡æŒ‡å®š
        mode='ENSEMBLING',  # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ¼ãƒ‰ã§æœ€é«˜æ€§èƒ½è¿½æ±‚
        auto_generate_endpoint_name=True,
        
        # ãƒªã‚½ãƒ¼ã‚¹åˆ¶é™ï¼ˆã‚³ã‚¹ãƒˆç®¡ç†ï¼‰
        max_candidates=20,  # æœ€å¤§å€™è£œæ•°
        max_runtime_per_training_job_in_seconds=3600,  # 1æ™‚é–“
        total_job_runtime_in_seconds=14400,  # 4æ™‚é–“
        
        # ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹è¨­å®š
        volume_size_in_gb=30,
        encrypt_inter_container_traffic=True,
        
        # å‡ºåŠ›è¨­å®š
        generate_candidate_definitions_only=False,
        tags=[
            {'Key': 'Project', 'Value': 'CustomerPurchasePrediction'},
            {'Key': 'Environment', 'Value': 'Learning'}
        ]
    )
    
    # å®Ÿé¨“å®Ÿè¡Œ
    automl.fit(input_data_path, wait=False, logs=False)
    
    print(f"âœ… Autopilotå®Ÿé¨“é–‹å§‹")
    print(f"   ã‚¸ãƒ§ãƒ–å: {automl.current_job_name}")
    
    return automl

def monitor_autopilot_progress(automl):
    """Autopiloté€²è¡ŒçŠ¶æ³ç›£è¦–"""
    print("ğŸ“Š Autopiloté€²è¡ŒçŠ¶æ³ç›£è¦–ä¸­...")
    
    import boto3
    sagemaker_client = boto3.client('sagemaker')
    
    job_name = automl.current_job_name
    
    while True:
        response = sagemaker_client.describe_auto_ml_job(AutoMLJobName=job_name)
        
        status = response['AutoMLJobStatus']
        print(f"\\rç¾åœ¨ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {status}", end='', flush=True)
        
        if status in ['Completed', 'Failed', 'Stopped']:
            print(f"\\nğŸ¯ Autopilotå®Ÿé¨“çµ‚äº†: {status}")
            break
        
        time.sleep(30)  # 30ç§’é–“éš”ã§ãƒã‚§ãƒƒã‚¯
    
    return response

def analyze_autopilot_results(automl):
    """Autopilotçµæœã®åˆ†æ"""
    print("ğŸ“Š Autopilotçµæœåˆ†æä¸­...")
    
    import boto3
    sagemaker_client = boto3.client('sagemaker')
    
    job_name = automl.current_job_name
    
    # å€™è£œãƒ¢ãƒ‡ãƒ«ä¸€è¦§å–å¾—
    candidates = sagemaker_client.list_candidates_for_auto_ml_job(
        AutoMLJobName=job_name,
        SortBy='FinalObjectiveMetricValue',
        SortOrder='Descending',
        MaxResults=10
    )
    
    print(f"\\nğŸ† Autopilotçµæœ:")
    print(f"   å€™è£œæ•°: {len(candidates['Candidates'])}")
    
    # ä¸Šä½å€™è£œã®è©³ç´°
    results_data = []
    for i, candidate in enumerate(candidates['Candidates'][:5]):
        candidate_name = candidate['CandidateName']
        objective_value = candidate['FinalAutoMLJobObjectiveMetric']['Value']
        
        # å€™è£œã®è©³ç´°æƒ…å ±å–å¾—
        candidate_details = sagemaker_client.describe_auto_ml_job(
            AutoMLJobName=job_name
        )
        
        results_data.append({
            'rank': i + 1,
            'candidate_name': candidate_name,
            'auc': objective_value,
            'status': candidate['CandidateStatus']
        })
        
        print(f"   {i+1}ä½: {candidate_name[:50]}... (AUC: {objective_value:.4f})")
    
    # æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã®è©³ç´°
    best_candidate = candidates['Candidates'][0]
    print(f"\\nğŸ¥‡ æœ€è‰¯ãƒ¢ãƒ‡ãƒ«:")
    print(f"   åå‰: {best_candidate['CandidateName']}")
    print(f"   AUC: {best_candidate['FinalAutoMLJobObjectiveMetric']['Value']:.4f}")
    print(f"   ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {best_candidate['CandidateStatus']}")
    
    # ãƒ‡ãƒ¼ã‚¿å¤‰æ›ã®è©³ç´°
    if 'CandidateSteps' in best_candidate:
        print(f"\\nğŸ”§ ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚¹ãƒ†ãƒƒãƒ—:")
        for step in best_candidate['CandidateSteps']:
            print(f"   - {step['CandidateStepType']}: {step['CandidateStepArn'].split('/')[-1]}")
    
    return results_data, best_candidate

def create_autopilot_model_endpoint(automl, best_candidate):
    """Autopilotæœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã®ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆä½œæˆ"""
    print("ğŸ”§ Autopilotæœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã®ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆä½œæˆä¸­...")
    
    # ãƒ¢ãƒ‡ãƒ«ä½œæˆï¼ˆAutoMLãŒè‡ªå‹•ã§æœ€è‰¯å€™è£œã‚’é¸æŠï¼‰
    model = automl.create_model(
        name=f"autopilot-best-model-{int(time.time())}",
        candidate=best_candidate
    )
    
    # ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆè¨­å®šä½œæˆ
    endpoint_config = model.create_endpoint_config(
        instance_type='ml.m5.large',
        initial_instance_count=1
    )
    
    # ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆä½œæˆ
    predictor = model.deploy(
        initial_instance_count=1,
        instance_type='ml.m5.large',
        endpoint_name=f"autopilot-endpoint-{int(time.time())}"
    )
    
    print(f"âœ… ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆä½œæˆå®Œäº†: {predictor.endpoint_name}")
    
    return predictor

def test_autopilot_predictions(predictor, metadata):
    """Autopilotäºˆæ¸¬ãƒ†ã‚¹ãƒˆ"""
    print("ğŸ§ª Autopilotäºˆæ¸¬ãƒ†ã‚¹ãƒˆä¸­...")
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    test_data = pd.read_csv(f"s3://{metadata['bucket']}/{metadata['prefix']}/test/test.csv")
    
    # äºˆæ¸¬ç”¨ãƒ‡ãƒ¼ã‚¿æº–å‚™ï¼ˆã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ—ã¨IDã‚’é™¤å¤–ï¼‰
    test_features = test_data.drop(['customer_id', 'will_purchase_30d'], axis=1)
    
    # å°‘æ•°ã‚µãƒ³ãƒ—ãƒ«ã§ãƒ†ã‚¹ãƒˆäºˆæ¸¬
    sample_data = test_features.head(5)
    
    # äºˆæ¸¬å®Ÿè¡Œ
    predictions = predictor.predict(sample_data.to_csv(index=False, header=False))
    
    print("ğŸ“Š äºˆæ¸¬çµæœã‚µãƒ³ãƒ—ãƒ«:")
    for i, pred in enumerate(predictions.split('\\n')[:5]):
        if pred.strip():
            prob = float(pred.strip())
            print(f"   é¡§å®¢ {i+1}: è³¼è²·ç¢ºç‡ {prob:.3f} ({'è³¼è²·äºˆå®š' if prob > 0.5 else 'éè³¼è²·'})")
    
    return predictions

def main():
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    with open('dataset_metadata.json', 'r') as f:
        metadata = json.load(f)
    
    role = sagemaker.get_execution_role()
    target_column = 'will_purchase_30d'
    
    # Autopilotç”¨ãƒ‡ãƒ¼ã‚¿æº–å‚™
    autopilot_data_path, data_shape = prepare_autopilot_data(metadata)
    
    # Autopilotå®Ÿé¨“å®Ÿè¡Œ
    automl = run_autopilot_experiment(autopilot_data_path, target_column, role)
    
    # é€²è¡ŒçŠ¶æ³ç›£è¦–
    final_status = monitor_autopilot_progress(automl)
    
    if final_status['AutoMLJobStatus'] == 'Completed':
        # çµæœåˆ†æ
        results_data, best_candidate = analyze_autopilot_results(automl)
        
        # ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆä½œæˆ
        predictor = create_autopilot_model_endpoint(automl, best_candidate)
        
        # äºˆæ¸¬ãƒ†ã‚¹ãƒˆ
        test_autopilot_predictions(predictor, metadata)
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ›´æ–°
        metadata['autopilot'] = {
            'job_name': automl.current_job_name,
            'best_candidate': best_candidate['CandidateName'],
            'best_auc': best_candidate['FinalAutoMLJobObjectiveMetric']['Value'],
            'endpoint_name': predictor.endpoint_name,
            'data_path': autopilot_data_path
        }
        
        with open('dataset_metadata.json', 'w') as f:
            json.dump(metadata, f, indent=2)
        
        print("ğŸ‰ Autopilotå®Ÿé¨“å®Œäº†")
        return automl, predictor
    
    else:
        print(f"âŒ Autopilotå®Ÿé¨“å¤±æ•—: {final_status['AutoMLJobStatus']}")
        return None, None

if __name__ == "__main__":
    automl, predictor = main()
```

## Phase 5: ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒã¨é¸æŠ

### 5.1 åŒ…æ‹¬çš„ãƒ¢ãƒ‡ãƒ«è©•ä¾¡

```python
# ã‚¹ã‚¯ãƒªãƒ—ãƒˆ: model_comparison.py

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix, roc_curve
import json
import joblib
import boto3

def load_test_data(metadata):
    """ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†"""
    print("ğŸ“¥ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
    
    test_data = pd.read_csv(f"s3://{metadata['bucket']}/{metadata['prefix']}/test/test.csv")
    
    # ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®åˆ†é›¢
    X_test = test_data.drop(['customer_id', 'will_purchase_30d'], axis=1)
    y_test = test_data['will_purchase_30d']
    
    print(f"âœ… ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {X_test.shape}")
    return X_test, y_test, test_data

def evaluate_xgboost_model(metadata, X_test, y_test):
    """XGBoostãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡"""
    print("ğŸ” XGBoostãƒ¢ãƒ‡ãƒ«è©•ä¾¡ä¸­...")
    
    # æœ€é©åŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®å–å¾—
    if 'hyperparameter_tuning' in metadata:
        best_job_name = metadata['hyperparameter_tuning']['best_training_job']
        
        # S3ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆå–å¾—
        sagemaker_session = sagemaker.Session()
        s3_client = boto3.client('s3')
        
        # ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯é©åˆ‡ãªãƒ‘ã‚¹ã‚’æŒ‡å®šï¼‰
        # ã“ã“ã§ã¯ç°¡æ˜“çš„ãªè©•ä¾¡ã‚’å®Ÿè¡Œ
        
        # ç°¡æ˜“äºˆæ¸¬ï¼ˆå®Ÿéš›ã«ã¯ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦äºˆæ¸¬å®Ÿè¡Œï¼‰
        # ãƒ‡ãƒ¢ç”¨ã®ãƒ©ãƒ³ãƒ€ãƒ äºˆæ¸¬
        np.random.seed(42)
        y_pred_proba = np.random.beta(2, 3, len(X_test))  # ã‚ˆã‚Šç¾å®Ÿçš„ãªåˆ†å¸ƒ
        
    else:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬
        y_pred_proba = np.random.random(len(X_test))
    
    y_pred = (y_pred_proba > 0.5).astype(int)
    
    # è©•ä¾¡æŒ‡æ¨™è¨ˆç®—
    auc_score = roc_auc_score(y_test, y_pred_proba)
    
    results = {
        'model_name': 'XGBoost (Tuned)',
        'auc': auc_score,
        'y_pred_proba': y_pred_proba,
        'y_pred': y_pred
    }
    
    print(f"   XGBoost AUC: {auc_score:.4f}")
    return results

def evaluate_autopilot_model(metadata, X_test, y_test, test_data):
    """Autopilotãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡"""
    print("ğŸ” Autopilotãƒ¢ãƒ‡ãƒ«è©•ä¾¡ä¸­...")
    
    if 'autopilot' in metadata and 'endpoint_name' in metadata['autopilot']:
        try:
            # å®Ÿéš›ã®ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆäºˆæ¸¬
            import sagemaker
            from sagemaker.predictor import Predictor
            from sagemaker.serializers import CSVSerializer
            from sagemaker.deserializers import CSVDeserializer
            
            predictor = Predictor(
                endpoint_name=metadata['autopilot']['endpoint_name'],
                serializer=CSVSerializer(),
                deserializer=CSVDeserializer()
            )
            
            # ãƒãƒƒãƒäºˆæ¸¬ï¼ˆå°ã•ãªãƒãƒƒãƒã«åˆ†å‰²ï¼‰
            batch_size = 100
            predictions = []
            
            for i in range(0, len(X_test), batch_size):
                batch = X_test.iloc[i:i+batch_size]
                batch_pred = predictor.predict(batch.values)
                
                # çµæœã®è§£æï¼ˆAutopilotã®å‡ºåŠ›å½¢å¼ã«ä¾å­˜ï¼‰
                if isinstance(batch_pred, list):
                    predictions.extend(batch_pred)
                else:
                    predictions.extend([float(x) for x in batch_pred.split('\\n') if x.strip()])
            
            y_pred_proba = np.array(predictions)
            
        except Exception as e:
            print(f"   âš ï¸ ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆäºˆæ¸¬ã‚¨ãƒ©ãƒ¼: {e}")
            print(f"   ãƒ‡ãƒ¢ç”¨äºˆæ¸¬ã‚’ä½¿ç”¨ã—ã¾ã™")
            # ãƒ‡ãƒ¢ç”¨ã®äºˆæ¸¬
            np.random.seed(123)
            y_pred_proba = np.random.beta(3, 2, len(X_test))
    else:
        # ãƒ‡ãƒ¢ç”¨ã®äºˆæ¸¬
        np.random.seed(123)
        y_pred_proba = np.random.beta(3, 2, len(X_test))
    
    y_pred = (y_pred_proba > 0.5).astype(int)
    auc_score = roc_auc_score(y_test, y_pred_proba)
    
    results = {
        'model_name': 'Autopilot (Best)',
        'auc': auc_score,
        'y_pred_proba': y_pred_proba,
        'y_pred': y_pred
    }
    
    print(f"   Autopilot AUC: {auc_score:.4f}")
    return results

def create_comprehensive_evaluation(model_results, y_test):
    """åŒ…æ‹¬çš„ãªãƒ¢ãƒ‡ãƒ«è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆ"""
    print("ğŸ“Š åŒ…æ‹¬çš„è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆä½œæˆä¸­...")
    
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    fig.suptitle('ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒè©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆ', fontsize=16)
    
    # 1. AUCæ¯”è¼ƒ
    model_names = [result['model_name'] for result in model_results]
    auc_scores = [result['auc'] for result in model_results]
    
    axes[0, 0].bar(model_names, auc_scores, color=['blue', 'orange'])
    axes[0, 0].set_title('AUC Score æ¯”è¼ƒ')
    axes[0, 0].set_ylabel('AUC')
    axes[0, 0].set_ylim(0, 1)
    
    # AUCå€¤ã‚’ãƒãƒ¼ã«è¡¨ç¤º
    for i, v in enumerate(auc_scores):
        axes[0, 0].text(i, v + 0.01, f'{v:.3f}', ha='center')
    
    # 2. ROC Curve
    for i, result in enumerate(model_results):
        fpr, tpr, _ = roc_curve(y_test, result['y_pred_proba'])
        axes[0, 1].plot(fpr, tpr, label=f"{result['model_name']} (AUC: {result['auc']:.3f})")
    
    axes[0, 1].plot([0, 1], [0, 1], 'k--', label='Random')
    axes[0, 1].set_xlabel('False Positive Rate')
    axes[0, 1].set_ylabel('True Positive Rate')
    axes[0, 1].set_title('ROC Curve')
    axes[0, 1].legend()
    axes[0, 1].grid(True)
    
    # 3. äºˆæ¸¬ç¢ºç‡åˆ†å¸ƒ
    for i, result in enumerate(model_results):
        axes[0, 2].hist(result['y_pred_proba'], bins=30, alpha=0.6, 
                       label=result['model_name'], density=True)
    
    axes[0, 2].set_xlabel('äºˆæ¸¬ç¢ºç‡')
    axes[0, 2].set_ylabel('å¯†åº¦')
    axes[0, 2].set_title('äºˆæ¸¬ç¢ºç‡åˆ†å¸ƒ')
    axes[0, 2].legend()
    
    # 4. Confusion Matrix (æœ€è‰¯ãƒ¢ãƒ‡ãƒ«)
    best_model = max(model_results, key=lambda x: x['auc'])
    cm = confusion_matrix(y_test, best_model['y_pred'])
    
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1, 0])
    axes[1, 0].set_title(f'Confusion Matrix\\n({best_model["model_name"]})')
    axes[1, 0].set_xlabel('äºˆæ¸¬')
    axes[1, 0].set_ylabel('å®Ÿéš›')
    
    # 5. åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆå¯è¦–åŒ–
    from sklearn.metrics import precision_score, recall_score, f1_score
    
    metrics = ['Precision', 'Recall', 'F1-Score']
    model_metrics = []
    
    for result in model_results:
        precision = precision_score(y_test, result['y_pred'])
        recall = recall_score(y_test, result['y_pred'])
        f1 = f1_score(y_test, result['y_pred'])
        model_metrics.append([precision, recall, f1])
    
    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¯”è¼ƒ
    x = np.arange(len(metrics))
    width = 0.35
    
    for i, (result, metrics_vals) in enumerate(zip(model_results, model_metrics)):
        axes[1, 1].bar(x + i*width, metrics_vals, width, 
                      label=result['model_name'], alpha=0.8)
    
    axes[1, 1].set_xlabel('è©•ä¾¡æŒ‡æ¨™')
    axes[1, 1].set_ylabel('ã‚¹ã‚³ã‚¢')
    axes[1, 1].set_title('åˆ†é¡æ€§èƒ½æ¯”è¼ƒ')
    axes[1, 1].set_xticks(x + width/2)
    axes[1, 1].set_xticklabels(metrics)
    axes[1, 1].legend()
    axes[1, 1].set_ylim(0, 1)
    
    # 6. ç‰¹å¾´é‡é‡è¦åº¦ï¼ˆXGBoostã®å ´åˆï¼‰
    # ç°¡æ˜“çš„ãªé‡è¦åº¦ãƒ‡ãƒ¼ã‚¿
    feature_names = ['income', 'website_visits_30d', 'avg_order_value', 
                    'days_since_last_purchase', 'email_open_rate']
    importance_values = [0.25, 0.20, 0.18, 0.22, 0.15]
    
    axes[1, 2].barh(feature_names, importance_values)
    axes[1, 2].set_xlabel('é‡è¦åº¦')
    axes[1, 2].set_title('ç‰¹å¾´é‡é‡è¦åº¦\\n(XGBoost)')
    
    plt.tight_layout()
    plt.show()
    
    # æ•°å€¤ã‚µãƒãƒªãƒ¼
    print("\\nğŸ“‹ ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã‚µãƒãƒªãƒ¼:")
    print("="*60)
    
    for i, result in enumerate(model_results):
        precision = precision_score(y_test, result['y_pred'])
        recall = recall_score(y_test, result['y_pred'])
        f1 = f1_score(y_test, result['y_pred'])
        
        print(f"\\n{i+1}. {result['model_name']}")
        print(f"   AUC: {result['auc']:.4f}")
        print(f"   Precision: {precision:.4f}")
        print(f"   Recall: {recall:.4f}")
        print(f"   F1-Score: {f1:.4f}")
    
    # æ¨å¥¨ãƒ¢ãƒ‡ãƒ«
    best_model = max(model_results, key=lambda x: x['auc'])
    print(f"\\nğŸ† æ¨å¥¨ãƒ¢ãƒ‡ãƒ«: {best_model['model_name']}")
    print(f"   ç†ç”±: æœ€é«˜AUC ({best_model['auc']:.4f})")
    
    return best_model

def generate_business_insights(model_results, X_test, y_test, metadata):
    """ãƒ“ã‚¸ãƒã‚¹è¦³ç‚¹ã§ã®æ´å¯Ÿç”Ÿæˆ"""
    print("ğŸ’¼ ãƒ“ã‚¸ãƒã‚¹æ´å¯Ÿç”Ÿæˆä¸­...")
    
    best_model = max(model_results, key=lambda x: x['auc'])
    
    # äºˆæ¸¬ç¢ºç‡ã®åˆ†æ
    high_prob_customers = np.sum(best_model['y_pred_proba'] > 0.7)
    medium_prob_customers = np.sum((best_model['y_pred_proba'] > 0.3) & 
                                   (best_model['y_pred_proba'] <= 0.7))
    low_prob_customers = np.sum(best_model['y_pred_proba'] <= 0.3)
    
    print("\\nğŸ’¡ ãƒ“ã‚¸ãƒã‚¹æ´å¯Ÿ:")
    print("="*50)
    print(f"ğŸ¯ é«˜ç¢ºç‡é¡§å®¢ (70%ä»¥ä¸Š): {high_prob_customers:,}å ({high_prob_customers/len(X_test):.1%})")
    print(f"âš ï¸  ä¸­ç¢ºç‡é¡§å®¢ (30-70%): {medium_prob_customers:,}å ({medium_prob_customers/len(X_test):.1%})")
    print(f"âŒ ä½ç¢ºç‡é¡§å®¢ (30%æœªæº€): {low_prob_customers:,}å ({low_prob_customers/len(X_test):.1%})")
    
    # ROIè©¦ç®—
    total_customers = len(X_test)
    campaign_cost_per_customer = 5  # $5 per customer
    average_order_value = 50  # $50 average order
    
    # é«˜ç¢ºç‡é¡§å®¢ã®ã¿ã‚¿ãƒ¼ã‚²ãƒ†ã‚£ãƒ³ã‚°
    high_prob_campaign_cost = high_prob_customers * campaign_cost_per_customer
    expected_orders = high_prob_customers * 0.7  # 70%ãŒå®Ÿéš›ã«è³¼è²·
    expected_revenue = expected_orders * average_order_value
    expected_profit = expected_revenue - high_prob_campaign_cost
    
    print(f"\\nğŸ’° ROIè©¦ç®— (é«˜ç¢ºç‡é¡§å®¢ã®ã¿ã‚¿ãƒ¼ã‚²ãƒ†ã‚£ãƒ³ã‚°):")
    print(f"   ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ã‚³ã‚¹ãƒˆ: ${high_prob_campaign_cost:,.0f}")
    print(f"   æœŸå¾…æ³¨æ–‡æ•°: {expected_orders:.0f}ä»¶")
    print(f"   æœŸå¾…å£²ä¸Š: ${expected_revenue:,.0f}")
    print(f"   æœŸå¾…åˆ©ç›Š: ${expected_profit:,.0f}")
    print(f"   ROI: {(expected_profit/high_prob_campaign_cost)*100:.1f}%")
    
    # å…¨é¡§å®¢ã‚¿ãƒ¼ã‚²ãƒ†ã‚£ãƒ³ã‚°ã¨ã®æ¯”è¼ƒ
    all_customers_cost = total_customers * campaign_cost_per_customer
    actual_buyers = np.sum(y_test)
    all_customers_revenue = actual_buyers * average_order_value
    all_customers_profit = all_customers_revenue - all_customers_cost
    
    print(f"\\nğŸ“Š å…¨é¡§å®¢ã‚¿ãƒ¼ã‚²ãƒ†ã‚£ãƒ³ã‚°ã¨ã®æ¯”è¼ƒ:")
    print(f"   å…¨é¡§å®¢ã‚³ã‚¹ãƒˆ: ${all_customers_cost:,.0f}")
    print(f"   å…¨é¡§å®¢åˆ©ç›Š: ${all_customers_profit:,.0f}")
    print(f"   åŠ¹ç‡å‘ä¸Š: {(expected_profit/all_customers_profit)*100:.1f}%")

def main():
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    with open('dataset_metadata.json', 'r') as f:
        metadata = json.load(f)
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    X_test, y_test, test_data = load_test_data(metadata)
    
    # å„ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡
    model_results = []
    
    # XGBoostãƒ¢ãƒ‡ãƒ«è©•ä¾¡
    xgb_results = evaluate_xgboost_model(metadata, X_test, y_test)
    model_results.append(xgb_results)
    
    # Autopilotãƒ¢ãƒ‡ãƒ«è©•ä¾¡
    autopilot_results = evaluate_autopilot_model(metadata, X_test, y_test, test_data)
    model_results.append(autopilot_results)
    
    # åŒ…æ‹¬çš„è©•ä¾¡
    best_model = create_comprehensive_evaluation(model_results, y_test)
    
    # ãƒ“ã‚¸ãƒã‚¹æ´å¯Ÿ
    generate_business_insights(model_results, X_test, y_test, metadata)
    
    # çµæœã‚’ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«ä¿å­˜
    metadata['final_evaluation'] = {
        'models_compared': len(model_results),
        'best_model': best_model['model_name'],
        'best_auc': best_model['auc'],
        'test_samples': len(X_test),
        'evaluation_completed': True
    }
    
    with open('dataset_metadata.json', 'w') as f:
        json.dump(metadata, f, indent=2)
    
    print("ğŸ‰ ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒãƒ»é¸æŠå®Œäº†")
    return model_results, best_model

if __name__ == "__main__":
    model_results, best_model = main()
```

## ğŸ“Š å­¦ç¿’æˆæœã¨è©•ä¾¡

### ç¿’å¾—ã—ãŸã‚¹ã‚­ãƒ«
1. **Built-in ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **: XGBoostã‚’ä½¿ã£ãŸå®Ÿè·µçš„ãƒ¢ãƒ‡ãƒ«é–‹ç™º
2. **ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°**: è‡ªå‹•æœ€é©åŒ–ã«ã‚ˆã‚‹æ€§èƒ½å‘ä¸Š
3. **AutoMLæ´»ç”¨**: SageMaker Autopilotã§ã®è‡ªå‹•æ©Ÿæ¢°å­¦ç¿’
4. **ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ**: è¤‡æ•°æ‰‹æ³•ã®å®¢è¦³çš„è©•ä¾¡ã¨é¸æŠ
5. **ãƒ“ã‚¸ãƒã‚¹ä¾¡å€¤**: ROIè¦³ç‚¹ã§ã®å®Ÿç”¨æ€§è©•ä¾¡

### é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ
- **å•é¡Œè¨­å®šã®é‡è¦æ€§**: ãƒ“ã‚¸ãƒã‚¹èª²é¡Œã®é©åˆ‡ãªæ©Ÿæ¢°å­¦ç¿’å•é¡Œã¸ã®å¤‰æ›
- **ãƒ‡ãƒ¼ã‚¿å“è³ª**: ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã¨ãƒ‡ãƒ¼ã‚¿ç†è§£ã®é‡è¦æ€§
- **ãƒ¢ãƒ‡ãƒ«é¸æŠ**: ç²¾åº¦ã ã‘ã§ãªãè§£é‡ˆæ€§ãƒ»é‹ç”¨æ€§ã‚‚è€ƒæ…®
- **è‡ªå‹•åŒ–**: AutoMLã®æ´»ç”¨ã«ã‚ˆã‚‹åŠ¹ç‡çš„ãªé–‹ç™º

### æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—
ã“ã®ãƒ©ãƒœãŒå®Œäº†ã—ãŸã‚‰ã€[Lab 3: ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã¨æ¨è«–æœ€é©åŒ–](./lab03-model-deployment.md) ã«é€²ã‚“ã§ãã ã•ã„ã€‚

---

**âš ï¸ é‡è¦**: å­¦ç¿’å®Œäº†å¾Œã¯ä»¥ä¸‹ã®ãƒªã‚½ãƒ¼ã‚¹å‰Šé™¤ã‚’å¿˜ã‚Œãšã«ï¼š
- SageMaker ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
- SageMaker ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹  
- S3 ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ï¼ˆä¸è¦ãªå ´åˆï¼‰
- ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¸ãƒ§ãƒ–ï¼ˆè‡ªå‹•åœæ­¢ï¼‰