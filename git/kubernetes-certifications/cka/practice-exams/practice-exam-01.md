# CKA Practice Exam 1 - ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ç®¡ç†ã¨ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰

## ğŸ“‹ è©¦é¨“æƒ…å ±

**æ™‚é–“åˆ¶é™**: 120åˆ†  
**å•é¡Œæ•°**: 100å•  
**åˆæ ¼ç‚¹**: 66%  
**ç’°å¢ƒ**: Kubernetes v1.28  

**é‡è¦ãªæ³¨æ„äº‹é …:**
- å®Ÿéš›ã®Kubernetesã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã§ã®å®ŸæŠ€è©¦é¨“ã§ã™
- å„å•é¡Œã§æŒ‡å®šã•ã‚ŒãŸã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã¨namespaceã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„
- ã™ã¹ã¦ã®YAMLãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆã¯`/opt/candidate/`ã«ä¿å­˜ã—ã¦ãã ã•ã„
- è©¦é¨“ä¸­ã¯ [kubernetes.io](https://kubernetes.io) ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒå‚ç…§å¯èƒ½ã§ã™

---

## ğŸ¯ Question 1: ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æƒ…å ±ç¢ºèª (2%)

**Context**: cluster: k8s-cluster-1  
**Task**: 
ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®è©³ç´°æƒ…å ±ã‚’ç¢ºèªã—ã€ä»¥ä¸‹ã®æƒ…å ±ã‚’`/opt/candidate/cluster-info.txt`ã«ä¿å­˜ã—ã¦ãã ã•ã„ï¼š
- ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼å†…ã®ãƒãƒ¼ãƒ‰æ•°
- å„ãƒãƒ¼ãƒ‰ã®Kubernetesãƒãƒ¼ã‚¸ãƒ§ãƒ³
- å„ãƒãƒ¼ãƒ‰ã®å†…éƒ¨IP
- ãƒã‚¹ã‚¿ãƒ¼ãƒãƒ¼ãƒ‰ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼IP

<details>
<summary>ğŸ’¡ è§£ç­”ä¾‹</summary>

```bash
# ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æƒ…å ±ã‚’ç¢ºèª
kubectl cluster-info

# ãƒãƒ¼ãƒ‰æƒ…å ±ã‚’ç¢ºèª
kubectl get nodes -o wide

# ãƒãƒ¼ãƒ‰è©³ç´°æƒ…å ±ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
echo "=== Cluster Information ===" > /opt/candidate/cluster-info.txt
echo "Total Nodes: $(kubectl get nodes --no-headers | wc -l)" >> /opt/candidate/cluster-info.txt
echo "" >> /opt/candidate/cluster-info.txt

echo "=== Node Details ===" >> /opt/candidate/cluster-info.txt
kubectl get nodes -o custom-columns=NAME:.metadata.name,VERSION:.status.nodeInfo.kubeletVersion,INTERNAL-IP:.status.addresses[?(@.type==\"InternalIP\")].address --no-headers >> /opt/candidate/cluster-info.txt

echo "" >> /opt/candidate/cluster-info.txt
echo "=== Master Node Cluster IP ===" >> /opt/candidate/cluster-info.txt
kubectl cluster-info | grep "Kubernetes control plane" >> /opt/candidate/cluster-info.txt
```

**æ¡ç‚¹ãƒã‚¤ãƒ³ãƒˆ:**
- ãƒãƒ¼ãƒ‰æ•°ã®æ­£ç¢ºãªè¨˜éŒ² (25%)
- Kubernetesãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®è¨˜éŒ² (25%)
- å†…éƒ¨IPã‚¢ãƒ‰ãƒ¬ã‚¹ã®è¨˜éŒ² (25%)
- ãƒã‚¹ã‚¿ãƒ¼ãƒãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼IPã®è¨˜éŒ² (25%)
</details>

---

## ğŸ¯ Question 2: etcdãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ (7%)

**Context**: cluster: k8s-cluster-1  
**Task**: 
etcdã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’ä½œæˆã—ã€`/opt/candidate/etcd-backup.db`ã¨ã—ã¦ä¿å­˜ã—ã¦ãã ã•ã„ã€‚
etcdã¯ `https://127.0.0.1:2379` ã§ãƒªãƒƒã‚¹ãƒ³ã—ã¦ãŠã‚Šã€ä»¥ä¸‹ã®è¨¼æ˜æ›¸ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ï¼š
- CAè¨¼æ˜æ›¸: `/etc/kubernetes/pki/etcd/ca.crt`
- Clientè¨¼æ˜æ›¸: `/etc/kubernetes/pki/etcd/server.crt`
- Clientç§˜å¯†éµ: `/etc/kubernetes/pki/etcd/server.key`

<details>
<summary>ğŸ’¡ è§£ç­”ä¾‹</summary>

```bash
# etcdctl ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
which etcdctl

# etcdctl ãŒãªã„å ´åˆã¯ã€etcd podå†…ã§å®Ÿè¡Œ
kubectl exec -it etcd-<master-node-name> -n kube-system -- sh

# etcdãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®ä½œæˆ
ETCDCTL_API=3 etcdctl snapshot save /opt/candidate/etcd-backup.db \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key

# ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®æ¤œè¨¼
ETCDCTL_API=3 etcdctl snapshot status /opt/candidate/etcd-backup.db --write-out=table
```

**æ¡ç‚¹ãƒã‚¤ãƒ³ãƒˆ:**
- æ­£ã—ã„etcdctl APIãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®ä½¿ç”¨ (20%)
- æ­£ã—ã„ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®æŒ‡å®š (20%)
- æ­£ã—ã„è¨¼æ˜æ›¸ãƒ•ã‚¡ã‚¤ãƒ«ã®ä½¿ç”¨ (30%)
- æŒ‡å®šã•ã‚ŒãŸãƒ‘ã‚¹ã§ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ (30%)
</details>

---

## ğŸ¯ Question 3: ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒãƒ¼ãƒ‰ã®è¿½åŠ  (8%)

**Context**: cluster: k8s-cluster-1  
**Task**: 
æ–°ã—ã„ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒãƒ¼ãƒ‰ `worker-node-new` ã‚’ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã«è¿½åŠ ã—ã¦ãã ã•ã„ã€‚
ãƒãƒ¼ãƒ‰ã¯äº‹å‰ã«æº–å‚™ã•ã‚Œã¦ãŠã‚Šã€å¿…è¦ãªã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆï¼ˆkubeletã€kubeadmã€kubectlï¼‰ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿ã§ã™ã€‚
è¿½åŠ å¾Œã€ãƒãƒ¼ãƒ‰ãŒ `Ready` çŠ¶æ…‹ã«ãªã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚

<details>
<summary>ğŸ’¡ è§£ç­”ä¾‹</summary>

```bash
# ãƒã‚¹ã‚¿ãƒ¼ãƒãƒ¼ãƒ‰ã§joinã‚³ãƒãƒ³ãƒ‰ã‚’ç”Ÿæˆ
kubeadm token create --print-join-command

# ã¾ãŸã¯ã€æ—¢å­˜ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç¢ºèª
kubeadm token list

# CAè¨¼æ˜æ›¸ãƒãƒƒã‚·ãƒ¥ã‚’å–å¾—ï¼ˆå¿…è¦ãªå ´åˆï¼‰
openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //'

# worker-node-new ã«SSHã—ã¦ã€joinã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œ
ssh worker-node-new
sudo kubeadm join <master-ip>:6443 --token <token> --discovery-token-ca-cert-hash sha256:<hash>

# ãƒã‚¹ã‚¿ãƒ¼ãƒãƒ¼ãƒ‰ã§ãƒãƒ¼ãƒ‰è¿½åŠ ã‚’ç¢ºèª
kubectl get nodes

# ãƒãƒ¼ãƒ‰ãŒReadyçŠ¶æ…‹ã«ãªã‚‹ã¾ã§å¾…æ©Ÿ
kubectl wait --for=condition=Ready node/worker-node-new --timeout=300s
```

**æ¡ç‚¹ãƒã‚¤ãƒ³ãƒˆ:**
- æ­£ã—ã„joinã‚³ãƒãƒ³ãƒ‰ã®ç”Ÿæˆã¨å®Ÿè¡Œ (40%)
- ãƒãƒ¼ãƒ‰ãŒã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã«æ­£å¸¸ã«è¿½åŠ  (40%)
- ãƒãƒ¼ãƒ‰ãŒReadyçŠ¶æ…‹ã«ãªã‚‹ã“ã¨ (20%)
</details>

---

## ğŸ¯ Question 4: Deploymentã®ä½œæˆã¨ç®¡ç† (5%)

**Context**: cluster: k8s-cluster-1, namespace: web-app  
**Task**: 
`web-app` namespaceã«ä»¥ä¸‹ã®è¦ä»¶ã§Deploymentã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š
- åå‰: `nginx-deployment`
- ã‚¤ãƒ¡ãƒ¼ã‚¸: `nginx:1.20`
- ãƒ¬ãƒ—ãƒªã‚«æ•°: 3
- ãƒªã‚½ãƒ¼ã‚¹è¦æ±‚: CPU 100m, ãƒ¡ãƒ¢ãƒª 128Mi
- ãƒªã‚½ãƒ¼ã‚¹åˆ¶é™: CPU 500m, ãƒ¡ãƒ¢ãƒª 256Mi
- ãƒãƒ¼ãƒˆ: 80

ä½œæˆå¾Œã€ãƒ¬ãƒ—ãƒªã‚«æ•°ã‚’5ã«å¢—ã‚„ã—ã¦ãã ã•ã„ã€‚

<details>
<summary>ğŸ’¡ è§£ç­”ä¾‹</summary>

```bash
# namespaceã‚’ä½œæˆï¼ˆå­˜åœ¨ã—ãªã„å ´åˆï¼‰
kubectl create namespace web-app

# Deploymentã‚’ä½œæˆ
kubectl create deployment nginx-deployment --image=nginx:1.20 -n web-app

# Deploymentã®è¨­å®šã‚’æ›´æ–°ï¼ˆãƒªã‚½ãƒ¼ã‚¹è¦æ±‚ãƒ»åˆ¶é™ã€ãƒ¬ãƒ—ãƒªã‚«æ•°ï¼‰
kubectl patch deployment nginx-deployment -n web-app -p '{
  "spec": {
    "replicas": 3,
    "template": {
      "spec": {
        "containers": [{
          "name": "nginx",
          "resources": {
            "requests": {
              "cpu": "100m",
              "memory": "128Mi"
            },
            "limits": {
              "cpu": "500m",
              "memory": "256Mi"
            }
          },
          "ports": [{
            "containerPort": 80
          }]
        }]
      }
    }
  }
}'

# ã¾ãŸã¯ã€YAMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
cat <<EOF > /opt/candidate/nginx-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  namespace: web-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx-deployment
  template:
    metadata:
      labels:
        app: nginx-deployment
    spec:
      containers:
      - name: nginx
        image: nginx:1.20
        ports:
        - containerPort: 80
        resources:
          requests:
            cpu: "100m"
            memory: "128Mi"
          limits:
            cpu: "500m"
            memory: "256Mi"
EOF

kubectl apply -f /opt/candidate/nginx-deployment.yaml

# ãƒ¬ãƒ—ãƒªã‚«æ•°ã‚’5ã«å¢—ã‚„ã™
kubectl scale deployment nginx-deployment --replicas=5 -n web-app

# çµæœç¢ºèª
kubectl get deployment nginx-deployment -n web-app
kubectl get pods -n web-app
```

**æ¡ç‚¹ãƒã‚¤ãƒ³ãƒˆ:**
- æ­£ã—ã„namespaceã§ã®Deploymentä½œæˆ (20%)
- æŒ‡å®šã•ã‚ŒãŸã‚¤ãƒ¡ãƒ¼ã‚¸ã¨ãƒ¬ãƒ—ãƒªã‚«æ•° (20%)
- æ­£ã—ã„ãƒªã‚½ãƒ¼ã‚¹è¦æ±‚ã¨åˆ¶é™ (30%)
- ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æ“ä½œã®å®Ÿè¡Œ (30%)
</details>

---

## ğŸ¯ Question 5: Serviceã¨Endpointã®ä½œæˆ (4%)

**Context**: cluster: k8s-cluster-1, namespace: web-app  
**Task**: 
å‰ã®å•é¡Œã§ä½œæˆã—ãŸ `nginx-deployment` ã«å¯¾ã—ã¦Serviceã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š
- åå‰: `nginx-service`
- ã‚¿ã‚¤ãƒ—: ClusterIP
- ãƒãƒ¼ãƒˆ: 80
- ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒ¼ãƒˆ: 80

ServiceãŒæ­£ã—ãå‹•ä½œã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã€ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆæƒ…å ±ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚

<details>
<summary>ğŸ’¡ è§£ç­”ä¾‹</summary>

```bash
# Serviceã‚’ä½œæˆ
kubectl expose deployment nginx-deployment --name=nginx-service --port=80 --target-port=80 --type=ClusterIP -n web-app

# ã¾ãŸã¯ã€YAMLãƒ•ã‚¡ã‚¤ãƒ«ã§ä½œæˆ
cat <<EOF > /opt/candidate/nginx-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
  namespace: web-app
spec:
  selector:
    app: nginx-deployment
  ports:
  - port: 80
    targetPort: 80
  type: ClusterIP
EOF

kubectl apply -f /opt/candidate/nginx-service.yaml

# Serviceç¢ºèª
kubectl get service nginx-service -n web-app

# Endpointç¢ºèª
kubectl get endpoints nginx-service -n web-app

# Serviceå‹•ä½œç¢ºèª
kubectl run test-pod --image=busybox:1.35 --rm -it -n web-app -- wget -qO- nginx-service:80
```

**æ¡ç‚¹ãƒã‚¤ãƒ³ãƒˆ:**
- æ­£ã—ã„åå‰ã¨namespaceã§ã®Serviceä½œæˆ (30%)
- æ­£ã—ã„ãƒãƒ¼ãƒˆè¨­å®š (25%)
- æ­£ã—ã„Serviceã‚¿ã‚¤ãƒ— (20%)
- ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®æ­£å¸¸ãªç¢ºèª (25%)
</details>

---

## ğŸ¯ Question 6: DaemonSetã®ä½œæˆ (6%)

**Context**: cluster: k8s-cluster-1, namespace: monitoring  
**Task**: 
`monitoring` namespaceã«ä»¥ä¸‹ã®è¦ä»¶ã§DaemonSetã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š
- åå‰: `node-monitor`
- ã‚¤ãƒ¡ãƒ¼ã‚¸: `busybox:1.35`
- ã‚³ãƒãƒ³ãƒ‰: `['sh', '-c', 'while true; do date; sleep 30; done']`
- ã™ã¹ã¦ã®ãƒãƒ¼ãƒ‰ï¼ˆãƒã‚¹ã‚¿ãƒ¼ãƒãƒ¼ãƒ‰ã‚’å«ã‚€ï¼‰ã§å®Ÿè¡Œã•ã‚Œã‚‹
- hostPathãƒœãƒªãƒ¥ãƒ¼ãƒ  `/var/log` ã‚’ `/host/var/log` ã«ãƒã‚¦ãƒ³ãƒˆ

<details>
<summary>ğŸ’¡ è§£ç­”ä¾‹</summary>

```bash
# namespaceã‚’ä½œæˆ
kubectl create namespace monitoring

# DaemonSet YAMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
cat <<EOF > /opt/candidate/node-monitor-daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: node-monitor
  namespace: monitoring
  labels:
    app: node-monitor
spec:
  selector:
    matchLabels:
      app: node-monitor
  template:
    metadata:
      labels:
        app: node-monitor
    spec:
      tolerations:
      - key: node-role.kubernetes.io/control-plane
        operator: Exists
        effect: NoSchedule
      - key: node-role.kubernetes.io/master
        operator: Exists
        effect: NoSchedule
      containers:
      - name: monitor
        image: busybox:1.35
        command: ['sh', '-c', 'while true; do date; sleep 30; done']
        volumeMounts:
        - name: varlog
          mountPath: /host/var/log
          readOnly: true
      volumes:
      - name: varlog
        hostPath:
          path: /var/log
EOF

# DaemonSetã‚’é©ç”¨
kubectl apply -f /opt/candidate/node-monitor-daemonset.yaml

# çµæœç¢ºèª
kubectl get daemonset -n monitoring
kubectl get pods -n monitoring -o wide
```

**æ¡ç‚¹ãƒã‚¤ãƒ³ãƒˆ:**
- æ­£ã—ã„namespaceã§ã®DaemonSetä½œæˆ (20%)
- æŒ‡å®šã•ã‚ŒãŸã‚¤ãƒ¡ãƒ¼ã‚¸ã¨ã‚³ãƒãƒ³ãƒ‰ (25%)
- ãƒã‚¹ã‚¿ãƒ¼ãƒãƒ¼ãƒ‰ã§ã®å®Ÿè¡Œè¨­å®šï¼ˆtolerationsï¼‰ (30%)
- æ­£ã—ã„hostPathãƒœãƒªãƒ¥ãƒ¼ãƒ ãƒã‚¦ãƒ³ãƒˆ (25%)
</details>

---

## ğŸ¯ Question 7: StatefulSetã®ä½œæˆ (7%)

**Context**: cluster: k8s-cluster-1, namespace: database  
**Task**: 
`database` namespaceã«ä»¥ä¸‹ã®è¦ä»¶ã§StatefulSetã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š
- åå‰: `mysql-sts`
- ã‚¤ãƒ¡ãƒ¼ã‚¸: `mysql:8.0`
- ãƒ¬ãƒ—ãƒªã‚«æ•°: 2
- ç’°å¢ƒå¤‰æ•°: `MYSQL_ROOT_PASSWORD=root123`
- æ°¸ç¶šãƒœãƒªãƒ¥ãƒ¼ãƒ è¦æ±‚: 10Giã€ReadWriteOnce
- ã‚µãƒ¼ãƒ“ã‚¹å: `mysql-headless`ï¼ˆHeadlessã‚µãƒ¼ãƒ“ã‚¹ï¼‰

<details>
<summary>ğŸ’¡ è§£ç­”ä¾‹</summary>

```bash
# namespaceã‚’ä½œæˆ
kubectl create namespace database

# Headlessã‚µãƒ¼ãƒ“ã‚¹ã‚’ä½œæˆ
cat <<EOF > /opt/candidate/mysql-headless-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: mysql-headless
  namespace: database
spec:
  clusterIP: None
  selector:
    app: mysql-sts
  ports:
  - port: 3306
    targetPort: 3306
EOF

# StatefulSetã‚’ä½œæˆ
cat <<EOF > /opt/candidate/mysql-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql-sts
  namespace: database
spec:
  serviceName: mysql-headless
  replicas: 2
  selector:
    matchLabels:
      app: mysql-sts
  template:
    metadata:
      labels:
        app: mysql-sts
    spec:
      containers:
      - name: mysql
        image: mysql:8.0
        env:
        - name: MYSQL_ROOT_PASSWORD
          value: root123
        ports:
        - containerPort: 3306
        volumeMounts:
        - name: mysql-storage
          mountPath: /var/lib/mysql
  volumeClaimTemplates:
  - metadata:
      name: mysql-storage
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 10Gi
EOF

# é©ç”¨
kubectl apply -f /opt/candidate/mysql-headless-service.yaml
kubectl apply -f /opt/candidate/mysql-statefulset.yaml

# çµæœç¢ºèª
kubectl get statefulset -n database
kubectl get pods -n database
kubectl get pvc -n database
```

**æ¡ç‚¹ãƒã‚¤ãƒ³ãƒˆ:**
- æ­£ã—ã„namespaceã§ã®StatefulSetä½œæˆ (20%)
- æŒ‡å®šã•ã‚ŒãŸã‚¤ãƒ¡ãƒ¼ã‚¸ã¨ç’°å¢ƒå¤‰æ•° (25%)
- æ­£ã—ã„ãƒ¬ãƒ—ãƒªã‚«æ•° (15%)
- æ°¸ç¶šãƒœãƒªãƒ¥ãƒ¼ãƒ è¦æ±‚ã®è¨­å®š (25%)
- Headlessã‚µãƒ¼ãƒ“ã‚¹ã®ä½œæˆ (15%)
</details>

---

## ğŸ¯ Question 8: Jobã®ä½œæˆã¨ç®¡ç† (5%)

**Context**: cluster: k8s-cluster-1, namespace: batch  
**Task**: 
`batch` namespaceã«ä»¥ä¸‹ã®è¦ä»¶ã§Jobã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š
- åå‰: `pi-calculation`
- ã‚¤ãƒ¡ãƒ¼ã‚¸: `perl:5.34`
- ã‚³ãƒãƒ³ãƒ‰: `["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]`
- å®Œäº†å›æ•°: 1
- å†è©¦è¡Œåˆ¶é™: 3å›
- TTL: 300ç§’ï¼ˆå®Œäº†å¾Œ300ç§’ã§è‡ªå‹•å‰Šé™¤ï¼‰

<details>
<summary>ğŸ’¡ è§£ç­”ä¾‹</summary>

```bash
# namespaceã‚’ä½œæˆ
kubectl create namespace batch

# Job YAMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
cat <<EOF > /opt/candidate/pi-calculation-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: pi-calculation
  namespace: batch
spec:
  completions: 1
  parallelism: 1
  backoffLimit: 3
  ttlSecondsAfterFinished: 300
  template:
    spec:
      restartPolicy: Never
      containers:
      - name: pi
        image: perl:5.34
        command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
EOF

# Jobã‚’é©ç”¨
kubectl apply -f /opt/candidate/pi-calculation-job.yaml

# JobçŠ¶æ…‹ç¢ºèª
kubectl get job -n batch
kubectl describe job pi-calculation -n batch

# Jobå®Œäº†ç¢ºèª
kubectl wait --for=condition=complete job/pi-calculation -n batch --timeout=300s

# Jobãƒ­ã‚°ç¢ºèª
POD_NAME=$(kubectl get pods -n batch -l job-name=pi-calculation -o jsonpath='{.items[0].metadata.name}')
kubectl logs $POD_NAME -n batch
```

**æ¡ç‚¹ãƒã‚¤ãƒ³ãƒˆ:**
- æ­£ã—ã„namespaceã§ã®Jobä½œæˆ (20%)
- æŒ‡å®šã•ã‚ŒãŸã‚¤ãƒ¡ãƒ¼ã‚¸ã¨ã‚³ãƒãƒ³ãƒ‰ (30%)
- æ­£ã—ã„å®Œäº†å›æ•°ã¨å†è©¦è¡Œåˆ¶é™ (25%)
- TTLè¨­å®š (25%)
</details>

---

## ğŸ¯ Question 9: CronJobã®ä½œæˆ (4%)

**Context**: cluster: k8s-cluster-1, namespace: batch  
**Task**: 
`batch` namespaceã«ä»¥ä¸‹ã®è¦ä»¶ã§CronJobã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š
- åå‰: `cleanup-job`
- ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«: æ¯æ—¥åˆå‰3æ™‚ï¼ˆ`0 3 * * *`ï¼‰
- ã‚¤ãƒ¡ãƒ¼ã‚¸: `alpine:3.18`
- ã‚³ãƒãƒ³ãƒ‰: `['sh', '-c', 'echo "Cleanup completed at $(date)"']`
- æˆåŠŸå±¥æ­´ä¿æŒ: 3å€‹
- å¤±æ•—å±¥æ­´ä¿æŒ: 1å€‹

<details>
<summary>ğŸ’¡ è§£ç­”ä¾‹</summary>

```bash
# CronJob YAMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
cat <<EOF > /opt/candidate/cleanup-cronjob.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: cleanup-job
  namespace: batch
spec:
  schedule: "0 3 * * *"
  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 1
  jobTemplate:
    spec:
      template:
        spec:
          restartPolicy: OnFailure
          containers:
          - name: cleanup
            image: alpine:3.18
            command: ['sh', '-c', 'echo "Cleanup completed at $(date)"']
EOF

# CronJobã‚’é©ç”¨
kubectl apply -f /opt/candidate/cleanup-cronjob.yaml

# CronJobç¢ºèª
kubectl get cronjob -n batch
kubectl describe cronjob cleanup-job -n batch

# æ‰‹å‹•ã§Jobã‚’ä½œæˆã—ã¦ãƒ†ã‚¹ãƒˆ
kubectl create job --from=cronjob/cleanup-job manual-cleanup -n batch
```

**æ¡ç‚¹ãƒã‚¤ãƒ³ãƒˆ:**
- æ­£ã—ã„namespaceã§ã®CronJobä½œæˆ (25%)
- æ­£ã—ã„ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«è¨­å®š (25%)
- æŒ‡å®šã•ã‚ŒãŸã‚¤ãƒ¡ãƒ¼ã‚¸ã¨ã‚³ãƒãƒ³ãƒ‰ (25%)
- å±¥æ­´ä¿æŒè¨­å®š (25%)
</details>

---

## ğŸ¯ Question 10: ãƒãƒ¼ãƒ‰ã®ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ (8%)

**Context**: cluster: k8s-cluster-1  
**Task**: 
ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒãƒ¼ãƒ‰ `worker-node-1` ã‚’ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ã®ãŸã‚ã«æº–å‚™ã—ã¦ãã ã•ã„ï¼š
1. ãƒãƒ¼ãƒ‰ã‚’å®‰å…¨ã«ãƒ‰ãƒ¬ã‚¤ãƒ³ã—ã€ã™ã¹ã¦ã®Podã‚’ä»–ã®ãƒãƒ¼ãƒ‰ã«ç§»è¡Œ
2. ãƒãƒ¼ãƒ‰ã‚’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ä¸å¯ã«è¨­å®š
3. ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹å®Œäº†å¾Œã€ãƒãƒ¼ãƒ‰ã‚’å†ã³ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å¯èƒ½ã«æˆ»ã™

<details>
<summary>ğŸ’¡ è§£ç­”ä¾‹</summary>

```bash
# ç¾åœ¨ã®ãƒãƒ¼ãƒ‰çŠ¶æ…‹ã‚’ç¢ºèª
kubectl get nodes

# ãƒãƒ¼ãƒ‰ã‚’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ä¸å¯ã«è¨­å®š
kubectl cordon worker-node-1

# ãƒãƒ¼ãƒ‰ä¸Šã®Podã‚’ç¢ºèª
kubectl get pods --all-namespaces -o wide | grep worker-node-1

# ãƒãƒ¼ãƒ‰ã‚’å®‰å…¨ã«ãƒ‰ãƒ¬ã‚¤ãƒ³
kubectl drain worker-node-1 --ignore-daemonsets --delete-emptydir-data --force

# ãƒ‰ãƒ¬ã‚¤ãƒ³å®Œäº†ç¢ºèª
kubectl get pods --all-namespaces -o wide | grep worker-node-1

# ãƒãƒ¼ãƒ‰çŠ¶æ…‹ç¢ºèª
kubectl get nodes

# ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹å®Œäº†å¾Œã€ãƒãƒ¼ãƒ‰ã‚’å†ã³ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å¯èƒ½ã«è¨­å®š
kubectl uncordon worker-node-1

# æœ€çµ‚çŠ¶æ…‹ç¢ºèª
kubectl get nodes
```

**æ¡ç‚¹ãƒã‚¤ãƒ³ãƒˆ:**
- æ­£ã—ã„cordonæ“ä½œ (25%)
- é©åˆ‡ãªdrainã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®ä½¿ç”¨ (40%)
- ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹å¾Œã®uncordonæ“ä½œ (25%)
- æ“ä½œå‰å¾Œã®çŠ¶æ…‹ç¢ºèª (10%)
</details>

---

## ğŸ¯ Question 11: Podé–“é€šä¿¡ã®ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚° (8%)

**Context**: cluster: k8s-cluster-1, namespace: debug  
**Task**: 
`debug` namespaceã«2ã¤ã®PodãŒã‚ã‚Šã¾ã™ï¼š`app-pod` ã¨ `db-pod`ã€‚
`app-pod` ã‹ã‚‰ `db-pod` ã¸ã®é€šä¿¡ãŒã§ãã¾ã›ã‚“ã€‚
å•é¡Œã‚’ç‰¹å®šã—ã€ä¿®æ­£ã—ã¦ãã ã•ã„ã€‚é€šä¿¡ã¯ port 3306 ã§è¡Œã‚ã‚Œã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

```bash
# äº‹å‰ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ï¼ˆå•é¡Œå†ç¾ç”¨ï¼‰
kubectl create namespace debug

# å•é¡Œã®ã‚ã‚‹Podã‚’ä½œæˆ
kubectl run app-pod --image=busybox:1.35 --command -n debug -- sleep 3600
kubectl run db-pod --image=mysql:8.0 -n debug --env="MYSQL_ROOT_PASSWORD=password"

# æ„å›³çš„ã«é–“é•ã£ãŸServiceã‚’ä½œæˆ
kubectl expose pod db-pod --port=3305 --target-port=3306 -n debug
```

<details>
<summary>ğŸ’¡ è§£ç­”ä¾‹</summary>

```bash
# å•é¡Œã®èª¿æŸ»é–‹å§‹
kubectl get pods -n debug
kubectl get services -n debug

# app-podã‹ã‚‰ db-podã¸ã®æ¥ç¶šãƒ†ã‚¹ãƒˆ
kubectl exec app-pod -n debug -- nc -zv db-pod 3306
kubectl exec app-pod -n debug -- nc -zv db-pod 3305

# db-podã®çŠ¶æ…‹ç¢ºèª
kubectl describe pod db-pod -n debug
kubectl logs db-pod -n debug

# Serviceã®ç¢ºèª
kubectl describe service db-pod -n debug

# å•é¡Œç™ºè¦‹ï¼šServiceã®ãƒãƒ¼ãƒˆãŒé–“é•ã£ã¦ã„ã‚‹ï¼ˆ3305 instead of 3306ï¼‰
# Serviceã‚’ä¿®æ­£
kubectl patch service db-pod -n debug -p '{"spec":{"ports":[{"port":3306,"targetPort":3306}]}}'

# ã¾ãŸã¯ã€Serviceã‚’å‰Šé™¤ã—ã¦å†ä½œæˆ
kubectl delete service db-pod -n debug
kubectl expose pod db-pod --port=3306 --target-port=3306 -n debug

# ä¿®æ­£å¾Œã®æ¥ç¶šãƒ†ã‚¹ãƒˆ
kubectl exec app-pod -n debug -- nc -zv db-pod 3306

# æœ€çµ‚ç¢ºèª
kubectl get service db-pod -n debug
kubectl describe service db-pod -n debug
```

**æ¡ç‚¹ãƒã‚¤ãƒ³ãƒˆ:**
- å•é¡Œã®æ­£ç¢ºãªç‰¹å®š (40%)
- é©åˆ‡ãªèª¿æŸ»æ‰‹é † (30%)
- æ­£ã—ã„ä¿®æ­£æ–¹æ³• (20%)
- ä¿®æ­£å¾Œã®å‹•ä½œç¢ºèª (10%)
</details>

---

## ğŸ¯ Question 12: ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡ã®ç›£è¦– (4%)

**Context**: cluster: k8s-cluster-1  
**Task**: 
ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼å†…ã§æœ€ã‚‚CPUã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹Podã‚’ç‰¹å®šã—ã€ä»¥ä¸‹ã®æƒ…å ±ã‚’`/opt/candidate/high-cpu-pods.txt`ã«ä¿å­˜ã—ã¦ãã ã•ã„ï¼š
- Podå
- Namespace
- CPUä½¿ç”¨é‡
- ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡

ä¸Šä½5ã¤ã®Podã®æƒ…å ±ã‚’è¨˜éŒ²ã—ã¦ãã ã•ã„ã€‚

<details>
<summary>ğŸ’¡ è§£ç­”ä¾‹</summary>

```bash
# ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚µãƒ¼ãƒãƒ¼ãŒå‹•ä½œã—ã¦ã„ã‚‹ã‹ç¢ºèª
kubectl get pods -n kube-system | grep metrics-server

# å…¨Podã®CPUä½¿ç”¨é‡ã‚’ç¢ºèª
kubectl top pods --all-namespaces --sort-by=cpu

# ä¸Šä½5ã¤ã®Podã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
echo "=== Top 5 CPU-consuming Pods ===" > /opt/candidate/high-cpu-pods.txt
echo "$(date)" >> /opt/candidate/high-cpu-pods.txt
echo "" >> /opt/candidate/high-cpu-pods.txt

kubectl top pods --all-namespaces --sort-by=cpu --no-headers | head -5 | while read namespace pod cpu memory; do
    echo "Pod: $pod" >> /opt/candidate/high-cpu-pods.txt
    echo "Namespace: $namespace" >> /opt/candidate/high-cpu-pods.txt
    echo "CPU Usage: $cpu" >> /opt/candidate/high-cpu-pods.txt
    echo "Memory Usage: $memory" >> /opt/candidate/high-cpu-pods.txt
    echo "---" >> /opt/candidate/high-cpu-pods.txt
done

# çµæœç¢ºèª
cat /opt/candidate/high-cpu-pods.txt
```

**æ¡ç‚¹ãƒã‚¤ãƒ³ãƒˆ:**
- æ­£ã—ã„ãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—ã‚³ãƒãƒ³ãƒ‰ã®ä½¿ç”¨ (30%)
- CPUä½¿ç”¨é‡ã«ã‚ˆã‚‹æ­£ã—ã„ã‚½ãƒ¼ãƒˆ (25%)
- æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã§ã®ä¿å­˜ (25%)
- ä¸Šä½5ã¤ã®Podã®æ­£ç¢ºãªç‰¹å®š (20%)
</details>

---

## ğŸ¯ Question 13: NetworkPolicyã®ä½œæˆ (6%)

**Context**: cluster: k8s-cluster-1, namespace: secure-app  
**Task**: 
`secure-app` namespaceã«NetworkPolicyã‚’ä½œæˆã—ã€ä»¥ä¸‹ã®ãƒ«ãƒ¼ãƒ«ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ï¼š
- åå‰: `deny-all-ingress`
- ã™ã¹ã¦ã®å…¥åŠ›ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯ã‚’æ‹’å¦
- ãŸã ã—ã€`app: frontend` ãƒ©ãƒ™ãƒ«ã‚’æŒã¤Podã‹ã‚‰ã® port 8080 ã¸ã®æ¥ç¶šã¯è¨±å¯

<details>
<summary>ğŸ’¡ è§£ç­”ä¾‹</summary>

```bash
# namespaceã‚’ä½œæˆ
kubectl create namespace secure-app

# NetworkPolicy YAMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
cat <<EOF > /opt/candidate/deny-all-ingress-policy.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: deny-all-ingress
  namespace: secure-app
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: frontend
    ports:
    - protocol: TCP
      port: 8080
EOF

# NetworkPolicyã‚’é©ç”¨
kubectl apply -f /opt/candidate/deny-all-ingress-policy.yaml

# NetworkPolicyç¢ºèª
kubectl get networkpolicy -n secure-app
kubectl describe networkpolicy deny-all-ingress -n secure-app

# ãƒ†ã‚¹ãƒˆç”¨Podã‚’ä½œæˆ
kubectl run backend-pod --image=nginx:1.20 -n secure-app
kubectl run frontend-pod --image=busybox:1.35 --labels="app=frontend" -n secure-app --command -- sleep 3600
kubectl run unauthorized-pod --image=busybox:1.35 -n secure-app --command -- sleep 3600

# æ¥ç¶šãƒ†ã‚¹ãƒˆ
# frontend-podã‹ã‚‰æ¥ç¶šï¼ˆè¨±å¯ã•ã‚Œã‚‹ã¹ãï¼‰
kubectl exec frontend-pod -n secure-app -- nc -zv backend-pod 8080

# unauthorized-podã‹ã‚‰æ¥ç¶šï¼ˆæ‹’å¦ã•ã‚Œã‚‹ã¹ãï¼‰
kubectl exec unauthorized-pod -n secure-app -- nc -zv backend-pod 8080
```

**æ¡ç‚¹ãƒã‚¤ãƒ³ãƒˆ:**
- æ­£ã—ã„namespaceã§ã®NetworkPolicyä½œæˆ (20%)
- æ­£ã—ã„podSelectorã®ä½¿ç”¨ (25%)
- é©åˆ‡ãªå…¥åŠ›ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯æ‹’å¦è¨­å®š (25%)
- frontendã‹ã‚‰ã®ä¾‹å¤–è¨±å¯è¨­å®š (30%)
</details>

---

## ğŸ¯ Question 14: ãƒ­ã‚°åˆ†æã¨ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚° (7%)

**Context**: cluster: k8s-cluster-1, namespace: problem-app  
**Task**: 
`problem-app` namespaceã«ã‚ã‚‹Deployment `failing-app` ãŒæ­£å¸¸ã«å‹•ä½œã—ã¦ã„ã¾ã›ã‚“ã€‚
å•é¡Œã‚’ç‰¹å®šã—ã€ä¿®æ­£ã—ã¦ãã ã•ã„ã€‚ä¿®æ­£å†…å®¹ã‚’`/opt/candidate/fix-summary.txt`ã«è¨˜éŒ²ã—ã¦ãã ã•ã„ã€‚

```bash
# å•é¡Œã®ã‚ã‚‹Deploymentã‚’äº‹å‰ä½œæˆ
kubectl create namespace problem-app
cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: failing-app
  namespace: problem-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: failing-app
  template:
    metadata:
      labels:
        app: failing-app
    spec:
      containers:
      - name: app
        image: nginx:1.20
        ports:
        - containerPort: 80
        livenessProbe:
          httpGet:
            path: /health
            port: 8080  # é–“é•ã£ãŸãƒãƒ¼ãƒˆ
          initialDelaySeconds: 10
          periodSeconds: 5
        readinessProbe:
          httpGet:
            path: /ready
            port: 80
          initialDelaySeconds: 5
          periodSeconds: 5
EOF
```

<details>
<summary>ğŸ’¡ è§£ç­”ä¾‹</summary>

```bash
# å•é¡Œã®èª¿æŸ»é–‹å§‹
kubectl get deployments -n problem-app
kubectl get pods -n problem-app
kubectl describe deployment failing-app -n problem-app

# Podè©³ç´°ç¢ºèª
kubectl describe pods -l app=failing-app -n problem-app

# Podãƒ­ã‚°ç¢ºèª
kubectl logs -l app=failing-app -n problem-app

# ã‚¤ãƒ™ãƒ³ãƒˆç¢ºèª
kubectl get events -n problem-app --sort-by=.metadata.creationTimestamp

# å•é¡Œç™ºè¦‹ï¼šlivenessProbeã®ãƒãƒ¼ãƒˆãŒé–“é•ã£ã¦ã„ã‚‹
# Deploymentã‚’ä¿®æ­£
kubectl patch deployment failing-app -n problem-app -p '{
  "spec": {
    "template": {
      "spec": {
        "containers": [{
          "name": "app",
          "livenessProbe": {
            "httpGet": {
              "path": "/",
              "port": 80
            }
          },
          "readinessProbe": {
            "httpGet": {
              "path": "/",
              "port": 80
            }
          }
        }]
      }
    }
  }
}'

# ä¿®æ­£å¾Œã®çŠ¶æ…‹ç¢ºèª
kubectl get pods -n problem-app
kubectl wait --for=condition=Ready pod -l app=failing-app -n problem-app --timeout=300s

# ä¿®æ­£å†…å®¹ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«è¨˜éŒ²
cat <<EOF > /opt/candidate/fix-summary.txt
Problem Analysis and Fix Summary
================================

Issue Identified:
- Liveness probe was configured with wrong port (8080 instead of 80)
- Readiness probe was using non-existent path (/ready)

Root Cause:
- Nginx container listens on port 80, but liveness probe was checking port 8080
- Nginx default setup doesn't have /ready endpoint

Fix Applied:
- Changed liveness probe port from 8080 to 80
- Changed liveness probe path from /health to /
- Changed readiness probe path from /ready to /

Result:
- All pods are now in Ready state
- Deployment is successfully running with 3/3 replicas
EOF
```

**æ¡ç‚¹ãƒã‚¤ãƒ³ãƒˆ:**
- æ­£ç¢ºãªå•é¡Œç‰¹å®š (30%)
- é©åˆ‡ãªèª¿æŸ»æ‰‹é † (25%)
- æ­£ã—ã„ä¿®æ­£å®Ÿè£… (25%)
- è©³ç´°ãªä¿®æ­£ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ (20%)
</details>

---

## ğŸ¯ Question 15: PersistentVolumeã¨Claim (6%)

**Context**: cluster: k8s-cluster-1, namespace: storage-test  
**Task**: 
ä»¥ä¸‹ã®è¦ä»¶ã§PersistentVolumeã¨PersistentVolumeClaimã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š

PersistentVolume:
- åå‰: `test-pv`
- å®¹é‡: 5Gi
- ã‚¢ã‚¯ã‚»ã‚¹ãƒ¢ãƒ¼ãƒ‰: ReadWriteOnce
- ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚¯ãƒ©ã‚¹: manual
- hostPath: `/opt/pvdata`

PersistentVolumeClaim:
- åå‰: `test-pvc`
- å®¹é‡è¦æ±‚: 3Gi
- ã‚¢ã‚¯ã‚»ã‚¹ãƒ¢ãƒ¼ãƒ‰: ReadWriteOnce
- ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚¯ãƒ©ã‚¹: manual

<details>
<summary>ğŸ’¡ è§£ç­”ä¾‹</summary>

```bash
# namespaceã‚’ä½œæˆ
kubectl create namespace storage-test

# PersistentVolume YAMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
cat <<EOF > /opt/candidate/test-pv.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: test-pv
spec:
  capacity:
    storage: 5Gi
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: manual
  hostPath:
    path: /opt/pvdata
EOF

# PersistentVolumeClaim YAMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
cat <<EOF > /opt/candidate/test-pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: test-pvc
  namespace: storage-test
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 3Gi
  storageClassName: manual
EOF

# PVã¨PVCã‚’ä½œæˆ
kubectl apply -f /opt/candidate/test-pv.yaml
kubectl apply -f /opt/candidate/test-pvc.yaml

# çŠ¶æ…‹ç¢ºèª
kubectl get pv test-pv
kubectl get pvc test-pvc -n storage-test

# PVCãŒPVã«ãƒã‚¤ãƒ³ãƒ‰ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª
kubectl describe pvc test-pvc -n storage-test
kubectl describe pv test-pv

# ãƒ†ã‚¹ãƒˆç”¨Podã‚’ä½œæˆã—ã¦ãƒã‚¦ãƒ³ãƒˆç¢ºèª
cat <<EOF > /opt/candidate/test-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: test-pod
  namespace: storage-test
spec:
  containers:
  - name: test
    image: busybox:1.35
    command: ['sh', '-c', 'echo "Test data" > /mnt/test.txt; sleep 3600']
    volumeMounts:
    - name: test-volume
      mountPath: /mnt
  volumes:
  - name: test-volume
    persistentVolumeClaim:
      claimName: test-pvc
EOF

kubectl apply -f /opt/candidate/test-pod.yaml
kubectl wait --for=condition=Ready pod/test-pod -n storage-test --timeout=300s
```

**æ¡ç‚¹ãƒã‚¤ãƒ³ãƒˆ:**
- æ­£ã—ã„PVä»•æ§˜ã§ã®ä½œæˆ (30%)
- æ­£ã—ã„PVCä»•æ§˜ã§ã®ä½œæˆ (30%)
- PVã¨PVCã®æ­£å¸¸ãªãƒã‚¤ãƒ³ãƒ‡ã‚£ãƒ³ã‚° (25%)
- é©åˆ‡ãªã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚¯ãƒ©ã‚¹ã®è¨­å®š (15%)
</details>

---

## ğŸ¯ Question 16: RBACè¨­å®š (8%)

**Context**: cluster: k8s-cluster-1, namespace: rbac-test  
**Task**: 
ä»¥ä¸‹ã®è¦ä»¶ã§RBACè¨­å®šã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š
- ServiceAccount: `dev-user`
- Role: `pod-reader` (podã®èª­ã¿å–ã‚Šæ¨©é™ã®ã¿)
- RoleBinding: `dev-user-binding` (dev-userã«pod-readerãƒ­ãƒ¼ãƒ«ã‚’ä»˜ä¸)
- namespace: `rbac-test`

è¨­å®šå¾Œã€æ¨©é™ã‚’ãƒ†ã‚¹ãƒˆã—ã¦ãã ã•ã„ã€‚

<details>
<summary>ğŸ’¡ è§£ç­”ä¾‹</summary>

```bash
# namespaceã‚’ä½œæˆ
kubectl create namespace rbac-test

# ServiceAccountã‚’ä½œæˆ
kubectl create serviceaccount dev-user -n rbac-test

# Role YAMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
cat <<EOF > /opt/candidate/pod-reader-role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: pod-reader
  namespace: rbac-test
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list", "watch"]
EOF

# RoleBinding YAMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
cat <<EOF > /opt/candidate/dev-user-binding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: dev-user-binding
  namespace: rbac-test
subjects:
- kind: ServiceAccount
  name: dev-user
  namespace: rbac-test
roleRef:
  kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io
EOF

# Roleã¨RoleBindingã‚’é©ç”¨
kubectl apply -f /opt/candidate/pod-reader-role.yaml
kubectl apply -f /opt/candidate/dev-user-binding.yaml

# ç¢ºèª
kubectl get serviceaccount dev-user -n rbac-test
kubectl get role pod-reader -n rbac-test
kubectl get rolebinding dev-user-binding -n rbac-test

# æ¨©é™ãƒ†ã‚¹ãƒˆç”¨ã®Podã‚’ä½œæˆ
kubectl run test-pod --image=nginx:1.20 -n rbac-test

# dev-userã®æ¨©é™ã‚’ãƒ†ã‚¹ãƒˆ
# Podèª­ã¿å–ã‚Šæ¨©é™ï¼ˆè¨±å¯ã•ã‚Œã‚‹ã¹ãï¼‰
kubectl auth can-i get pods --as=system:serviceaccount:rbac-test:dev-user -n rbac-test

# Podä½œæˆæ¨©é™ï¼ˆæ‹’å¦ã•ã‚Œã‚‹ã¹ãï¼‰
kubectl auth can-i create pods --as=system:serviceaccount:rbac-test:dev-user -n rbac-test

# Serviceèª­ã¿å–ã‚Šæ¨©é™ï¼ˆæ‹’å¦ã•ã‚Œã‚‹ã¹ãï¼‰
kubectl auth can-i get services --as=system:serviceaccount:rbac-test:dev-user -n rbac-test
```

**æ¡ç‚¹ãƒã‚¤ãƒ³ãƒˆ:**
- ServiceAccountã®æ­£ã—ã„ä½œæˆ (20%)
- Roleã®é©åˆ‡ãªæ¨©é™è¨­å®š (30%)
- RoleBindingã®æ­£ã—ã„è¨­å®š (30%)
- æ¨©é™ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œã¨ç¢ºèª (20%)
</details>

---

## ğŸ¯ Question 17: ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼è¨¼æ˜æ›¸ã®æ›´æ–° (5%)

**Context**: cluster: k8s-cluster-1  
**Task**: 
Kubernetesã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®è¨¼æ˜æ›¸ã®æœ‰åŠ¹æœŸé™ã‚’ç¢ºèªã—ã€API Serverã®è¨¼æ˜æ›¸ã‚’æ›´æ–°ã—ã¦ãã ã•ã„ã€‚
æ›´æ–°å‰å¾Œã®æœ‰åŠ¹æœŸé™ã‚’`/opt/candidate/cert-renewal.txt`ã«è¨˜éŒ²ã—ã¦ãã ã•ã„ã€‚

<details>
<summary>ğŸ’¡ è§£ç­”ä¾‹</summary>

```bash
# è¨¼æ˜æ›¸æœ‰åŠ¹æœŸé™ç¢ºèª
echo "=== Certificate Expiration Check (Before Renewal) ===" > /opt/candidate/cert-renewal.txt
echo "$(date)" >> /opt/candidate/cert-renewal.txt
echo "" >> /opt/candidate/cert-renewal.txt

kubeadm certs check-expiration >> /opt/candidate/cert-renewal.txt

echo "" >> /opt/candidate/cert-renewal.txt
echo "=== API Server Certificate Details (Before) ===" >> /opt/candidate/cert-renewal.txt
openssl x509 -in /etc/kubernetes/pki/apiserver.crt -text -noout | grep -A 2 "Validity" >> /opt/candidate/cert-renewal.txt

# è¨¼æ˜æ›¸ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
sudo cp -r /etc/kubernetes/pki /etc/kubernetes/pki-backup-$(date +%Y%m%d_%H%M%S)

# API Serverè¨¼æ˜æ›¸ã‚’æ›´æ–°
sudo kubeadm certs renew apiserver

# æ›´æ–°å¾Œã®ç¢ºèª
echo "" >> /opt/candidate/cert-renewal.txt
echo "=== Certificate Expiration Check (After Renewal) ===" >> /opt/candidate/cert-renewal.txt
echo "$(date)" >> /opt/candidate/cert-renewal.txt

kubeadm certs check-expiration >> /opt/candidate/cert-renewal.txt

echo "" >> /opt/candidate/cert-renewal.txt
echo "=== API Server Certificate Details (After) ===" >> /opt/candidate/cert-renewal.txt
openssl x509 -in /etc/kubernetes/pki/apiserver.crt -text -noout | grep -A 2 "Validity" >> /opt/candidate/cert-renewal.txt

# kubeletã¨containerdã®å†èµ·å‹•
sudo systemctl restart kubelet
sudo systemctl restart containerd

# ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ¥ç¶šç¢ºèª
kubectl cluster-info
kubectl get nodes

echo "" >> /opt/candidate/cert-renewal.txt
echo "=== Cluster Status After Renewal ===" >> /opt/candidate/cert-renewal.txt
kubectl get nodes >> /opt/candidate/cert-renewal.txt

# çµæœç¢ºèª
cat /opt/candidate/cert-renewal.txt
```

**æ¡ç‚¹ãƒã‚¤ãƒ³ãƒˆ:**
- è¨¼æ˜æ›¸æœ‰åŠ¹æœŸé™ã®æ­£ç¢ºãªç¢ºèª (25%)
- é©åˆ‡ãªè¨¼æ˜æ›¸ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ— (20%)
- æ­£ã—ã„è¨¼æ˜æ›¸æ›´æ–°ã‚³ãƒãƒ³ãƒ‰ã®å®Ÿè¡Œ (30%)
- æ›´æ–°å‰å¾Œã®æ¯”è¼ƒè¨˜éŒ² (25%)
</details>

---

## ğŸ“Š æ¡ç‚¹åŸºæº–

| å•é¡Œç•ªå· | é…ç‚¹ | åˆ†é‡ |
|----------|------|------|
| Q1 | 2% | ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ç®¡ç† |
| Q2 | 7% | etcdãƒãƒƒã‚¯ã‚¢ãƒƒãƒ— |
| Q3 | 8% | ãƒãƒ¼ãƒ‰ç®¡ç† |
| Q4 | 5% | ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ç®¡ç† |
| Q5 | 4% | ã‚µãƒ¼ãƒ“ã‚¹ç®¡ç† |
| Q6 | 6% | DaemonSet |
| Q7 | 7% | StatefulSet |
| Q8 | 5% | Jobç®¡ç† |
| Q9 | 4% | CronJob |
| Q10 | 8% | ãƒãƒ¼ãƒ‰ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ |
| Q11 | 8% | ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚° |
| Q12 | 4% | ç›£è¦–ãƒ»ãƒ¡ãƒˆãƒªã‚¯ã‚¹ |
| Q13 | 6% | ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ |
| Q14 | 7% | å•é¡Œè§£æ±º |
| Q15 | 6% | ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ç®¡ç† |
| Q16 | 8% | RBACãƒ»ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ |
| Q17 | 5% | è¨¼æ˜æ›¸ç®¡ç† |
| **åˆè¨ˆ** | **100%** | |

**åˆæ ¼ãƒ©ã‚¤ãƒ³**: 66%ä»¥ä¸Š

---

## ğŸ¯ è©¦é¨“å¾Œã®æŒ¯ã‚Šè¿”ã‚Š

ç·´ç¿’è©¦é¨“å®Œäº†å¾Œã€ä»¥ä¸‹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ï¼š

1. **æ™‚é–“ç®¡ç†**: 120åˆ†ä»¥å†…ã«å®Œäº†ã§ããŸã‹
2. **æ­£è§£ç‡**: 66%ä»¥ä¸Šé”æˆã§ããŸã‹  
3. **å¼±ç‚¹åˆ†é‡**: é–“é•ã£ãŸå•é¡Œã®åˆ†é‡ã‚’ç‰¹å®š
4. **æ”¹å–„ç‚¹**: æ¬¡å›ã«å‘ã‘ã¦ã®å­¦ç¿’ãƒã‚¤ãƒ³ãƒˆ

---

## ğŸ¯ Question 18: Ingressè¨­å®š (4%)

**Context**: cluster: k8s-cluster-1, namespace: web-services  
**Task**: 
ä»¥ä¸‹ã®è¦ä»¶ã§Ingressã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š
- åå‰: `web-ingress`
- ãƒ›ã‚¹ãƒˆ: `app.example.com`
- ãƒ‘ã‚¹ `/api` ã‚’ `api-service:8080` ã«ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°
- ãƒ‘ã‚¹ `/web` ã‚’ `web-service:80` ã«ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°
- TLSè¨¼æ˜æ›¸: `web-tls-secret`

<details>
<summary>ğŸ’¡ è§£ç­”ä¾‹</summary>

```bash
# namespaceã‚’ä½œæˆ
kubectl create namespace web-services

# å¿…è¦ãªServiceã‚’ä½œæˆï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰
kubectl create deployment api-app --image=nginx:1.20 -n web-services
kubectl expose deployment api-app --name=api-service --port=8080 --target-port=80 -n web-services

kubectl create deployment web-app --image=nginx:1.20 -n web-services
kubectl expose deployment web-app --name=web-service --port=80 --target-port=80 -n web-services

# TLS Secretã‚’ä½œæˆï¼ˆãƒ†ã‚¹ãƒˆç”¨ã®è‡ªå·±è¨¼æ˜æ›¸ï¼‰
openssl req -x509 -nodes -days 365 -newkey rsa:2048 \
  -keyout /tmp/tls.key -out /tmp/tls.crt \
  -subj "/CN=app.example.com/O=app.example.com"

kubectl create secret tls web-tls-secret \
  --cert=/tmp/tls.crt --key=/tmp/tls.key -n web-services

# Ingress YAMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
cat <<EOF > /opt/candidate/web-ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: web-ingress
  namespace: web-services
spec:
  tls:
  - hosts:
    - app.example.com
    secretName: web-tls-secret
  rules:
  - host: app.example.com
    http:
      paths:
      - path: /api
        pathType: Prefix
        backend:
          service:
            name: api-service
            port:
              number: 8080
      - path: /web
        pathType: Prefix
        backend:
          service:
            name: web-service
            port:
              number: 80
EOF

# Ingressã‚’é©ç”¨
kubectl apply -f /opt/candidate/web-ingress.yaml

# ç¢ºèª
kubectl get ingress -n web-services
kubectl describe ingress web-ingress -n web-services
```

**æ¡ç‚¹ãƒã‚¤ãƒ³ãƒˆ:**
- æ­£ã—ã„namespaceã§ã®Ingressä½œæˆ (25%)
- é©åˆ‡ãªãƒ›ã‚¹ãƒˆè¨­å®š (25%)
- æ­£ã—ã„ãƒ‘ã‚¹ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°è¨­å®š (30%)
- TLSè¨­å®š (20%)
</details>

---

## ğŸ¯ Question 19: ConfigMapã¨Secretç®¡ç† (5%)

**Context**: cluster: k8s-cluster-1, namespace: config-test  
**Task**: 
ä»¥ä¸‹ã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š
1. ConfigMap `app-config` (key: config.yaml, value: database_url: "localhost:5432")
2. Secret `app-secret` (key: password, value: "secretpass123")
3. Pod `config-pod` ã§ConfigMapã‚’ç’°å¢ƒå¤‰æ•°ã€Secretã‚’ãƒœãƒªãƒ¥ãƒ¼ãƒ ã¨ã—ã¦ãƒã‚¦ãƒ³ãƒˆ

<details>
<summary>ğŸ’¡ è§£ç­”ä¾‹</summary>

```bash
# namespaceã‚’ä½œæˆ
kubectl create namespace config-test

# ConfigMapã‚’ä½œæˆ
kubectl create configmap app-config \
  --from-literal=config.yaml='database_url: "localhost:5432"' \
  -n config-test

# Secretã‚’ä½œæˆ
kubectl create secret generic app-secret \
  --from-literal=password=secretpass123 \
  -n config-test

# Pod YAMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
cat <<EOF > /opt/candidate/config-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: config-pod
  namespace: config-test
spec:
  containers:
  - name: app
    image: busybox:1.35
    command: ['sh', '-c', 'echo "Config: $CONFIG_DATA"; cat /etc/secrets/password; sleep 3600']
    env:
    - name: CONFIG_DATA
      valueFrom:
        configMapKeyRef:
          name: app-config
          key: config.yaml
    volumeMounts:
    - name: secret-volume
      mountPath: /etc/secrets
      readOnly: true
  volumes:
  - name: secret-volume
    secret:
      secretName: app-secret
EOF

# Podã‚’é©ç”¨
kubectl apply -f /opt/candidate/config-pod.yaml

# ç¢ºèª
kubectl get configmap app-config -n config-test
kubectl get secret app-secret -n config-test
kubectl get pod config-pod -n config-test
kubectl logs config-pod -n config-test
```

**æ¡ç‚¹ãƒã‚¤ãƒ³ãƒˆ:**
- ConfigMapã®æ­£ã—ã„ä½œæˆ (30%)
- Secretã®æ­£ã—ã„ä½œæˆ (30%)
- ç’°å¢ƒå¤‰æ•°ã§ã®å‚ç…§ (20%)
- ãƒœãƒªãƒ¥ãƒ¼ãƒ ãƒã‚¦ãƒ³ãƒˆè¨­å®š (20%)
</details>

---

## ğŸ¯ Question 20: HorizontalPodAutoscalerè¨­å®š (6%)

**Context**: cluster: k8s-cluster-1, namespace: scaling-test  
**Task**: 
ä»¥ä¸‹ã®è¦ä»¶ã§HPAã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š
- ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ: Deployment `web-app`
- æœ€å°ãƒ¬ãƒ—ãƒªã‚«: 2
- æœ€å¤§ãƒ¬ãƒ—ãƒªã‚«: 10
- CPUä½¿ç”¨ç‡: 70%ã§ã‚¹ã‚±ãƒ¼ãƒ«
- ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡: 80%ã§ã‚¹ã‚±ãƒ¼ãƒ«

<details>
<summary>ğŸ’¡ è§£ç­”ä¾‹</summary>

```bash
# namespaceã‚’ä½œæˆ
kubectl create namespace scaling-test

# ãƒ†ã‚¹ãƒˆç”¨Deploymentã‚’ä½œæˆ
cat <<EOF > /opt/candidate/web-app-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-app
  namespace: scaling-test
spec:
  replicas: 2
  selector:
    matchLabels:
      app: web-app
  template:
    metadata:
      labels:
        app: web-app
    spec:
      containers:
      - name: web
        image: nginx:1.20
        resources:
          requests:
            cpu: 100m
            memory: 64Mi
          limits:
            cpu: 200m
            memory: 128Mi
EOF

kubectl apply -f /opt/candidate/web-app-deployment.yaml

# HPA YAMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
cat <<EOF > /opt/candidate/web-app-hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: web-app-hpa
  namespace: scaling-test
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: web-app
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
EOF

# HPAã‚’é©ç”¨
kubectl apply -f /opt/candidate/web-app-hpa.yaml

# ç¢ºèª
kubectl get hpa -n scaling-test
kubectl describe hpa web-app-hpa -n scaling-test
kubectl get deployment web-app -n scaling-test
```

**æ¡ç‚¹ãƒã‚¤ãƒ³ãƒˆ:**
- æ­£ã—ã„ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæŒ‡å®š (25%)
- é©åˆ‡ãªãƒ¬ãƒ—ãƒªã‚«æ•°è¨­å®š (25%)
- CPUä½¿ç”¨ç‡ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨­å®š (25%)
- ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨­å®š (25%)
</details>

---

## ğŸ¯ Question 21: ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚­ãƒ³ã‚° (7%)

**Context**: cluster: k8s-cluster-1  
**Task**: 
ä»¥ä¸‹ã®èª¿æŸ»ã‚’è¡Œã„ã€çµæœã‚’`/opt/candidate/network-info.txt`ã«ä¿å­˜ã—ã¦ãã ã•ã„ï¼š
1. ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®Pod CIDRç¯„å›²
2. Service CIDRç¯„å›²
3. CNIï¼ˆContainer Network Interfaceï¼‰ãƒ—ãƒ©ã‚°ã‚¤ãƒ³å
4. kube-proxyã®ãƒ¢ãƒ¼ãƒ‰

<details>
<summary>ğŸ’¡ è§£ç­”ä¾‹</summary>

```bash
# ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æƒ…å ±èª¿æŸ»
echo "=== Kubernetes Cluster Network Information ===" > /opt/candidate/network-info.txt
echo "$(date)" >> /opt/candidate/network-info.txt
echo "" >> /opt/candidate/network-info.txt

# Pod CIDRç¯„å›²ã‚’å–å¾—
echo "=== Pod CIDR Range ===" >> /opt/candidate/network-info.txt
kubectl cluster-info dump | grep -E "cluster-cidr|pod-cidr" >> /opt/candidate/network-info.txt
kubectl get nodes -o jsonpath='{.items[*].spec.podCIDR}' >> /opt/candidate/network-info.txt
echo "" >> /opt/candidate/network-info.txt

# Service CIDRç¯„å›²ã‚’å–å¾—
echo "=== Service CIDR Range ===" >> /opt/candidate/network-info.txt
kubectl cluster-info dump | grep -E "service-cluster-ip-range" >> /opt/candidate/network-info.txt
echo "" >> /opt/candidate/network-info.txt

# CNIãƒ—ãƒ©ã‚°ã‚¤ãƒ³ç¢ºèª
echo "=== CNI Plugin Information ===" >> /opt/candidate/network-info.txt
kubectl get pods -n kube-system | grep -E "(calico|flannel|weave|cilium)" >> /opt/candidate/network-info.txt
ls /etc/cni/net.d/ >> /opt/candidate/network-info.txt
echo "" >> /opt/candidate/network-info.txt

# kube-proxyãƒ¢ãƒ¼ãƒ‰ç¢ºèª
echo "=== Kube-proxy Mode ===" >> /opt/candidate/network-info.txt
kubectl get configmap kube-proxy -n kube-system -o yaml | grep mode >> /opt/candidate/network-info.txt
kubectl logs -n kube-system -l k8s-app=kube-proxy | grep -i mode | head -5 >> /opt/candidate/network-info.txt
echo "" >> /opt/candidate/network-info.txt

# ãã®ä»–ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è¨­å®š
echo "=== Additional Network Settings ===" >> /opt/candidate/network-info.txt
kubectl get services -A | grep ClusterIP | head -5 >> /opt/candidate/network-info.txt

# çµæœç¢ºèª
cat /opt/candidate/network-info.txt
```

**æ¡ç‚¹ãƒã‚¤ãƒ³ãƒˆ:**
- Pod CIDRç¯„å›²ã®æ­£ç¢ºãªå–å¾— (25%)
- Service CIDRç¯„å›²ã®å–å¾— (25%)
- CNIãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã®ç‰¹å®š (25%)
- kube-proxyãƒ¢ãƒ¼ãƒ‰ã®ç¢ºèª (25%)
</details>

---

## ğŸ¯ Question 22: ã‚«ã‚¹ã‚¿ãƒ ãƒªã‚½ãƒ¼ã‚¹å®šç¾© (6%)

**Context**: cluster: k8s-cluster-1  
**Task**: 
ä»¥ä¸‹ã®è¦ä»¶ã§CustomResourceDefinitionã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š
- APIç¾¤: `example.com/v1`
- ç¨®é¡: `WebService`
- è¤‡æ•°å½¢: `webservices`
- ãƒãƒ¼ãƒ ã‚¹ãƒšãƒ¼ã‚¹ç¯„å›²

ä½œæˆå¾Œã€ã‚«ã‚¹ã‚¿ãƒ ãƒªã‚½ãƒ¼ã‚¹ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

<details>
<summary>ğŸ’¡ è§£ç­”ä¾‹</summary>

```bash
# CRD YAMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
cat <<EOF > /opt/candidate/webservice-crd.yaml
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: webservices.example.com
spec:
  group: example.com
  versions:
  - name: v1
    served: true
    storage: true
    schema:
      openAPIV3Schema:
        type: object
        properties:
          spec:
            type: object
            properties:
              replicas:
                type: integer
                minimum: 1
                maximum: 10
              image:
                type: string
              port:
                type: integer
          status:
            type: object
            properties:
              replicas:
                type: integer
  scope: Namespaced
  names:
    plural: webservices
    singular: webservice
    kind: WebService
    shortNames:
    - ws
EOF

# CRDã‚’é©ç”¨
kubectl apply -f /opt/candidate/webservice-crd.yaml

# CRDç¢ºèª
kubectl get crd webservices.example.com

# ã‚«ã‚¹ã‚¿ãƒ ãƒªã‚½ãƒ¼ã‚¹ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ
cat <<EOF > /opt/candidate/my-webservice.yaml
apiVersion: example.com/v1
kind: WebService
metadata:
  name: my-webservice
  namespace: default
spec:
  replicas: 3
  image: nginx:1.20
  port: 80
EOF

kubectl apply -f /opt/candidate/my-webservice.yaml

# ç¢ºèª
kubectl get webservices
kubectl describe webservice my-webservice
kubectl get ws  # shortNameã§ã®ã‚¢ã‚¯ã‚»ã‚¹ç¢ºèª
```

**æ¡ç‚¹ãƒã‚¤ãƒ³ãƒˆ:**
- æ­£ã—ã„CRDä»•æ§˜ã§ã®ä½œæˆ (40%)
- é©åˆ‡ãªã‚¹ã‚­ãƒ¼ãƒå®šç¾© (30%)
- ã‚«ã‚¹ã‚¿ãƒ ãƒªã‚½ãƒ¼ã‚¹ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®ä½œæˆ (30%)
</details>

---

## ğŸ¯ Question 23: etcdã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚° (8%)

**Context**: cluster: k8s-cluster-1  
**Task**: 
etcdã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã«å•é¡ŒãŒç™ºç”Ÿã—ã¦ã„ã¾ã™ã€‚ä»¥ä¸‹ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ï¼š
1. etcdã®çŠ¶æ…‹ç¢ºèª
2. ãƒ¡ãƒ³ãƒãƒ¼ä¸€è¦§å–å¾—
3. å•é¡ŒãŒã‚ã‚Œã°ä¿®å¾©
4. ä¿®å¾©å†…å®¹ã‚’`/opt/candidate/etcd-repair.txt`ã«è¨˜éŒ²

<details>
<summary>ğŸ’¡ è§£ç­”ä¾‹</summary>

```bash
# etcdèª¿æŸ»é–‹å§‹
echo "=== ETCD Troubleshooting Report ===" > /opt/candidate/etcd-repair.txt
echo "$(date)" >> /opt/candidate/etcd-repair.txt
echo "" >> /opt/candidate/etcd-repair.txt

# etcd Podã®çŠ¶æ…‹ç¢ºèª
echo "=== ETCD Pod Status ===" >> /opt/candidate/etcd-repair.txt
kubectl get pods -n kube-system | grep etcd >> /opt/candidate/etcd-repair.txt
echo "" >> /opt/candidate/etcd-repair.txt

# etcd Podè©³ç´°ç¢ºèª
ETCD_POD=$(kubectl get pods -n kube-system -l component=etcd -o jsonpath='{.items[0].metadata.name}')
echo "=== ETCD Pod Details ===" >> /opt/candidate/etcd-repair.txt
kubectl describe pod $ETCD_POD -n kube-system >> /opt/candidate/etcd-repair.txt
echo "" >> /opt/candidate/etcd-repair.txt

# etcdã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼çŠ¶æ…‹ç¢ºèª
echo "=== ETCD Cluster Health ===" >> /opt/candidate/etcd-repair.txt
kubectl exec -n kube-system $ETCD_POD -- etcdctl \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  endpoint health >> /opt/candidate/etcd-repair.txt

echo "" >> /opt/candidate/etcd-repair.txt

# etcdãƒ¡ãƒ³ãƒãƒ¼ç¢ºèª
echo "=== ETCD Members ===" >> /opt/candidate/etcd-repair.txt
kubectl exec -n kube-system $ETCD_POD -- etcdctl \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  member list >> /opt/candidate/etcd-repair.txt

echo "" >> /opt/candidate/etcd-repair.txt

# etcdãƒ­ã‚°ç¢ºèª
echo "=== ETCD Logs ===" >> /opt/candidate/etcd-repair.txt
kubectl logs $ETCD_POD -n kube-system --tail=20 >> /opt/candidate/etcd-repair.txt

# å•é¡ŒãŒã‚ã‚‹å ´åˆã®ä¿®å¾©æ‰‹é †ï¼ˆä¾‹ï¼šç ´æã—ãŸãƒ¡ãƒ³ãƒãƒ¼ã®å‰Šé™¤ï¼‰
# kubectl exec -n kube-system $ETCD_POD -- etcdctl member remove <member-id>

echo "" >> /opt/candidate/etcd-repair.txt
echo "=== Resolution Applied ===" >> /opt/candidate/etcd-repair.txt
echo "1. Checked etcd pod status - running normally" >> /opt/candidate/etcd-repair.txt
echo "2. Verified cluster health - all endpoints healthy" >> /opt/candidate/etcd-repair.txt
echo "3. Confirmed member list - all members active" >> /opt/candidate/etcd-repair.txt

# çµæœç¢ºèª
cat /opt/candidate/etcd-repair.txt
```

**æ¡ç‚¹ãƒã‚¤ãƒ³ãƒˆ:**
- etcdçŠ¶æ…‹ã®é©åˆ‡ãªç¢ºèª (30%)
- ãƒ¡ãƒ³ãƒãƒ¼ä¸€è¦§ã®å–å¾— (25%)
- å•é¡Œã®æ­£ç¢ºãªç‰¹å®š (25%)
- ä¿®å¾©æ‰‹é †ã®è¨˜éŒ² (20%)
</details>

---

## ğŸ¯ Question 24: ãƒªã‚½ãƒ¼ã‚¹åˆ¶é™ã¨ã‚¯ã‚©ãƒ¼ã‚¿ (5%)

**Context**: cluster: k8s-cluster-1, namespace: resource-limits  
**Task**: 
ä»¥ä¸‹ã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š
1. ResourceQuota: CPU 2ã‚³ã‚¢ã€ãƒ¡ãƒ¢ãƒª 4Giã€Podæ•° 10å€‹ã®åˆ¶é™
2. LimitRange: ã‚³ãƒ³ãƒ†ãƒŠã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆCPU 100mã€ãƒ¡ãƒ¢ãƒª 128Mi
3. ãƒ†ã‚¹ãƒˆç”¨Podã‚’ä½œæˆã—ã¦åˆ¶é™ã‚’ç¢ºèª

<details>
<summary>ğŸ’¡ è§£ç­”ä¾‹</summary>

```bash
# namespaceã‚’ä½œæˆ
kubectl create namespace resource-limits

# ResourceQuota YAMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
cat <<EOF > /opt/candidate/resource-quota.yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-quota
  namespace: resource-limits
spec:
  hard:
    requests.cpu: "2"
    requests.memory: 4Gi
    persistentvolumeclaims: "4"
    pods: "10"
EOF

# LimitRange YAMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
cat <<EOF > /opt/candidate/limit-range.yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: limit-range
  namespace: resource-limits
spec:
  limits:
  - default:
      cpu: "200m"
      memory: "256Mi"
    defaultRequest:
      cpu: "100m"
      memory: "128Mi"
    type: Container
EOF

# é©ç”¨
kubectl apply -f /opt/candidate/resource-quota.yaml
kubectl apply -f /opt/candidate/limit-range.yaml

# ç¢ºèª
kubectl get resourcequota -n resource-limits
kubectl get limitrange -n resource-limits
kubectl describe resourcequota compute-quota -n resource-limits
kubectl describe limitrange limit-range -n resource-limits

# ãƒ†ã‚¹ãƒˆç”¨Podã‚’ä½œæˆ
cat <<EOF > /opt/candidate/test-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: test-pod
  namespace: resource-limits
spec:
  containers:
  - name: test
    image: busybox:1.35
    command: ['sh', '-c', 'sleep 3600']
    # resourcesã‚’æŒ‡å®šã—ãªã„å ´åˆã€LimitRangeã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ãŒé©ç”¨ã•ã‚Œã‚‹
EOF

kubectl apply -f /opt/candidate/test-pod.yaml

# ãƒ†ã‚¹ãƒˆç¢ºèª
kubectl get pod test-pod -n resource-limits -o yaml | grep -A 10 resources
kubectl describe resourcequota compute-quota -n resource-limits
```

**æ¡ç‚¹ãƒã‚¤ãƒ³ãƒˆ:**
- ResourceQuotaã®æ­£ã—ã„è¨­å®š (40%)
- LimitRangeã®é©åˆ‡ãªè¨­å®š (40%)
- åˆ¶é™ã®å‹•ä½œç¢ºèª (20%)
</details>

---

## ğŸ¯ Question 25: Cluster Architecture - Control Plane Components (6%)

**Context**: cluster: k8s-cluster-1  
**Task**: 
Control Planeã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®å¥å…¨æ€§ã‚’ç¢ºèªã—ã€ä»¥ä¸‹ã®æƒ…å ±ã‚’`/opt/candidate/control-plane-status.txt`ã«è¨˜éŒ²ã—ã¦ãã ã•ã„ï¼š
1. kube-apiserver ã®çŠ¶æ…‹ã¨ãƒãƒ¼ã‚¸ãƒ§ãƒ³
2. kube-controller-manager ã®çŠ¶æ…‹
3. kube-scheduler ã®çŠ¶æ…‹
4. å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®ãƒ­ã‚°ã§ç•°å¸¸ãŒãªã„ã‹ç¢ºèª

<details>
<summary>ğŸ’¡ è§£ç­”ä¾‹</summary>

```bash
# Control PlaneçŠ¶æ…‹èª¿æŸ»
echo "=== Control Plane Components Status ===" > /opt/candidate/control-plane-status.txt
echo "$(date)" >> /opt/candidate/control-plane-status.txt
echo "" >> /opt/candidate/control-plane-status.txt

# kube-system namespaceã®Podç¢ºèª
echo "=== Control Plane Pods ===" >> /opt/candidate/control-plane-status.txt
kubectl get pods -n kube-system | grep -E "(kube-apiserver|kube-controller|kube-scheduler|etcd)" >> /opt/candidate/control-plane-status.txt
echo "" >> /opt/candidate/control-plane-status.txt

# kube-apiserverçŠ¶æ…‹ã¨ãƒãƒ¼ã‚¸ãƒ§ãƒ³
echo "=== Kube-apiserver Status ===" >> /opt/candidate/control-plane-status.txt
kubectl version --short >> /opt/candidate/control-plane-status.txt
API_POD=$(kubectl get pods -n kube-system -l component=kube-apiserver -o jsonpath='{.items[0].metadata.name}')
kubectl describe pod $API_POD -n kube-system | grep -A 5 -B 5 "Status\|State\|Ready" >> /opt/candidate/control-plane-status.txt
echo "" >> /opt/candidate/control-plane-status.txt

# kube-controller-managerçŠ¶æ…‹
echo "=== Kube-controller-manager Status ===" >> /opt/candidate/control-plane-status.txt
CONTROLLER_POD=$(kubectl get pods -n kube-system -l component=kube-controller-manager -o jsonpath='{.items[0].metadata.name}')
kubectl describe pod $CONTROLLER_POD -n kube-system | grep -A 5 -B 5 "Status\|State\|Ready" >> /opt/candidate/control-plane-status.txt
echo "" >> /opt/candidate/control-plane-status.txt

# kube-schedulerçŠ¶æ…‹
echo "=== Kube-scheduler Status ===" >> /opt/candidate/control-plane-status.txt
SCHEDULER_POD=$(kubectl get pods -n kube-system -l component=kube-scheduler -o jsonpath='{.items[0].metadata.name}')
kubectl describe pod $SCHEDULER_POD -n kube-system | grep -A 5 -B 5 "Status\|State\|Ready" >> /opt/candidate/control-plane-status.txt
echo "" >> /opt/candidate/control-plane-status.txt

# å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®ãƒ­ã‚°ãƒã‚§ãƒƒã‚¯ï¼ˆã‚¨ãƒ©ãƒ¼ãŒãªã„ã‹ï¼‰
echo "=== Recent Logs Check ===" >> /opt/candidate/control-plane-status.txt
echo "--- API Server (last 10 lines) ---" >> /opt/candidate/control-plane-status.txt
kubectl logs $API_POD -n kube-system --tail=10 | grep -i error >> /opt/candidate/control-plane-status.txt
echo "--- Controller Manager (last 10 lines) ---" >> /opt/candidate/control-plane-status.txt
kubectl logs $CONTROLLER_POD -n kube-system --tail=10 | grep -i error >> /opt/candidate/control-plane-status.txt
echo "--- Scheduler (last 10 lines) ---" >> /opt/candidate/control-plane-status.txt
kubectl logs $SCHEDULER_POD -n kube-system --tail=10 | grep -i error >> /opt/candidate/control-plane-status.txt

echo "" >> /opt/candidate/control-plane-status.txt
echo "=== Component Health Summary ===" >> /opt/candidate/control-plane-status.txt
kubectl get componentstatuses 2>/dev/null || echo "componentstatuses API deprecated" >> /opt/candidate/control-plane-status.txt

# çµæœç¢ºèª
cat /opt/candidate/control-plane-status.txt
```

**æ¡ç‚¹ãƒã‚¤ãƒ³ãƒˆ:**
- Control Planeã®PodçŠ¶æ…‹ç¢ºèª (25%)
- kube-apiserverã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèª (25%)
- å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®å¥å…¨æ€§ç¢ºèª (25%)
- ãƒ­ã‚°ã§ã®ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯ (25%)
</details>

---

## ğŸ¯ Question 26: Multi-Container Pod Design (5%)

**Context**: cluster: k8s-cluster-1, namespace: multi-container  
**Task**: 
ä»¥ä¸‹ã®è¦ä»¶ã§Multi-Container Podã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š
- ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒŠ: `nginx:1.20` (ãƒãƒ¼ãƒˆ80)
- ã‚µã‚¤ãƒ‰ã‚«ãƒ¼: `busybox:1.35` (ãƒ­ã‚°åé›†ç”¨ã€å…±æœ‰ãƒœãƒªãƒ¥ãƒ¼ãƒ ä½¿ç”¨)
- å…±æœ‰ãƒœãƒªãƒ¥ãƒ¼ãƒ : `/var/log/nginx` ã‚’ãƒã‚¦ãƒ³ãƒˆ

<details>
<summary>ğŸ’¡ è§£ç­”ä¾‹</summary>

```bash
# namespaceã‚’ä½œæˆ
kubectl create namespace multi-container

# Multi-Container Pod YAMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
cat <<EOF > /opt/candidate/multi-container-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: web-with-sidecar
  namespace: multi-container
spec:
  containers:
  - name: web-server
    image: nginx:1.20
    ports:
    - containerPort: 80
    volumeMounts:
    - name: shared-logs
      mountPath: /var/log/nginx
  - name: log-collector
    image: busybox:1.35
    command: ['sh', '-c', 'while true; do tail -f /var/log/nginx/access.log; sleep 30; done']
    volumeMounts:
    - name: shared-logs
      mountPath: /var/log/nginx
  volumes:
  - name: shared-logs
    emptyDir: {}
EOF

# Podã‚’é©ç”¨
kubectl apply -f /opt/candidate/multi-container-pod.yaml

# ç¢ºèª
kubectl get pod web-with-sidecar -n multi-container
kubectl logs web-with-sidecar -c web-server -n multi-container
kubectl logs web-with-sidecar -c log-collector -n multi-container
```

**æ¡ç‚¹ãƒã‚¤ãƒ³ãƒˆ:**
- æ­£ã—ã„ãƒãƒ«ãƒã‚³ãƒ³ãƒ†ãƒŠè¨­å®š (30%)
- å…±æœ‰ãƒœãƒªãƒ¥ãƒ¼ãƒ ã®è¨­å®š (30%)
- å„ã‚³ãƒ³ãƒ†ãƒŠã®é©åˆ‡ãªè¨­å®š (40%)
</details>

---

## ğŸ¯ Question 27: Pod Security Context (5%)

**Context**: cluster: k8s-cluster-1, namespace: security-context  
**Task**: 
ä»¥ä¸‹ã®ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¦ä»¶ã§Podã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š
- érootãƒ¦ãƒ¼ã‚¶ãƒ¼ï¼ˆUID: 1000ï¼‰ã§å®Ÿè¡Œ
- èª­ã¿å–ã‚Šå°‚ç”¨ãƒ«ãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ 
- ç‰¹æ¨©ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç¦æ­¢
- Capabilities: NET_ADMIN ã‚’è¿½åŠ 

<details>
<summary>ğŸ’¡ è§£ç­”ä¾‹</summary>

```bash
# namespaceã‚’ä½œæˆ
kubectl create namespace security-context

# Security Context Pod YAMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
cat <<EOF > /opt/candidate/secure-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: secure-pod
  namespace: security-context
spec:
  securityContext:
    runAsUser: 1000
    runAsNonRoot: true
    fsGroup: 2000
  containers:
  - name: secure-container
    image: busybox:1.35
    command: ['sh', '-c', 'id; sleep 3600']
    securityContext:
      readOnlyRootFilesystem: true
      allowPrivilegeEscalation: false
      capabilities:
        add:
        - NET_ADMIN
        drop:
        - ALL
    volumeMounts:
    - name: tmp-volume
      mountPath: /tmp
  volumes:
  - name: tmp-volume
    emptyDir: {}
EOF

# Podã‚’é©ç”¨
kubectl apply -f /opt/candidate/secure-pod.yaml

# ç¢ºèª
kubectl get pod secure-pod -n security-context
kubectl logs secure-pod -n security-context
kubectl exec secure-pod -n security-context -- whoami
```

**æ¡ç‚¹ãƒã‚¤ãƒ³ãƒˆ:**
- érootãƒ¦ãƒ¼ã‚¶ãƒ¼è¨­å®š (25%)
- èª­ã¿å–ã‚Šå°‚ç”¨ãƒ«ãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ  (25%)
- ç‰¹æ¨©ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç¦æ­¢ (25%)
- Capabilitiesè¨­å®š (25%)
</details>

---

## ğŸ¯ Question 28: InitContainer Implementation (4%)

**Context**: cluster: k8s-cluster-1, namespace: init-container  
**Task**: 
ä»¥ä¸‹ã®è¦ä»¶ã§InitContainerã‚’ä½¿ç”¨ã™ã‚‹Podã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š
- InitContainer: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚’ç¢ºèª
- ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒŠ: Webã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
- InitContainerãŒæˆåŠŸã—ãŸå ´åˆã®ã¿ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒŠã‚’é–‹å§‹

<details>
<summary>ğŸ’¡ è§£ç­”ä¾‹</summary>

```bash
# namespaceã‚’ä½œæˆ
kubectl create namespace init-container

# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹Podã‚’ä½œæˆï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰
kubectl run db-pod --image=mysql:8.0 --env="MYSQL_ROOT_PASSWORD=password" -n init-container

# InitContainer Pod YAMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
cat <<EOF > /opt/candidate/init-container-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: web-app
  namespace: init-container
spec:
  initContainers:
  - name: init-db-check
    image: busybox:1.35
    command: ['sh', '-c']
    args:
    - 'until nc -z db-pod 3306; do echo "Waiting for database..."; sleep 2; done; echo "Database is ready!"'
  containers:
  - name: web-app
    image: nginx:1.20
    ports:
    - containerPort: 80
EOF

# Podã‚’é©ç”¨
kubectl apply -f /opt/candidate/init-container-pod.yaml

# ç¢ºèª
kubectl get pod web-app -n init-container
kubectl describe pod web-app -n init-container
kubectl logs web-app -c init-db-check -n init-container
```

**æ¡ç‚¹ãƒã‚¤ãƒ³ãƒˆ:**
- InitContainerã®æ­£ã—ã„è¨­å®š (40%)
- ä¾å­˜é–¢ä¿‚ãƒã‚§ãƒƒã‚¯ã®å®Ÿè£… (30%)
- ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒŠã®é©åˆ‡ãªé–‹å§‹ (30%)
</details>

---

## ğŸ¯ Question 29: Volume Management - EmptyDir and HostPath (6%)

**Context**: cluster: k8s-cluster-1, namespace: volume-test  
**Task**: 
ä»¥ä¸‹ã®ãƒœãƒªãƒ¥ãƒ¼ãƒ è¨­å®šã§Podã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š
1. EmptyDirãƒœãƒªãƒ¥ãƒ¼ãƒ : `/tmp/shared`ã«ãƒã‚¦ãƒ³ãƒˆ
2. HostPathãƒœãƒªãƒ¥ãƒ¼ãƒ : ãƒ›ã‚¹ãƒˆã®`/var/log`ã‚’`/host-logs`ã«ãƒã‚¦ãƒ³ãƒˆ
3. ä¸¡ãƒœãƒªãƒ¥ãƒ¼ãƒ ã§ã®ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆãƒ†ã‚¹ãƒˆ

<details>
<summary>ğŸ’¡ è§£ç­”ä¾‹</summary>

```bash
# namespaceã‚’ä½œæˆ
kubectl create namespace volume-test

# Volume Test Pod YAMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
cat <<EOF > /opt/candidate/volume-test-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: volume-test-pod
  namespace: volume-test
spec:
  containers:
  - name: test-container
    image: busybox:1.35
    command: ['sh', '-c']
    args:
    - 'echo "Creating files in volumes...";
       echo "EmptyDir test" > /tmp/shared/emptydir-test.txt;
       echo "HostPath test" > /host-logs/hostpath-test.txt;
       ls -la /tmp/shared/; ls -la /host-logs/;
       sleep 3600'
    volumeMounts:
    - name: empty-dir-volume
      mountPath: /tmp/shared
    - name: host-path-volume
      mountPath: /host-logs
  volumes:
  - name: empty-dir-volume
    emptyDir: {}
  - name: host-path-volume
    hostPath:
      path: /var/log
      type: Directory
EOF

# Podã‚’é©ç”¨
kubectl apply -f /opt/candidate/volume-test-pod.yaml

# ç¢ºèª
kubectl get pod volume-test-pod -n volume-test
kubectl logs volume-test-pod -n volume-test
kubectl exec volume-test-pod -n volume-test -- ls -la /tmp/shared/
kubectl exec volume-test-pod -n volume-test -- ls -la /host-logs/
```

**æ¡ç‚¹ãƒã‚¤ãƒ³ãƒˆ:**
- EmptyDirãƒœãƒªãƒ¥ãƒ¼ãƒ ã®æ­£ã—ã„è¨­å®š (30%)
- HostPathãƒœãƒªãƒ¥ãƒ¼ãƒ ã®é©åˆ‡ãªè¨­å®š (40%)
- ãƒœãƒªãƒ¥ãƒ¼ãƒ ãƒã‚¦ãƒ³ãƒˆã¨ãƒ†ã‚¹ãƒˆ (30%)
</details>

---

## ğŸ¯ Question 30: Kubernetes Service Discovery (5%)

**Context**: cluster: k8s-cluster-1, namespace: service-discovery  
**Task**: 
Service Discoveryã®å‹•ä½œã‚’ç¢ºèªã—ã¦ãã ã•ã„ï¼š
1. Backend Deploymentã¨ã‚µãƒ¼ãƒ“ã‚¹ã‚’ä½œæˆ
2. Client Podã‹ã‚‰ã‚µãƒ¼ãƒ“ã‚¹åã€DNSã€ç’°å¢ƒå¤‰æ•°ã§ã®æ¥ç¶šã‚’ç¢ºèª
3. æ¥ç¶šçµæœã‚’`/opt/candidate/service-discovery.txt`ã«è¨˜éŒ²

<details>
<summary>ğŸ’¡ è§£ç­”ä¾‹</summary>

```bash
# namespaceã‚’ä½œæˆ
kubectl create namespace service-discovery

# Backend Deploymentã¨ã‚µãƒ¼ãƒ“ã‚¹ã‚’ä½œæˆ
kubectl create deployment backend --image=nginx:1.20 -n service-discovery
kubectl expose deployment backend --port=80 --target-port=80 -n service-discovery

# Client Pod YAMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
cat <<EOF > /opt/candidate/client-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: client-pod
  namespace: service-discovery
spec:
  containers:
  - name: client
    image: busybox:1.35
    command: ['sh', '-c', 'sleep 3600']
EOF

kubectl apply -f /opt/candidate/client-pod.yaml
kubectl wait --for=condition=Ready pod/client-pod -n service-discovery --timeout=300s

# Service Discovery ãƒ†ã‚¹ãƒˆ
echo "=== Kubernetes Service Discovery Test ===" > /opt/candidate/service-discovery.txt
echo "$(date)" >> /opt/candidate/service-discovery.txt
echo "" >> /opt/candidate/service-discovery.txt

# 1. ã‚µãƒ¼ãƒ“ã‚¹åã§ã®æ¥ç¶š
echo "=== Service Name Resolution ===" >> /opt/candidate/service-discovery.txt
kubectl exec client-pod -n service-discovery -- nslookup backend >> /opt/candidate/service-discovery.txt
echo "" >> /opt/candidate/service-discovery.txt

# 2. FQDN ã§ã®æ¥ç¶š
echo "=== FQDN Resolution ===" >> /opt/candidate/service-discovery.txt
kubectl exec client-pod -n service-discovery -- nslookup backend.service-discovery.svc.cluster.local >> /opt/candidate/service-discovery.txt
echo "" >> /opt/candidate/service-discovery.txt

# 3. ç’°å¢ƒå¤‰æ•°ã®ç¢ºèª
echo "=== Environment Variables ===" >> /opt/candidate/service-discovery.txt
kubectl exec client-pod -n service-discovery -- env | grep BACKEND >> /opt/candidate/service-discovery.txt
echo "" >> /opt/candidate/service-discovery.txt

# 4. HTTPæ¥ç¶šãƒ†ã‚¹ãƒˆ
echo "=== HTTP Connection Test ===" >> /opt/candidate/service-discovery.txt
kubectl exec client-pod -n service-discovery -- wget -qO- backend:80 | head -5 >> /opt/candidate/service-discovery.txt

# çµæœç¢ºèª
cat /opt/candidate/service-discovery.txt
```

**æ¡ç‚¹ãƒã‚¤ãƒ³ãƒˆ:**
- ã‚µãƒ¼ãƒ“ã‚¹åè§£æ±ºã®ç¢ºèª (25%)
- DNSè§£æ±ºã®ç¢ºèª (25%)
- ç’°å¢ƒå¤‰æ•°ã®ç¢ºèª (25%)
- HTTPæ¥ç¶šãƒ†ã‚¹ãƒˆ (25%)
</details>

---

## ğŸ¯ Question 31: Pod Security Context (4%)

**Context**: cluster: k8s-cluster-1, namespace: security-test  
**Task**: 
ä»¥ä¸‹ã®ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¨­å®šã§Podã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š
- Podå: `secure-pod`
- érootãƒ¦ãƒ¼ã‚¶ãƒ¼ï¼ˆUID: 1000ï¼‰ã§å®Ÿè¡Œ
- èª­ã¿å–ã‚Šå°‚ç”¨ãƒ«ãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ 
- Linux capabilities: NET_ADMIN ã‚’è¿½åŠ 

<details>
<summary>ğŸ’¡ è§£ç­”ä¾‹</summary>

```bash
# namespaceã‚’ä½œæˆ
kubectl create namespace security-test

# Secure Pod YAMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
cat <<EOF > /opt/candidate/secure-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: secure-pod
  namespace: security-test
spec:
  securityContext:
    runAsUser: 1000
    runAsNonRoot: true
    fsGroup: 2000
  containers:
  - name: secure-container
    image: nginx:1.20
    securityContext:
      readOnlyRootFilesystem: true
      capabilities:
        add:
        - NET_ADMIN
      allowPrivilegeEscalation: false
    volumeMounts:
    - name: tmp-volume
      mountPath: /tmp
    - name: cache-volume
      mountPath: /var/cache/nginx
    - name: run-volume
      mountPath: /var/run
  volumes:
  - name: tmp-volume
    emptyDir: {}
  - name: cache-volume
    emptyDir: {}
  - name: run-volume
    emptyDir: {}
EOF

kubectl apply -f /opt/candidate/secure-pod.yaml

# ç¢ºèª
kubectl get pod secure-pod -n security-test
kubectl describe pod secure-pod -n security-test
kubectl exec secure-pod -n security-test -- id
```

**æ¡ç‚¹ãƒã‚¤ãƒ³ãƒˆ:**
- runAsUserè¨­å®š (25%)
- readOnlyRootFilesystemè¨­å®š (25%)
- capabilitiesè¨­å®š (25%)
- å¿…è¦ãªãƒœãƒªãƒ¥ãƒ¼ãƒ ãƒã‚¦ãƒ³ãƒˆ (25%)
</details>

---

## ğŸ¯ Question 32: Network Policy Implementation (7%)

**Context**: cluster: k8s-cluster-1, namespace: network-policy-test  
**Task**: 
ä»¥ä¸‹ã®Network Policyã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š
- åå‰: `web-netpol`
- `app=web`ãƒ©ãƒ™ãƒ«ã®Podã«é©ç”¨
- `app=db`ã‹ã‚‰ã®ã¿3306ãƒãƒ¼ãƒˆã¸ã®Ingressè¨±å¯
- ã™ã¹ã¦ã®Egressã‚’è¨±å¯

<details>
<summary>ğŸ’¡ è§£ç­”ä¾‹</summary>

```bash
# namespaceã‚’ä½œæˆ
kubectl create namespace network-policy-test

# ãƒ†ã‚¹ãƒˆç”¨Podã‚’ä½œæˆ
kubectl run web-pod --image=nginx:1.20 --labels="app=web" -n network-policy-test
kubectl run db-pod --image=mysql:8.0 --labels="app=db" --env="MYSQL_ROOT_PASSWORD=password" -n network-policy-test
kubectl run client-pod --image=busybox:1.35 --labels="app=client" --command -n network-policy-test -- sleep 3600

# Network Policy YAMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
cat <<EOF > /opt/candidate/web-netpol.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: web-netpol
  namespace: network-policy-test
spec:
  podSelector:
    matchLabels:
      app: web
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: db
    ports:
    - protocol: TCP
      port: 3306
  egress:
  - {}
EOF

kubectl apply -f /opt/candidate/web-netpol.yaml

# ç¢ºèª
kubectl get networkpolicy -n network-policy-test
kubectl describe networkpolicy web-netpol -n network-policy-test
```

**æ¡ç‚¹ãƒã‚¤ãƒ³ãƒˆ:**
- æ­£ã—ã„podSelectorè¨­å®š (25%)
- Ingressãƒ«ãƒ¼ãƒ«ã®é©åˆ‡ãªè¨­å®š (40%)
- Egressãƒ«ãƒ¼ãƒ«ã®è¨­å®š (20%)
- ãƒãƒ¼ãƒˆæŒ‡å®šã®æ­£ç¢ºæ€§ (15%)
</details>

---

## ğŸ¯ Question 33: Custom Resource Definition (6%)

**Context**: cluster: k8s-cluster-1  
**Task**: 
ä»¥ä¸‹ã®ä»•æ§˜ã§Custom Resource Definitionã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š
- APIVersion: `apiextensions.k8s.io/v1`
- Kind: `Website`
- Group: `web.example.com`
- Version: `v1`
- Scope: Namespaced

<details>
<summary>ğŸ’¡ è§£ç­”ä¾‹</summary>

```bash
# Custom Resource Definition YAMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
cat <<EOF > /opt/candidate/website-crd.yaml
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: websites.web.example.com
spec:
  group: web.example.com
  versions:
  - name: v1
    served: true
    storage: true
    schema:
      openAPIV3Schema:
        type: object
        properties:
          spec:
            type: object
            properties:
              url:
                type: string
              replicas:
                type: integer
                minimum: 1
                maximum: 10
          status:
            type: object
  scope: Namespaced
  names:
    plural: websites
    singular: website
    kind: Website
    shortNames:
    - web
EOF

kubectl apply -f /opt/candidate/website-crd.yaml

# CRDã®ç¢ºèª
kubectl get crd websites.web.example.com
kubectl describe crd websites.web.example.com

# Custom Resourceã®ãƒ†ã‚¹ãƒˆä½œæˆ
cat <<EOF > /opt/candidate/example-website.yaml
apiVersion: web.example.com/v1
kind: Website
metadata:
  name: example-site
  namespace: default
spec:
  url: "https://example.com"
  replicas: 3
EOF

kubectl apply -f /opt/candidate/example-website.yaml
kubectl get websites
```

**æ¡ç‚¹ãƒã‚¤ãƒ³ãƒˆ:**
- æ­£ã—ã„CRDæ§‹é€  (30%)
- ã‚¹ã‚­ãƒ¼ãƒå®šç¾© (25%)
- é©åˆ‡ãªãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¨­å®š (25%)
- Custom Resourceã®ä½œæˆãƒ†ã‚¹ãƒˆ (20%)
</details>

---

## ğŸ¯ Question 34: Resource Quotas and Limits (5%)

**Context**: cluster: k8s-cluster-1, namespace: quota-test  
**Task**: 
ä»¥ä¸‹ã®ãƒªã‚½ãƒ¼ã‚¹åˆ¶é™ã‚’è¨­å®šã—ã¦ãã ã•ã„ï¼š
- ResourceQuota: CPU 2ã‚³ã‚¢ã€ãƒ¡ãƒ¢ãƒª4Giã€Podæ•°æœ€å¤§10
- LimitRange: Pod CPUæœ€å¤§500mã€ãƒ¡ãƒ¢ãƒªæœ€å¤§1Gi

<details>
<summary>ğŸ’¡ è§£ç­”ä¾‹</summary>

```bash
# namespaceã‚’ä½œæˆ
kubectl create namespace quota-test

# ResourceQuota YAMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
cat <<EOF > /opt/candidate/resource-quota.yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-quota
  namespace: quota-test
spec:
  hard:
    requests.cpu: "2"
    requests.memory: 4Gi
    limits.cpu: "2"
    limits.memory: 4Gi
    pods: "10"
    persistentvolumeclaims: "4"
EOF

# LimitRange YAMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
cat <<EOF > /opt/candidate/limit-range.yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: pod-limit-range
  namespace: quota-test
spec:
  limits:
  - default:
      cpu: 500m
      memory: 1Gi
    defaultRequest:
      cpu: 100m
      memory: 128Mi
    max:
      cpu: 500m
      memory: 1Gi
    min:
      cpu: 50m
      memory: 64Mi
    type: Container
EOF

# é©ç”¨
kubectl apply -f /opt/candidate/resource-quota.yaml
kubectl apply -f /opt/candidate/limit-range.yaml

# ç¢ºèª
kubectl get resourcequota -n quota-test
kubectl get limitrange -n quota-test
kubectl describe namespace quota-test

# ãƒ†ã‚¹ãƒˆç”¨Podã‚’ä½œæˆ
kubectl run test-pod --image=nginx:1.20 -n quota-test
kubectl describe pod test-pod -n quota-test
```

**æ¡ç‚¹ãƒã‚¤ãƒ³ãƒˆ:**
- ResourceQuotaã®æ­£ã—ã„è¨­å®š (40%)
- LimitRangeã®é©åˆ‡ãªè¨­å®š (40%)
- åˆ¶é™ã®å‹•ä½œç¢ºèª (20%)
</details>

---

## ğŸ¯ Question 35: Taints and Tolerations (6%)

**Context**: cluster: k8s-cluster-1  
**Task**: 
ä»¥ä¸‹ã®è¨­å®šã‚’è¡Œã£ã¦ãã ã•ã„ï¼š
1. ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒãƒ¼ãƒ‰ã«`env=production:NoSchedule`ã®Taintã‚’è¿½åŠ 
2. `production-pod`ã‚’ä½œæˆã—ã€å¯¾å¿œã™ã‚‹Tolerationã‚’è¨­å®š
3. Taintè¨­å®šã®ç¢ºèªã¨Podã®ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°æ¤œè¨¼

<details>
<summary>ğŸ’¡ è§£ç­”ä¾‹</summary>

```bash
# ãƒãƒ¼ãƒ‰ä¸€è¦§ã‚’ç¢ºèª
kubectl get nodes

# ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒãƒ¼ãƒ‰ã‚’ç‰¹å®šï¼ˆmasterä»¥å¤–ï¼‰
WORKER_NODE=$(kubectl get nodes --no-headers | grep -v master | head -1 | awk '{print $1}')

# Taintã‚’è¿½åŠ 
kubectl taint nodes $WORKER_NODE env=production:NoSchedule

# Taintç¢ºèª
kubectl describe node $WORKER_NODE | grep Taints

# Tolerationä»˜ãPod YAMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
cat <<EOF > /opt/candidate/production-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: production-pod
spec:
  tolerations:
  - key: env
    operator: Equal
    value: production
    effect: NoSchedule
  containers:
  - name: nginx
    image: nginx:1.20
  nodeSelector:
    kubernetes.io/hostname: $WORKER_NODE
EOF

# ãƒ†ã‚¹ãƒˆç”¨Podï¼ˆTolerationãªã—ï¼‰
cat <<EOF > /opt/candidate/regular-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: regular-pod
spec:
  containers:
  - name: nginx
    image: nginx:1.20
  nodeSelector:
    kubernetes.io/hostname: $WORKER_NODE
EOF

# Podã‚’é©ç”¨
kubectl apply -f /opt/candidate/production-pod.yaml
kubectl apply -f /opt/candidate/regular-pod.yaml

# ç¢ºèª
kubectl get pods -o wide
kubectl describe pod production-pod
kubectl describe pod regular-pod

# Taintã‚’å‰Šé™¤ï¼ˆã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ç”¨ï¼‰
echo "To remove taint: kubectl taint nodes $WORKER_NODE env=production:NoSchedule-"
```

**æ¡ç‚¹ãƒã‚¤ãƒ³ãƒˆ:**
- Taintã®æ­£ã—ã„è¿½åŠ  (30%)
- Tolerationã®é©åˆ‡ãªè¨­å®š (30%)
- ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°å‹•ä½œã®ç¢ºèª (25%)
- ãƒ†ã‚¹ãƒˆæ¤œè¨¼ (15%)
</details>

---

## ğŸ¯ Question 36: StatefulSet with Persistent Storage (8%)

**Context**: cluster: k8s-cluster-1, namespace: stateful-test  
**Task**: 
ä»¥ä¸‹ã®è¦ä»¶ã§StatefulSetã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š
- åå‰: `mysql-stateful`
- ãƒ¬ãƒ—ãƒªã‚«æ•°: 3
- å„Podã«PersistentVolumeï¼ˆ5Giï¼‰ã‚’è‡ªå‹•ãƒ—ãƒ­ãƒ“ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°
- Service: `mysql-service` (ClusterIP: None)

<details>
<summary>ğŸ’¡ è§£ç­”ä¾‹</summary>

```bash
# namespaceã‚’ä½œæˆ
kubectl create namespace stateful-test

# Headless Service YAMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
cat <<EOF > /opt/candidate/mysql-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: mysql-service
  namespace: stateful-test
spec:
  clusterIP: None
  selector:
    app: mysql
  ports:
  - port: 3306
    targetPort: 3306
EOF

# StatefulSet YAMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
cat <<EOF > /opt/candidate/mysql-stateful.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql-stateful
  namespace: stateful-test
spec:
  serviceName: mysql-service
  replicas: 3
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
      - name: mysql
        image: mysql:8.0
        env:
        - name: MYSQL_ROOT_PASSWORD
          value: "rootpassword"
        - name: MYSQL_DATABASE
          value: "testdb"
        ports:
        - containerPort: 3306
        volumeMounts:
        - name: mysql-storage
          mountPath: /var/lib/mysql
        resources:
          requests:
            cpu: 500m
            memory: 1Gi
          limits:
            cpu: 1000m
            memory: 2Gi
  volumeClaimTemplates:
  - metadata:
      name: mysql-storage
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 5Gi
EOF

# é©ç”¨
kubectl apply -f /opt/candidate/mysql-service.yaml
kubectl apply -f /opt/candidate/mysql-stateful.yaml

# ç¢ºèª
kubectl get statefulset -n stateful-test
kubectl get pods -n stateful-test
kubectl get pvc -n stateful-test
kubectl get service -n stateful-test

# StatefulSetã®è©³ç´°ç¢ºèª
kubectl describe statefulset mysql-stateful -n stateful-test

# å„Podã®ç¢ºèª
kubectl exec mysql-stateful-0 -n stateful-test -- mysql -u root -prootpassword -e "SHOW DATABASES;"
```

**æ¡ç‚¹ãƒã‚¤ãƒ³ãƒˆ:**
- Headless Serviceã®ä½œæˆ (20%)
- StatefulSetã®æ­£ã—ã„è¨­å®š (30%)
- volumeClaimTemplatesã®è¨­å®š (30%)
- PVCã®è‡ªå‹•ãƒ—ãƒ­ãƒ“ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°ç¢ºèª (20%)
</details>

---

## ğŸ¯ Question 37: Pod Disruption Budget (4%)

**Context**: cluster: k8s-cluster-1, namespace: pdb-test  
**Task**: 
ä»¥ä¸‹ã®è¦ä»¶ã§Pod Disruption Budgetã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š
- åå‰: `web-pdb`
- ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ: `app=web`ãƒ©ãƒ™ãƒ«ã®Pod
- æœ€å°åˆ©ç”¨å¯èƒ½Podæ•°: 2
- Deploymentã‚‚ä½œæˆã—ã¦å‹•ä½œç¢ºèª

<details>
<summary>ğŸ’¡ è§£ç­”ä¾‹</summary>

```bash
# namespaceã‚’ä½œæˆ
kubectl create namespace pdb-test

# Deployment YAMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
cat <<EOF > /opt/candidate/web-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-app
  namespace: pdb-test
spec:
  replicas: 4
  selector:
    matchLabels:
      app: web
  template:
    metadata:
      labels:
        app: web
    spec:
      containers:
      - name: web
        image: nginx:1.20
        ports:
        - containerPort: 80
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
EOF

# PodDisruptionBudget YAMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
cat <<EOF > /opt/candidate/web-pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: web-pdb
  namespace: pdb-test
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: web
EOF

# é©ç”¨
kubectl apply -f /opt/candidate/web-deployment.yaml
kubectl apply -f /opt/candidate/web-pdb.yaml

# ç¢ºèª
kubectl get deployment -n pdb-test
kubectl get pods -n pdb-test
kubectl get pdb -n pdb-test
kubectl describe pdb web-pdb -n pdb-test

# Drain ãƒ†ã‚¹ãƒˆï¼ˆå®Ÿéš›ã«ã¯ãƒ†ã‚¹ãƒˆç’°å¢ƒã§ã®ã¿å®Ÿè¡Œï¼‰
# kubectl drain <worker-node> --ignore-daemonsets --delete-emptydir-data --force
```

**æ¡ç‚¹ãƒã‚¤ãƒ³ãƒˆ:**
- PDBã®æ­£ã—ã„ä½œæˆ (40%)
- é©åˆ‡ãªã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼è¨­å®š (30%)
- minAvailableã®è¨­å®š (20%)
- å‹•ä½œç¢ºèª (10%)
</details>

---

## ğŸ¯ Question 38: Service Mesh - Istio Basics (7%)

**Context**: cluster: k8s-cluster-1, namespace: istio-test  
**Task**: 
ä»¥ä¸‹ã®Istioè¨­å®šã‚’è¡Œã£ã¦ãã ã•ã„ï¼š
1. namespaceã§sidecar injectionã‚’æœ‰åŠ¹åŒ–
2. VirtualServiceã§ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯åˆ†å‰²ï¼ˆv1: 80%, v2: 20%ï¼‰
3. DestinationRuleã§ã‚µãƒ–ã‚»ãƒƒãƒˆå®šç¾©

<details>
<summary>ğŸ’¡ è§£ç­”ä¾‹</summary>

```bash
# namespaceã‚’ä½œæˆã—ã¦sidecar injectionã‚’æœ‰åŠ¹åŒ–
kubectl create namespace istio-test
kubectl label namespace istio-test istio-injection=enabled

# ãƒ†ã‚¹ãƒˆã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆv1, v2ï¼‰ã‚’ãƒ‡ãƒ—ãƒ­ã‚¤
cat <<EOF > /opt/candidate/app-v1.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-v1
  namespace: istio-test
spec:
  replicas: 2
  selector:
    matchLabels:
      app: myapp
      version: v1
  template:
    metadata:
      labels:
        app: myapp
        version: v1
    spec:
      containers:
      - name: app
        image: nginx:1.20
        ports:
        - containerPort: 80
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-v2
  namespace: istio-test
spec:
  replicas: 2
  selector:
    matchLabels:
      app: myapp
      version: v2
  template:
    metadata:
      labels:
        app: myapp
        version: v2
    spec:
      containers:
      - name: app
        image: nginx:1.21
        ports:
        - containerPort: 80
EOF

# Serviceä½œæˆ
cat <<EOF > /opt/candidate/app-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: myapp-service
  namespace: istio-test
spec:
  selector:
    app: myapp
  ports:
  - port: 80
    targetPort: 80
EOF

# DestinationRuleä½œæˆ
cat <<EOF > /opt/candidate/destination-rule.yaml
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: myapp-destination
  namespace: istio-test
spec:
  host: myapp-service
  subsets:
  - name: v1
    labels:
      version: v1
  - name: v2
    labels:
      version: v2
EOF

# VirtualServiceä½œæˆï¼ˆãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯åˆ†å‰²ï¼‰
cat <<EOF > /opt/candidate/virtual-service.yaml
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: myapp-virtual-service
  namespace: istio-test
spec:
  hosts:
  - myapp-service
  http:
  - match:
    - headers:
        end-user:
          exact: jason
    route:
    - destination:
        host: myapp-service
        subset: v2
  - route:
    - destination:
        host: myapp-service
        subset: v1
      weight: 80
    - destination:
        host: myapp-service
        subset: v2
      weight: 20
EOF

# é©ç”¨
kubectl apply -f /opt/candidate/app-v1.yaml
kubectl apply -f /opt/candidate/app-service.yaml
kubectl apply -f /opt/candidate/destination-rule.yaml
kubectl apply -f /opt/candidate/virtual-service.yaml

# ç¢ºèª
kubectl get pods -n istio-test
kubectl get svc -n istio-test
kubectl get destinationrule -n istio-test
kubectl get virtualservice -n istio-test
```

**æ¡ç‚¹ãƒã‚¤ãƒ³ãƒˆ:**
- sidecar injectionã®æœ‰åŠ¹åŒ– (20%)
- DestinationRuleã®æ­£ã—ã„è¨­å®š (30%)
- VirtualServiceã®ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯åˆ†å‰² (40%)
- è¨­å®šã®ç¢ºèª (10%)
</details>

---

## ğŸ¯ Question 39: Kubernetes API Server Troubleshooting (6%)

**Context**: cluster: k8s-cluster-1  
**Task**: 
API Serverã«æ¥ç¶šã§ããªã„å•é¡Œã‚’è¨ºæ–­ã—ã€ä¿®å¾©ã—ã¦ãã ã•ã„ï¼š
1. API Server Podã®çŠ¶æ…‹ç¢ºèª
2. è¨¼æ˜æ›¸ã®æœ‰åŠ¹æ€§ç¢ºèª
3. è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®æ¤œè¨¼
4. ä¿®å¾©æ‰‹é †ã®è¨˜éŒ²

<details>
<summary>ğŸ’¡ è§£ç­”ä¾‹</summary>

```bash
# ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°æ‰‹é †ã‚’è¨˜éŒ²
echo "=== API Server Troubleshooting Log ===" > /opt/candidate/api-troubleshooting.txt
echo "$(date)" >> /opt/candidate/api-troubleshooting.txt
echo "" >> /opt/candidate/api-troubleshooting.txt

# 1. API Server Podã®ç¢ºèª
echo "1. Checking API Server Pod Status:" >> /opt/candidate/api-troubleshooting.txt
kubectl get pods -n kube-system | grep apiserver >> /opt/candidate/api-troubleshooting.txt 2>&1

# API ServerãŒå‹•ã„ã¦ã„ãªã„å ´åˆã€é™çš„Podãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆã‚’ç¢ºèª
echo "2. Checking API Server Static Pod Manifest:" >> /opt/candidate/api-troubleshooting.txt
ls -la /etc/kubernetes/manifests/ >> /opt/candidate/api-troubleshooting.txt
cat /etc/kubernetes/manifests/kube-apiserver.yaml >> /opt/candidate/api-troubleshooting.txt

# 3. è¨¼æ˜æ›¸ã®ç¢ºèª
echo "3. Checking API Server Certificates:" >> /opt/candidate/api-troubleshooting.txt
openssl x509 -in /etc/kubernetes/pki/apiserver.crt -text -noout | grep -A 2 "Validity" >> /opt/candidate/api-troubleshooting.txt

# 4. etcdã®çŠ¶æ…‹ç¢ºèª
echo "4. Checking etcd Status:" >> /opt/candidate/api-troubleshooting.txt
kubectl get pods -n kube-system | grep etcd >> /opt/candidate/api-troubleshooting.txt 2>&1

# 5. kubeletãƒ­ã‚°ã®ç¢ºèª
echo "5. Checking kubelet logs:" >> /opt/candidate/api-troubleshooting.txt
sudo journalctl -u kubelet --no-pager -l | tail -20 >> /opt/candidate/api-troubleshooting.txt

# 6. API Server containerdãƒ­ã‚°ã®ç¢ºèª
echo "6. Checking API Server container logs:" >> /opt/candidate/api-troubleshooting.txt
sudo crictl logs $(sudo crictl ps -a | grep kube-apiserver | awk '{print $1}') | tail -20 >> /opt/candidate/api-troubleshooting.txt 2>&1

# 7. ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šç¢ºèª
echo "7. Network Connectivity Check:" >> /opt/candidate/api-troubleshooting.txt
netstat -tulpn | grep :6443 >> /opt/candidate/api-troubleshooting.txt

# 8. kubeconfigç¢ºèª
echo "8. Checking kubeconfig:" >> /opt/candidate/api-troubleshooting.txt
kubectl config view >> /opt/candidate/api-troubleshooting.txt

# ä¿®å¾©æ‰‹é †ã®ä¾‹
echo "9. Common Repair Steps:" >> /opt/candidate/api-troubleshooting.txt
echo "- Restart kubelet: sudo systemctl restart kubelet" >> /opt/candidate/api-troubleshooting.txt
echo "- Check API server manifest: /etc/kubernetes/manifests/kube-apiserver.yaml" >> /opt/candidate/api-troubleshooting.txt
echo "- Verify etcd health: kubectl get pods -n kube-system" >> /opt/candidate/api-troubleshooting.txt
echo "- Check node resources: df -h, free -h" >> /opt/candidate/api-troubleshooting.txt

# çµæœè¡¨ç¤º
cat /opt/candidate/api-troubleshooting.txt
```

**æ¡ç‚¹ãƒã‚¤ãƒ³ãƒˆ:**
- ä½“ç³»çš„ãªè¨ºæ–­æ‰‹é † (30%)
- ãƒ­ã‚°ç¢ºèªã®å®Ÿæ–½ (25%)
- è¨¼æ˜æ›¸ã®æ¤œè¨¼ (20%)
- ä¿®å¾©æ‰‹é †ã®è¨˜éŒ² (25%)
</details>

---

## ğŸ¯ Question 40: Multi-Container Pod with Shared Volume (5%)

**Context**: cluster: k8s-cluster-1, namespace: multi-container  
**Task**: 
ä»¥ä¸‹ã®ãƒãƒ«ãƒã‚³ãƒ³ãƒ†ãƒŠPodã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š
1. Web container: nginx:1.20
2. Log processor: busyboxï¼ˆãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç›£è¦–ï¼‰
3. å…±æœ‰ãƒœãƒªãƒ¥ãƒ¼ãƒ : /var/log/nginx
4. initContainer: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’æº–å‚™

<details>
<summary>ğŸ’¡ è§£ç­”ä¾‹</summary>

```bash
# namespaceã‚’ä½œæˆ
kubectl create namespace multi-container

# Multi-container Pod YAMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
cat <<EOF > /opt/candidate/multi-container-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: multi-container-pod
  namespace: multi-container
spec:
  initContainers:
  - name: config-setup
    image: busybox:1.35
    command: ['sh', '-c']
    args:
    - 'echo "server { listen 80; location / { root /usr/share/nginx/html; } }" > /etc/nginx/conf.d/default.conf'
    volumeMounts:
    - name: nginx-config
      mountPath: /etc/nginx/conf.d
  containers:
  - name: web-server
    image: nginx:1.20
    ports:
    - containerPort: 80
    volumeMounts:
    - name: shared-logs
      mountPath: /var/log/nginx
    - name: nginx-config
      mountPath: /etc/nginx/conf.d
  - name: log-processor
    image: busybox:1.35
    command: ['sh', '-c']
    args:
    - 'while true; do
         if [ -f /var/log/nginx/access.log ]; then
           echo "$(date): Processing access log...";
           tail -5 /var/log/nginx/access.log | wc -l;
         else
           echo "$(date): Waiting for access.log...";
         fi;
         sleep 10;
       done'
    volumeMounts:
    - name: shared-logs
      mountPath: /var/log/nginx
  - name: sidecar-monitor
    image: busybox:1.35
    command: ['sh', '-c']
    args:
    - 'while true; do
         echo "$(date): Monitoring system...";
         ps aux;
         sleep 30;
       done'
  volumes:
  - name: shared-logs
    emptyDir: {}
  - name: nginx-config
    emptyDir: {}
EOF

# Podã‚’é©ç”¨
kubectl apply -f /opt/candidate/multi-container-pod.yaml

# ç¢ºèª
kubectl get pod multi-container-pod -n multi-container
kubectl describe pod multi-container-pod -n multi-container

# å„ã‚³ãƒ³ãƒ†ãƒŠã®ãƒ­ã‚°ç¢ºèª
kubectl logs multi-container-pod -c web-server -n multi-container
kubectl logs multi-container-pod -c log-processor -n multi-container
kubectl logs multi-container-pod -c sidecar-monitor -n multi-container

# initContainerã®ãƒ­ã‚°ç¢ºèª
kubectl logs multi-container-pod -c config-setup -n multi-container

# Podå†…ã®ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª
kubectl exec multi-container-pod -c web-server -n multi-container -- ls -la /var/log/nginx/
kubectl exec multi-container-pod -c log-processor -n multi-container -- ls -la /var/log/nginx/
```

**æ¡ç‚¹ãƒã‚¤ãƒ³ãƒˆ:**
- initContainerã®æ­£ã—ã„è¨­å®š (25%)
- ãƒãƒ«ãƒã‚³ãƒ³ãƒ†ãƒŠã®è¨­å®š (25%)
- å…±æœ‰ãƒœãƒªãƒ¥ãƒ¼ãƒ ã®è¨­å®š (25%)
- ãƒ­ã‚°å‡¦ç†ã®å®Ÿè£… (25%)
</details>

---

## ğŸ¯ Questions 41-100: Advanced CKA Topics

**æ®‹ã‚Š60å•ã®æ¦‚è¦:**

### ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ç®¡ç†ãƒ»é‹ç”¨ (Questions 41-50)
- Kubernetes Upgrade Process (8%)
- etcd ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆå¾©å…ƒ (7%)
- ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æˆ¦ç•¥ (6%)
- ãƒãƒ¼ãƒ‰ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹è‡ªå‹•åŒ– (5%)
- ãã®ä»–ã®é«˜åº¦ãªã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ç®¡ç†

### ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚­ãƒ³ã‚° (Questions 51-60)
- CNI ãƒ—ãƒ©ã‚°ã‚¤ãƒ³è¨­å®šãƒ»å¤‰æ›´ (8%)
- Service ãƒ¡ãƒƒã‚·ãƒ¥å®Ÿè£… (7%)
- é«˜åº¦ãªãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒãƒªã‚·ãƒ¼ (6%)
- Load Balancerè¨­å®š (5%)
- ãã®ä»–ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è¨­å®š

### ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ (Questions 61-70)
- Pod Security Standardså®Ÿè£… (8%)
- OPA Gatekeeperè¨­å®š (7%)
- Certificate Management (6%)
- RBACé«˜åº¦è¨­å®š (5%)
- ãã®ä»–ã®ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¨­å®š

### ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ (Questions 71-80)
- CSI ãƒ‰ãƒ©ã‚¤ãƒãƒ¼å®Ÿè£… (8%)
- å‹•çš„ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ãƒ—ãƒ­ãƒ“ã‚¸ãƒ§ãƒ‹ãƒ³ã‚° (7%)
- ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚¯ãƒ©ã‚¹è¨­å®š (6%)
- ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—/å¾©å…ƒ (5%)
- ãã®ä»–ã®ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ç®¡ç†

### ç›£è¦–ãƒ»ãƒ­ã‚° (Questions 81-90)
- Prometheus/Grafanaè¨­å®š (8%)
- é›†ä¸­ãƒ­ã‚°ç®¡ç† (7%)
- ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›† (6%)
- ã‚¢ãƒ©ãƒ¼ãƒˆè¨­å®š (5%)
- ãã®ä»–ã®ç›£è¦–è¨­å®š

### ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚° (Questions 91-100)
- è¤‡é›‘ãªå•é¡Œè¨ºæ–­ (8%)
- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° (7%)
- ãƒªã‚½ãƒ¼ã‚¹æœ€é©åŒ– (6%)
- é‹ç”¨ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ (5%)
- ãã®ä»–ã®é«˜åº¦ãªãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

**åˆè¨ˆ**: 100å•ï¼ˆå®Ÿéš›ã®CKAè©¦é¨“ãƒ¬ãƒ™ãƒ«ã®åŒ…æ‹¬çš„ãªå®Ÿè·µå•é¡Œï¼‰

å„å•é¡Œã¯å®Ÿéš›ã®Kubernetesé‹ç”¨ã§é­é‡ã™ã‚‹å®Ÿè·µçš„ãªã‚·ãƒŠãƒªã‚ªã«åŸºã¥ã„ã¦ãŠã‚Šã€CKAèªå®šè©¦é¨“ã®è¦æ±‚ãƒ¬ãƒ™ãƒ«ã«å¯¾å¿œã—ã¦ã„ã¾ã™ã€‚

---

## ğŸ“Š ç·åˆæ¡ç‚¹åŸºæº–

| åˆ†é‡ | å•é¡Œæ•° | é…ç‚¹æ¯”ç‡ |
|------|--------|----------|
| ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ç®¡ç†ãƒ»é‹ç”¨ | 25å• | 25% |
| ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚­ãƒ³ã‚° | 20å• | 20% |
| ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ | 20å• | 20% |
| ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ | 15å• | 15% |
| ç›£è¦–ãƒ»ãƒ­ã‚° | 10å• | 10% |
| ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚° | 10å• | 10% |

**åˆæ ¼ãƒ©ã‚¤ãƒ³**: 66%ä»¥ä¸Š  
**æ¨å¥¨å­¦ç¿’æ™‚é–“**: 120åˆ†ï¼ˆå®Ÿéš›ã®è©¦é¨“æ™‚é–“ã¨åŒã˜ï¼‰  
**å‰æçŸ¥è­˜**: KubernetesåŸºç¤ã€Linux ã‚·ã‚¹ãƒ†ãƒ ç®¡ç†ã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åŸºç¤

---

## ğŸ¯ Question 31: Node Selector and Affinity (6%)

**Context**: cluster: k8s-cluster-1, namespace: scheduling  
**Task**: 
ä»¥ä¸‹ã®ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°è¦ä»¶ã§Podã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š
1. NodeSelector: `disktype=ssd` ãƒ©ãƒ™ãƒ«ã‚’æŒã¤ãƒãƒ¼ãƒ‰ã§å®Ÿè¡Œ
2. Node Affinity: `zone=us-west-1` ã‚’å„ªå…ˆã€`zone=us-east-1` ã‚’é¿ã‘ã‚‹
3. Pod Anti-Affinity: åŒã˜Podã‚’ç•°ãªã‚‹ãƒãƒ¼ãƒ‰ã§å®Ÿè¡Œ

<details>
<summary>ğŸ’¡ è§£ç­”ä¾‹</summary>

```bash
# namespaceã‚’ä½œæˆ
kubectl create namespace scheduling

# ãƒãƒ¼ãƒ‰ã«ãƒ©ãƒ™ãƒ«ã‚’è¿½åŠ ï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰
NODE1=$(kubectl get nodes -o jsonpath='{.items[0].metadata.name}')
NODE2=$(kubectl get nodes -o jsonpath='{.items[1].metadata.name}' 2>/dev/null || echo $NODE1)

kubectl label node $NODE1 disktype=ssd zone=us-west-1
kubectl label node $NODE2 disktype=hdd zone=us-east-1 --overwrite

# Scheduling Pod YAMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
cat <<EOF > /opt/candidate/scheduling-pod.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: scheduled-app
  namespace: scheduling
spec:
  replicas: 2
  selector:
    matchLabels:
      app: scheduled-app
  template:
    metadata:
      labels:
        app: scheduled-app
    spec:
      nodeSelector:
        disktype: ssd
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            preference:
              matchExpressions:
              - key: zone
                operator: In
                values: ["us-west-1"]
          - weight: 50
            preference:
              matchExpressions:
              - key: zone
                operator: NotIn
                values: ["us-east-1"]
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values: ["scheduled-app"]
              topologyKey: kubernetes.io/hostname
      containers:
      - name: app
        image: nginx:1.20
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
EOF

# Deploymentã‚’é©ç”¨
kubectl apply -f /opt/candidate/scheduling-pod.yaml

# ç¢ºèª
kubectl get pods -n scheduling -o wide
kubectl describe pod -l app=scheduled-app -n scheduling | grep -A 5 "Node-Selectors\|Affinity"
```

**æ¡ç‚¹ãƒã‚¤ãƒ³ãƒˆ:**
- NodeSelectorã®æ­£ã—ã„è¨­å®š (25%)
- Node Affinityã®é©åˆ‡ãªè¨­å®š (35%)
- Pod Anti-Affinityã®è¨­å®š (25%)
- ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°çµæœã®ç¢ºèª (15%)
</details>

---

## ğŸ¯ Question 32: Taint and Toleration (5%)

**Context**: cluster: k8s-cluster-1  
**Task**: 
ä»¥ä¸‹ã®Taintã¨Tolerationã‚’è¨­å®šã—ã¦ãã ã•ã„ï¼š
1. ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒãƒ¼ãƒ‰ã« `environment=production:NoSchedule` ã®Taintã‚’è¿½åŠ 
2. Productionç”¨Podã«å¯¾å¿œã™ã‚‹Tolerationã‚’è¨­å®š
3. è¨­å®šå¾Œã®å‹•ä½œã‚’ç¢ºèª

<details>
<summary>ğŸ’¡ è§£ç­”ä¾‹</summary>

```bash
# ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒãƒ¼ãƒ‰ã‚’ç‰¹å®š
WORKER_NODE=$(kubectl get nodes --no-headers | grep -v master | head -1 | awk '{print $1}')

# ãƒãƒ¼ãƒ‰ã«Taintã‚’è¿½åŠ 
kubectl taint node $WORKER_NODE environment=production:NoSchedule

# Taintç¢ºèª
kubectl describe node $WORKER_NODE | grep -A 5 Taints

# Tolerationç„¡ã—ã®Podã‚’ãƒ†ã‚¹ãƒˆ
cat <<EOF > /opt/candidate/no-toleration-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: no-toleration-pod
spec:
  containers:
  - name: test
    image: busybox:1.35
    command: ['sh', '-c', 'sleep 3600']
EOF

kubectl apply -f /opt/candidate/no-toleration-pod.yaml

# Tolerationæœ‰ã‚Šã®Podã‚’ä½œæˆ
cat <<EOF > /opt/candidate/production-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: production-pod
spec:
  tolerations:
  - key: "environment"
    operator: "Equal"
    value: "production"
    effect: "NoSchedule"
  containers:
  - name: app
    image: nginx:1.20
    resources:
      requests:
        cpu: 100m
        memory: 128Mi
EOF

kubectl apply -f /opt/candidate/production-pod.yaml

# çµæœç¢ºèª
echo "=== Taint and Toleration Test Results ===" > /opt/candidate/taint-test.txt
echo "$(date)" >> /opt/candidate/taint-test.txt
echo "" >> /opt/candidate/taint-test.txt

echo "=== Node Taint ===" >> /opt/candidate/taint-test.txt
kubectl describe node $WORKER_NODE | grep -A 3 Taints >> /opt/candidate/taint-test.txt
echo "" >> /opt/candidate/taint-test.txt

echo "=== Pod Scheduling Results ===" >> /opt/candidate/taint-test.txt
kubectl get pods -o wide >> /opt/candidate/taint-test.txt

# Taintå‰Šé™¤ï¼ˆã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼‰
kubectl taint node $WORKER_NODE environment=production:NoSchedule-
```

**æ¡ç‚¹ãƒã‚¤ãƒ³ãƒˆ:**
- Taintã®æ­£ã—ã„è¨­å®š (30%)
- Tolerationã®é©åˆ‡ãªè¨­å®š (40%)
- ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°å‹•ä½œã®ç¢ºèª (30%)
</details>

---

## ğŸ¯ Question 33: Kubernetes Logging Architecture (6%)

**Context**: cluster: k8s-cluster-1, namespace: logging-test  
**Task**: 
ä»¥ä¸‹ã®ãƒ­ã‚°ç®¡ç†è¨­å®šã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ï¼š
1. ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³Podã®ãƒ­ã‚°ã‚’èª¿æŸ»
2. ãƒ­ã‚°ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã®è¨­å®šç¢ºèª
3. ãƒ­ã‚°é›†ç´„ã®ãŸã‚ã®ã‚µã‚¤ãƒ‰ã‚«ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³å®Ÿè£…

<details>
<summary>ğŸ’¡ è§£ç­”ä¾‹</summary>

```bash
# namespaceã‚’ä½œæˆ
kubectl create namespace logging-test

# ãƒ­ã‚°ç”Ÿæˆã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ä½œæˆ
cat <<EOF > /opt/candidate/log-generator.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: log-generator
  namespace: logging-test
spec:
  replicas: 1
  selector:
    matchLabels:
      app: log-generator
  template:
    metadata:
      labels:
        app: log-generator
    spec:
      containers:
      - name: app
        image: busybox:1.35
        command: ['sh', '-c']
        args:
        - 'while true; do
             echo "$(date): INFO - Application is running normally" | tee -a /var/log/app.log;
             echo "$(date): ERROR - Simulated error occurred" | tee -a /var/log/error.log;
             sleep 10;
           done'
        volumeMounts:
        - name: log-volume
          mountPath: /var/log
      - name: log-sidecar
        image: busybox:1.35
        command: ['sh', '-c']
        args:
        - 'tail -f /var/log/app.log /var/log/error.log'
        volumeMounts:
        - name: log-volume
          mountPath: /var/log
      volumes:
      - name: log-volume
        emptyDir: {}
EOF

kubectl apply -f /opt/candidate/log-generator.yaml

# ãƒ­ã‚°èª¿æŸ»
sleep 30  # ãƒ­ã‚°ç”Ÿæˆã‚’å¾…ã¤

echo "=== Kubernetes Logging Analysis ===" > /opt/candidate/logging-analysis.txt
echo "$(date)" >> /opt/candidate/logging-analysis.txt
echo "" >> /opt/candidate/logging-analysis.txt

# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ­ã‚°ç¢ºèª
echo "=== Application Logs ===" >> /opt/candidate/logging-analysis.txt
kubectl logs -l app=log-generator -c app -n logging-test --tail=10 >> /opt/candidate/logging-analysis.txt
echo "" >> /opt/candidate/logging-analysis.txt

# ã‚µã‚¤ãƒ‰ã‚«ãƒ¼ãƒ­ã‚°ç¢ºèª
echo "=== Sidecar Logs ===" >> /opt/candidate/logging-analysis.txt
kubectl logs -l app=log-generator -c log-sidecar -n logging-test --tail=10 >> /opt/candidate/logging-analysis.txt
echo "" >> /opt/candidate/logging-analysis.txt

# ãƒãƒ¼ãƒ‰ãƒ¬ãƒ™ãƒ«ãƒ­ã‚°è¨­å®šç¢ºèª
echo "=== Node Log Configuration ===" >> /opt/candidate/logging-analysis.txt
kubectl get nodes -o wide >> /opt/candidate/logging-analysis.txt

# çµæœç¢ºèª
cat /opt/candidate/logging-analysis.txt
```

**æ¡ç‚¹ãƒã‚¤ãƒ³ãƒˆ:**
- ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ­ã‚°ã®ç¢ºèª (25%)
- ã‚µã‚¤ãƒ‰ã‚«ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å®Ÿè£… (40%)
- ãƒ­ã‚°è¨­å®šã®èª¿æŸ» (25%)
- ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ (10%)
</details>

---

## ğŸ¯ Question 34: Cluster Upgrade Preparation (8%)

**Context**: cluster: k8s-cluster-1  
**Task**: 
ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰ã®æº–å‚™ã‚’è¡Œã£ã¦ãã ã•ã„ï¼š
1. ç¾åœ¨ã®Kubernetesãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèª
2. ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰å¯èƒ½ãªãƒãƒ¼ã‚¸ãƒ§ãƒ³èª¿æŸ»
3. ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰å‰ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å–å¾—
4. ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰æ‰‹é †æ›¸ã‚’`/opt/candidate/upgrade-plan.txt`ã«ä½œæˆ

<details>
<summary>ğŸ’¡ è§£ç­”ä¾‹</summary>

```bash
# ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰è¨ˆç”»æ›¸ä½œæˆ
echo "=== Kubernetes Cluster Upgrade Plan ===" > /opt/candidate/upgrade-plan.txt
echo "$(date)" >> /opt/candidate/upgrade-plan.txt
echo "" >> /opt/candidate/upgrade-plan.txt

# ç¾åœ¨ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèª
echo "=== Current Cluster Version ===" >> /opt/candidate/upgrade-plan.txt
kubectl version --short >> /opt/candidate/upgrade-plan.txt
echo "" >> /opt/candidate/upgrade-plan.txt

# ãƒãƒ¼ãƒ‰ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèª
echo "=== Node Versions ===" >> /opt/candidate/upgrade-plan.txt
kubectl get nodes -o wide >> /opt/candidate/upgrade-plan.txt
echo "" >> /opt/candidate/upgrade-plan.txt

# kubeadmã§ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰å¯èƒ½ãªãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèª
echo "=== Available Upgrades ===" >> /opt/candidate/upgrade-plan.txt
kubeadm upgrade plan >> /opt/candidate/upgrade-plan.txt 2>&1 || echo "kubeadm upgrade plan failed" >> /opt/candidate/upgrade-plan.txt
echo "" >> /opt/candidate/upgrade-plan.txt

# etcdãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ
echo "=== ETCD Backup ===" >> /opt/candidate/upgrade-plan.txt
ETCD_POD=$(kubectl get pods -n kube-system -l component=etcd -o jsonpath='{.items[0].metadata.name}')
BACKUP_PATH="/opt/candidate/etcd-backup-$(date +%Y%m%d_%H%M%S).db"

kubectl exec -n kube-system $ETCD_POD -- etcdctl \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  snapshot save $BACKUP_PATH

echo "ETCD backup created at: $BACKUP_PATH" >> /opt/candidate/upgrade-plan.txt
echo "" >> /opt/candidate/upgrade-plan.txt

# ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰æ‰‹é †æ›¸ä½œæˆ
cat << UPGRADE_STEPS >> /opt/candidate/upgrade-plan.txt
=== Upgrade Procedure ===

1. Pre-upgrade Checklist:
   - âœ“ Backup etcd cluster
   - âœ“ Backup /etc/kubernetes/
   - âœ“ Verify cluster health
   - âœ“ Drain worker nodes

2. Control Plane Upgrade:
   - Update kubeadm on master node
   - Run: kubeadm upgrade plan
   - Run: kubeadm upgrade apply v1.x.x
   - Update kubelet and kubectl
   - Restart kubelet

3. Worker Node Upgrade:
   - Drain node: kubectl drain <node-name> --ignore-daemonsets
   - Update kubeadm, kubelet, kubectl
   - Run: kubeadm upgrade node
   - Restart kubelet
   - Uncordon node: kubectl uncordon <node-name>

4. Post-upgrade Verification:
   - Verify all nodes are Ready
   - Verify all pods are running
   - Run cluster validation tests

5. Rollback Plan:
   - Restore etcd from backup if needed
   - Downgrade kubeadm, kubelet, kubectl
   - Restore configuration files

UPGRADE_STEPS

# çµæœç¢ºèª
cat /opt/candidate/upgrade-plan.txt
```

**æ¡ç‚¹ãƒã‚¤ãƒ³ãƒˆ:**
- ç¾åœ¨ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®æ­£ç¢ºãªç¢ºèª (20%)
- ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰å¯èƒ½ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®èª¿æŸ» (20%)
- etcdãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®å®Ÿè¡Œ (30%)
- è©³ç´°ãªæ‰‹é †æ›¸ä½œæˆ (30%)
</details>

---

## ğŸ¯ Question 35: Application Performance Monitoring (5%)

**Context**: cluster: k8s-cluster-1, namespace: monitoring  
**Task**: 
ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚’è¨­å®šã—ã¦ãã ã•ã„ï¼š
1. CPU/ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®é«˜ã„Podã‚’ç‰¹å®š
2. ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡ã®å±¥æ­´ã‚’è¨˜éŒ²
3. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆ

<details>
<summary>ğŸ’¡ è§£ç­”ä¾‹</summary>

```bash
# namespaceã‚’ä½œæˆ
kubectl create namespace monitoring

# è² è·ç”Ÿæˆç”¨Deploymentã‚’ä½œæˆ
cat <<EOF > /opt/candidate/load-generator.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: load-generator
  namespace: monitoring
spec:
  replicas: 3
  selector:
    matchLabels:
      app: load-generator
  template:
    metadata:
      labels:
        app: load-generator
    spec:
      containers:
      - name: cpu-stress
        image: busybox:1.35
        command: ['sh', '-c']
        args:
        - 'while true; do
             for i in 1 2 3 4 5; do
               echo "CPU stress test iteration $i";
               dd if=/dev/zero of=/dev/null count=100000 bs=1024 &
             done;
             sleep 30;
             killall dd 2>/dev/null;
             sleep 30;
           done'
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 500m
            memory: 256Mi
EOF

kubectl apply -f /opt/candidate/load-generator.yaml
sleep 60  # è² è·ç”Ÿæˆã‚’å¾…ã¤

# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ
echo "=== Application Performance Monitoring Report ===" > /opt/candidate/performance-report.txt
echo "$(date)" >> /opt/candidate/performance-report.txt
echo "" >> /opt/candidate/performance-report.txt

# å…¨Podã®ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡
echo "=== Current Resource Usage (All Pods) ===" >> /opt/candidate/performance-report.txt
kubectl top pods --all-namespaces --sort-by=cpu >> /opt/candidate/performance-report.txt
echo "" >> /opt/candidate/performance-report.txt

# ç‰¹å®šnamespaceå†…ã®ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡
echo "=== Monitoring Namespace Resource Usage ===" >> /opt/candidate/performance-report.txt
kubectl top pods -n monitoring --sort-by=cpu >> /opt/candidate/performance-report.txt
echo "" >> /opt/candidate/performance-report.txt

# ãƒãƒ¼ãƒ‰ãƒ¬ãƒ™ãƒ«ã®ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡
echo "=== Node Resource Usage ===" >> /opt/candidate/performance-report.txt
kubectl top nodes >> /opt/candidate/performance-report.txt
echo "" >> /opt/candidate/performance-report.txt

# é«˜ä½¿ç”¨ç‡Podã®è©³ç´°åˆ†æ
echo "=== High CPU Usage Pod Analysis ===" >> /opt/candidate/performance-report.txt
HIGH_CPU_POD=$(kubectl top pods -n monitoring --no-headers --sort-by=cpu | head -1 | awk '{print $1}')
kubectl describe pod $HIGH_CPU_POD -n monitoring | grep -A 20 "Containers:" >> /opt/candidate/performance-report.txt
echo "" >> /opt/candidate/performance-report.txt

# ãƒªã‚½ãƒ¼ã‚¹åˆ¶é™ã¨å®Ÿéš›ã®ä½¿ç”¨é‡æ¯”è¼ƒ
echo "=== Resource Limits vs Usage ===" >> /opt/candidate/performance-report.txt
kubectl get pods -n monitoring -o custom-columns=NAME:.metadata.name,CPU_REQ:.spec.containers[*].resources.requests.cpu,MEM_REQ:.spec.containers[*].resources.requests.memory,CPU_LIM:.spec.containers[*].resources.limits.cpu,MEM_LIM:.spec.containers[*].resources.limits.memory >> /opt/candidate/performance-report.txt

# çµæœç¢ºèª
cat /opt/candidate/performance-report.txt
```

**æ¡ç‚¹ãƒã‚¤ãƒ³ãƒˆ:**
- ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡ã®æ­£ç¢ºãªå–å¾— (30%)
- é«˜ä½¿ç”¨ç‡Podã®ç‰¹å®š (25%)
- è©³ç´°åˆ†æã®å®Ÿæ–½ (25%)
- ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ (20%)
</details>

---

## ğŸ¯ Question 36: Service Mesh - Istio Basics (7%)

**Context**: cluster: k8s-cluster-1, namespace: istio-test  
**Task**: 
Service Meshã®åŸºæœ¬çš„ãªè¨­å®šã‚’è¡Œã£ã¦ãã ã•ã„ï¼š
1. Istio sidecar injectionã®æœ‰åŠ¹åŒ–
2. Virtual Serviceã®ä½œæˆ
3. Destination Ruleã®è¨­å®š
4. ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯åˆ†å‰²ã®å®Ÿè£…

<details>
<summary>ğŸ’¡ è§£ç­”ä¾‹</summary>

```bash
# namespaceã‚’ä½œæˆã—ã¦Istio injectionæœ‰åŠ¹åŒ–
kubectl create namespace istio-test
kubectl label namespace istio-test istio-injection=enabled

# ãƒã‚¤ã‚¯ãƒ­ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ãƒ‡ãƒ—ãƒ­ã‚¤
cat <<EOF > /opt/candidate/microservice-app.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
  namespace: istio-test
spec:
  replicas: 1
  selector:
    matchLabels:
      app: frontend
      version: v1
  template:
    metadata:
      labels:
        app: frontend
        version: v1
    spec:
      containers:
      - name: frontend
        image: nginx:1.20
        ports:
        - containerPort: 80
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: backend-v1
  namespace: istio-test
spec:
  replicas: 1
  selector:
    matchLabels:
      app: backend
      version: v1
  template:
    metadata:
      labels:
        app: backend
        version: v1
    spec:
      containers:
      - name: backend
        image: nginx:1.20
        ports:
        - containerPort: 80
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: backend-v2
  namespace: istio-test
spec:
  replicas: 1
  selector:
    matchLabels:
      app: backend
      version: v2
  template:
    metadata:
      labels:
        app: backend
        version: v2
    spec:
      containers:
      - name: backend
        image: nginx:1.21
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: frontend-service
  namespace: istio-test
spec:
  selector:
    app: frontend
  ports:
  - port: 80
    targetPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: backend-service
  namespace: istio-test
spec:
  selector:
    app: backend
  ports:
  - port: 80
    targetPort: 80
EOF

kubectl apply -f /opt/candidate/microservice-app.yaml

# Istio Virtual Serviceã‚’ä½œæˆ
cat <<EOF > /opt/candidate/virtual-service.yaml
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: backend-vs
  namespace: istio-test
spec:
  hosts:
  - backend-service
  http:
  - match:
    - headers:
        user-type:
          exact: premium
    route:
    - destination:
        host: backend-service
        subset: v2
  - route:
    - destination:
        host: backend-service
        subset: v1
      weight: 90
    - destination:
        host: backend-service
        subset: v2
      weight: 10
EOF

# Destination Ruleã‚’ä½œæˆ
cat <<EOF > /opt/candidate/destination-rule.yaml
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: backend-dr
  namespace: istio-test
spec:
  host: backend-service
  subsets:
  - name: v1
    labels:
      version: v1
  - name: v2
    labels:
      version: v2
EOF

# Istioè¨­å®šã‚’é©ç”¨ï¼ˆIstioãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿ã®å ´åˆï¼‰
kubectl apply -f /opt/candidate/virtual-service.yaml 2>/dev/null || echo "Istio not installed - creating config files only"
kubectl apply -f /opt/candidate/destination-rule.yaml 2>/dev/null || echo "Istio not installed - creating config files only"

# è¨­å®šç¢ºèª
echo "=== Service Mesh Configuration ===" > /opt/candidate/istio-config.txt
echo "$(date)" >> /opt/candidate/istio-config.txt
echo "" >> /opt/candidate/istio-config.txt

echo "=== Namespace Labels ===" >> /opt/candidate/istio-config.txt
kubectl get namespace istio-test --show-labels >> /opt/candidate/istio-config.txt
echo "" >> /opt/candidate/istio-config.txt

echo "=== Pods with Sidecars ===" >> /opt/candidate/istio-config.txt
kubectl get pods -n istio-test -o custom-columns=NAME:.metadata.name,READY:.status.containerStatuses[*].ready,CONTAINERS:.spec.containers[*].name >> /opt/candidate/istio-config.txt
echo "" >> /opt/candidate/istio-config.txt

echo "=== Services ===" >> /opt/candidate/istio-config.txt
kubectl get services -n istio-test >> /opt/candidate/istio-config.txt

# çµæœç¢ºèª
cat /opt/candidate/istio-config.txt
```

**æ¡ç‚¹ãƒã‚¤ãƒ³ãƒˆ:**
- Istio injectionè¨­å®š (25%)
- Virtual Serviceä½œæˆ (30%)
- Destination Ruleè¨­å®š (25%)
- ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯åˆ†å‰²å®Ÿè£… (20%)
</details>

---

## ğŸ¯ Question 37: Kubernetes API and Custom Controllers (6%)

**Context**: cluster: k8s-cluster-1  
**Task**: 
Kubernetes APIã®æ“ä½œã¨ã‚«ã‚¹ã‚¿ãƒ ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ©ãƒ¼ã®åŸºæœ¬ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ï¼š
1. kubectl proxyã‚’ä½¿ç”¨ã—ãŸAPIç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹
2. ã‚«ã‚¹ã‚¿ãƒ ãƒªã‚½ãƒ¼ã‚¹ã®CRUDæ“ä½œ
3. Webhookè¨­å®šã®æº–å‚™

<details>
<summary>ğŸ’¡ è§£ç­”ä¾‹</summary>

```bash
# kubectl proxyã‚’èµ·å‹•ï¼ˆãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ï¼‰
kubectl proxy --port=8080 &
PROXY_PID=$!
sleep 5

# APIæ“ä½œãƒ¬ãƒãƒ¼ãƒˆä½œæˆ
echo "=== Kubernetes API Operations ===" > /opt/candidate/api-operations.txt
echo "$(date)" >> /opt/candidate/api-operations.txt
echo "" >> /opt/candidate/api-operations.txt

# 1. API ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèª
echo "=== API Versions ===" >> /opt/candidate/api-operations.txt
curl -s http://localhost:8080/api/ | jq . >> /opt/candidate/api-operations.txt 2>/dev/null || curl -s http://localhost:8080/api/ >> /opt/candidate/api-operations.txt
echo "" >> /opt/candidate/api-operations.txt

# 2. Namespaceä¸€è¦§ã‚’APIçµŒç”±ã§å–å¾—
echo "=== Namespaces via API ===" >> /opt/candidate/api-operations.txt
curl -s http://localhost:8080/api/v1/namespaces | jq '.items[] | .metadata.name' >> /opt/candidate/api-operations.txt 2>/dev/null || curl -s http://localhost:8080/api/v1/namespaces >> /opt/candidate/api-operations.txt
echo "" >> /opt/candidate/api-operations.txt

# 3. ã‚«ã‚¹ã‚¿ãƒ ãƒªã‚½ãƒ¼ã‚¹å®šç¾©ã®ç¢ºèª
echo "=== Custom Resource Definitions ===" >> /opt/candidate/api-operations.txt
curl -s http://localhost:8080/apis/apiextensions.k8s.io/v1/customresourcedefinitions | jq '.items[] | .metadata.name' >> /opt/candidate/api-operations.txt 2>/dev/null || echo "No CRDs found or jq not available" >> /opt/candidate/api-operations.txt
echo "" >> /opt/candidate/api-operations.txt

# 4. Webhookã®è¨­å®šä¾‹ã‚’ä½œæˆ
cat <<EOF > /opt/candidate/validating-webhook.yaml
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingAdmissionWebhook
metadata:
  name: pod-validator
webhooks:
- name: pod-validator.example.com
  clientConfig:
    service:
      name: pod-validator-service
      namespace: webhook
      path: "/validate"
  rules:
  - operations: ["CREATE", "UPDATE"]
    apiGroups: [""]
    apiVersions: ["v1"]
    resources: ["pods"]
  admissionReviewVersions: ["v1"]
  sideEffects: None
  failurePolicy: Fail
EOF

echo "=== Webhook Configuration Created ===" >> /opt/candidate/api-operations.txt
echo "Validating webhook configuration saved to /opt/candidate/validating-webhook.yaml" >> /opt/candidate/api-operations.txt
echo "" >> /opt/candidate/api-operations.txt

# 5. ã‚«ã‚¹ã‚¿ãƒ ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ©ãƒ¼ã®åŸºæœ¬æ§‹é€ ä¾‹
cat <<EOF > /opt/candidate/controller-example.py
#!/usr/bin/env python3
"""
Example Custom Controller for Kubernetes
This is a basic template for a custom controller
"""

import time
import kubernetes
from kubernetes import client, config, watch

def main():
    # Load kubeconfig
    config.load_incluster_config()  # For in-cluster
    # config.load_kube_config()  # For local development
    
    v1 = client.CoreV1Api()
    
    # Watch for Pod events
    w = watch.Watch()
    for event in w.stream(v1.list_pod_for_all_namespaces):
        event_type = event['type']
        pod = event['object']
        
        print(f"Event: {event_type} Pod: {pod.metadata.name} Namespace: {pod.metadata.namespace}")
        
        # Custom logic here
        if event_type == "ADDED":
            handle_pod_added(pod)
        elif event_type == "DELETED":
            handle_pod_deleted(pod)

def handle_pod_added(pod):
    # Custom logic for pod creation
    print(f"Handling new pod: {pod.metadata.name}")

def handle_pod_deleted(pod):
    # Custom logic for pod deletion
    print(f"Handling deleted pod: {pod.metadata.name}")

if __name__ == "__main__":
    main()
EOF

echo "=== Custom Controller Example ===" >> /opt/candidate/api-operations.txt
echo "Controller template saved to /opt/candidate/controller-example.py" >> /opt/candidate/api-operations.txt

# proxyåœæ­¢
kill $PROXY_PID 2>/dev/null

# çµæœç¢ºèª
cat /opt/candidate/api-operations.txt
```

**æ¡ç‚¹ãƒã‚¤ãƒ³ãƒˆ:**
- kubectl proxyã®ä½¿ç”¨ (20%)
- APIç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹ (30%)
- Webhookè¨­å®šæº–å‚™ (25%)
- ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ©ãƒ¼ä¾‹ã®ä½œæˆ (25%)
</details>

---

## ğŸ¯ Question 38: Cluster Autoscaling (5%)

**Context**: cluster: k8s-cluster-1, namespace: autoscaling  
**Task**: 
ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚ªãƒ¼ãƒˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã®è¨­å®šã¨å‹•ä½œç¢ºèªã‚’è¡Œã£ã¦ãã ã•ã„ï¼š
1. Cluster Autoscalerã®è¨­å®šç¢ºèª
2. ãƒãƒ¼ãƒ‰ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã®ãƒˆãƒªã‚¬ãƒ¼ä½œæˆ
3. ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å‹•ä½œã®ç›£è¦–

<details>
<summary>ğŸ’¡ è§£ç­”ä¾‹</summary>

```bash
# namespaceã‚’ä½œæˆ
kubectl create namespace autoscaling

# Cluster Autoscalerè¨­å®šç¢ºèª
echo "=== Cluster Autoscaling Configuration ===" > /opt/candidate/autoscaling-config.txt
echo "$(date)" >> /opt/candidate/autoscaling-config.txt
echo "" >> /opt/candidate/autoscaling-config.txt

# ãƒãƒ¼ãƒ‰æƒ…å ±ç¢ºèª
echo "=== Current Node Information ===" >> /opt/candidate/autoscaling-config.txt
kubectl get nodes -o wide >> /opt/candidate/autoscaling-config.txt
echo "" >> /opt/candidate/autoscaling-config.txt

# Cluster Autoscaler Podã®ç¢ºèª
echo "=== Cluster Autoscaler Pod ===" >> /opt/candidate/autoscaling-config.txt
kubectl get pods -n kube-system | grep cluster-autoscaler >> /opt/candidate/autoscaling-config.txt || echo "Cluster Autoscaler not found" >> /opt/candidate/autoscaling-config.txt
echo "" >> /opt/candidate/autoscaling-config.txt

# ãƒªã‚½ãƒ¼ã‚¹æ¶ˆè²»ã®å¤§ãã„Deploymentã‚’ä½œæˆï¼ˆã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆãƒˆãƒªã‚¬ãƒ¼ç”¨ï¼‰
cat <<EOF > /opt/candidate/resource-intensive-app.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: resource-intensive-app
  namespace: autoscaling
spec:
  replicas: 1
  selector:
    matchLabels:
      app: resource-intensive-app
  template:
    metadata:
      labels:
        app: resource-intensive-app
    spec:
      containers:
      - name: app
        image: nginx:1.20
        resources:
          requests:
            cpu: 1000m    # 1 CPU core request
            memory: 2Gi   # 2GB memory request
          limits:
            cpu: 2000m
            memory: 4Gi
EOF

kubectl apply -f /opt/candidate/resource-intensive-app.yaml

# HPAã‚‚ä½œæˆã—ã¦Podãƒ¬ãƒ™ãƒ«ã§ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚‚è¨­å®š
cat <<EOF > /opt/candidate/hpa-config.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: resource-intensive-hpa
  namespace: autoscaling
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: resource-intensive-app
  minReplicas: 1
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 50
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 70
EOF

kubectl apply -f /opt/candidate/hpa-config.yaml

# ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡ã¨åˆ¶ç´„ã®ç›£è¦–
echo "=== Resource Usage Monitoring ===" >> /opt/candidate/autoscaling-config.txt
kubectl top nodes >> /opt/candidate/autoscaling-config.txt 2>/dev/null || echo "Metrics server not available" >> /opt/candidate/autoscaling-config.txt
echo "" >> /opt/candidate/autoscaling-config.txt

# HPAã®çŠ¶æ…‹ç¢ºèª
echo "=== HPA Status ===" >> /opt/candidate/autoscaling-config.txt
kubectl get hpa -n autoscaling >> /opt/candidate/autoscaling-config.txt
echo "" >> /opt/candidate/autoscaling-config.txt

# PodçŠ¶æ…‹ç¢ºèª
echo "=== Pod Status ===" >> /opt/candidate/autoscaling-config.txt
kubectl get pods -n autoscaling -o wide >> /opt/candidate/autoscaling-config.txt
echo "" >> /opt/candidate/autoscaling-config.txt

# ã‚¤ãƒ™ãƒ³ãƒˆç¢ºèª
echo "=== Recent Events ===" >> /opt/candidate/autoscaling-config.txt
kubectl get events -n autoscaling --sort-by=.metadata.creationTimestamp | tail -10 >> /opt/candidate/autoscaling-config.txt

# ãƒ¬ãƒ—ãƒªã‚«æ•°ã‚’å¢—ã‚„ã—ã¦ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚’ãƒˆãƒªã‚¬ãƒ¼
kubectl scale deployment resource-intensive-app --replicas=5 -n autoscaling

echo "" >> /opt/candidate/autoscaling-config.txt
echo "=== Scaling Triggered ===" >> /opt/candidate/autoscaling-config.txt
echo "Deployment scaled to 5 replicas to trigger autoscaling" >> /opt/candidate/autoscaling-config.txt

# çµæœç¢ºèª
cat /opt/candidate/autoscaling-config.txt
```

**æ¡ç‚¹ãƒã‚¤ãƒ³ãƒˆ:**
- Cluster Autoscalerè¨­å®šç¢ºèª (25%)
- ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãƒˆãƒªã‚¬ãƒ¼ä½œæˆ (35%)
- HPAè¨­å®š (25%)
- ç›£è¦–ã¨ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ (15%)
</details>

---

## ğŸ¯ Question 39: Network Troubleshooting - DNS Issues (7%)

**Context**: cluster: k8s-cluster-1, namespace: dns-debug  
**Task**: 
DNSé–¢é€£ã®å•é¡Œã‚’ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã—ã¦ãã ã•ã„ï¼š
1. ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼DNSã‚µãƒ¼ãƒ“ã‚¹ã®çŠ¶æ…‹ç¢ºèª
2. Podé–“ã®åå‰è§£æ±ºãƒ†ã‚¹ãƒˆ
3. DNSè¨­å®šã®æ¤œè¨¼ã¨ä¿®æ­£

<details>
<summary>ğŸ’¡ è§£ç­”ä¾‹</summary>

```bash
# namespaceã‚’ä½œæˆ
kubectl create namespace dns-debug

# DNS ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ
echo "=== DNS Troubleshooting Report ===" > /opt/candidate/dns-troubleshooting.txt
echo "$(date)" >> /opt/candidate/dns-troubleshooting.txt
echo "" >> /opt/candidate/dns-troubleshooting.txt

# 1. ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼DNSã‚µãƒ¼ãƒ“ã‚¹ç¢ºèª
echo "=== Cluster DNS Service Status ===" >> /opt/candidate/dns-troubleshooting.txt
kubectl get svc -n kube-system | grep dns >> /opt/candidate/dns-troubleshooting.txt
echo "" >> /opt/candidate/dns-troubleshooting.txt

# CoreDNS PodçŠ¶æ…‹ç¢ºèª
echo "=== CoreDNS Pod Status ===" >> /opt/candidate/dns-troubleshooting.txt
kubectl get pods -n kube-system -l k8s-app=kube-dns >> /opt/candidate/dns-troubleshooting.txt
echo "" >> /opt/candidate/dns-troubleshooting.txt

# CoreDNSè¨­å®šç¢ºèª
echo "=== CoreDNS Configuration ===" >> /opt/candidate/dns-troubleshooting.txt
kubectl get configmap coredns -n kube-system -o yaml | grep -A 20 Corefile >> /opt/candidate/dns-troubleshooting.txt
echo "" >> /opt/candidate/dns-troubleshooting.txt

# ãƒ†ã‚¹ãƒˆç”¨Podã‚’ä½œæˆ
cat <<EOF > /opt/candidate/dns-test-pods.yaml
apiVersion: v1
kind: Pod
metadata:
  name: dns-test-client
  namespace: dns-debug
spec:
  containers:
  - name: client
    image: busybox:1.35
    command: ['sh', '-c', 'sleep 3600']
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: test-service
  namespace: dns-debug
spec:
  replicas: 1
  selector:
    matchLabels:
      app: test-service
  template:
    metadata:
      labels:
        app: test-service
    spec:
      containers:
      - name: server
        image: nginx:1.20
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: test-service
  namespace: dns-debug
spec:
  selector:
    app: test-service
  ports:
  - port: 80
    targetPort: 80
EOF

kubectl apply -f /opt/candidate/dns-test-pods.yaml
kubectl wait --for=condition=Ready pod/dns-test-client -n dns-debug --timeout=300s

# 2. DNSè§£æ±ºãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
echo "=== DNS Resolution Tests ===" >> /opt/candidate/dns-troubleshooting.txt

# ã‚µãƒ¼ãƒ“ã‚¹åè§£æ±ºãƒ†ã‚¹ãƒˆ
echo "--- Service Name Resolution ---" >> /opt/candidate/dns-troubleshooting.txt
kubectl exec dns-test-client -n dns-debug -- nslookup test-service >> /opt/candidate/dns-troubleshooting.txt 2>&1
echo "" >> /opt/candidate/dns-troubleshooting.txt

# FQDNè§£æ±ºãƒ†ã‚¹ãƒˆ
echo "--- FQDN Resolution ---" >> /opt/candidate/dns-troubleshooting.txt
kubectl exec dns-test-client -n dns-debug -- nslookup test-service.dns-debug.svc.cluster.local >> /opt/candidate/dns-troubleshooting.txt 2>&1
echo "" >> /opt/candidate/dns-troubleshooting.txt

# å¤–éƒ¨DNSè§£æ±ºãƒ†ã‚¹ãƒˆ
echo "--- External DNS Resolution ---" >> /opt/candidate/dns-troubleshooting.txt
kubectl exec dns-test-client -n dns-debug -- nslookup google.com >> /opt/candidate/dns-troubleshooting.txt 2>&1
echo "" >> /opt/candidate/dns-troubleshooting.txt

# Podå†…ã®resolv.confç¢ºèª
echo "=== Pod DNS Configuration ===" >> /opt/candidate/dns-troubleshooting.txt
kubectl exec dns-test-client -n dns-debug -- cat /etc/resolv.conf >> /opt/candidate/dns-troubleshooting.txt
echo "" >> /opt/candidate/dns-troubleshooting.txt

# DNSãƒãƒªã‚·ãƒ¼ç¢ºèª
echo "=== DNS Policy Configuration ===" >> /opt/candidate/dns-troubleshooting.txt
kubectl get pod dns-test-client -n dns-debug -o yaml | grep -A 5 dnsPolicy >> /opt/candidate/dns-troubleshooting.txt
echo "" >> /opt/candidate/dns-troubleshooting.txt

# CoreDNSãƒ­ã‚°ç¢ºèª
echo "=== CoreDNS Logs ===" >> /opt/candidate/dns-troubleshooting.txt
COREDNS_POD=$(kubectl get pods -n kube-system -l k8s-app=kube-dns -o jsonpath='{.items[0].metadata.name}')
kubectl logs $COREDNS_POD -n kube-system --tail=10 >> /opt/candidate/dns-troubleshooting.txt
echo "" >> /opt/candidate/dns-troubleshooting.txt

# ä¿®æ­£ææ¡ˆ
echo "=== Troubleshooting Recommendations ===" >> /opt/candidate/dns-troubleshooting.txt
cat << RECOMMENDATIONS >> /opt/candidate/dns-troubleshooting.txt
1. If DNS resolution fails:
   - Check CoreDNS pod status and logs
   - Verify kube-dns service is running
   - Confirm DNS policy in pod specification

2. If external DNS fails:
   - Check CoreDNS forward configuration
   - Verify upstream DNS servers
   - Check network connectivity from nodes

3. If service discovery fails:
   - Verify service endpoints exist
   - Check service selector labels
   - Confirm namespace isolation settings

4. Common fixes:
   - Restart CoreDNS pods: kubectl rollout restart deployment/coredns -n kube-system
   - Check cluster DNS IP: kubectl get svc kube-dns -n kube-system
   - Verify kubelet DNS settings on nodes
RECOMMENDATIONS

# çµæœç¢ºèª
cat /opt/candidate/dns-troubleshooting.txt
```

**æ¡ç‚¹ãƒã‚¤ãƒ³ãƒˆ:**
- DNS ã‚µãƒ¼ãƒ“ã‚¹çŠ¶æ…‹ç¢ºèª (25%)
- DNSè§£æ±ºãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ (35%)
- è¨­å®šã®æ¤œè¨¼ (25%)
- ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ææ¡ˆ (15%)
</details>

---

## ğŸ¯ Question 40: Backup and Disaster Recovery (6%)

**Context**: cluster: k8s-cluster-1  
**Task**: 
ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã¨ç½å®³å¾©æ—§ã®å®Ÿè£…ã‚’è¡Œã£ã¦ãã ã•ã„ï¼š
1. etcdã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®å®Œå…¨ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
2. æ°¸ç¶šãƒœãƒªãƒ¥ãƒ¼ãƒ ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æˆ¦ç•¥
3. å¾©æ—§æ‰‹é †æ›¸ã®ä½œæˆ
4. ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®è‡ªå‹•åŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆä½œæˆ

<details>
<summary>ğŸ’¡ è§£ç­”ä¾‹</summary>

```bash
# ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
mkdir -p /opt/candidate/backups
BACKUP_DIR="/opt/candidate/backups"
TIMESTAMP=$(date +%Y%m%d_%H%M%S)

# ç½å®³å¾©æ—§è¨ˆç”»æ›¸ä½œæˆ
echo "=== Kubernetes Backup and Disaster Recovery Plan ===" > /opt/candidate/disaster-recovery-plan.txt
echo "$(date)" >> /opt/candidate/disaster-recovery-plan.txt
echo "" >> /opt/candidate/disaster-recovery-plan.txt

# 1. etcdãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®å®Ÿè¡Œ
echo "=== ETCD Backup Procedure ===" >> /opt/candidate/disaster-recovery-plan.txt
ETCD_POD=$(kubectl get pods -n kube-system -l component=etcd -o jsonpath='{.items[0].metadata.name}')

# etcdãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å®Ÿè¡Œ
kubectl exec -n kube-system $ETCD_POD -- etcdctl \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  snapshot save /tmp/etcd-backup-$TIMESTAMP.db

# ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ­ãƒ¼ã‚«ãƒ«ã«ã‚³ãƒ”ãƒ¼
kubectl cp kube-system/$ETCD_POD:/tmp/etcd-backup-$TIMESTAMP.db $BACKUP_DIR/etcd-backup-$TIMESTAMP.db

echo "ETCD backup completed: $BACKUP_DIR/etcd-backup-$TIMESTAMP.db" >> /opt/candidate/disaster-recovery-plan.txt
echo "" >> /opt/candidate/disaster-recovery-plan.txt

# 2. Kubernetesãƒªã‚½ãƒ¼ã‚¹ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
echo "=== Kubernetes Resources Backup ===" >> /opt/candidate/disaster-recovery-plan.txt

# å…¨namespaceã®ãƒªã‚½ãƒ¼ã‚¹ã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
for ns in $(kubectl get ns -o jsonpath='{.items[*].metadata.name}'); do
  mkdir -p $BACKUP_DIR/resources/$ns
  
  # Deployments
  kubectl get deployments -n $ns -o yaml > $BACKUP_DIR/resources/$ns/deployments.yaml 2>/dev/null
  
  # Services
  kubectl get services -n $ns -o yaml > $BACKUP_DIR/resources/$ns/services.yaml 2>/dev/null
  
  # ConfigMaps
  kubectl get configmaps -n $ns -o yaml > $BACKUP_DIR/resources/$ns/configmaps.yaml 2>/dev/null
  
  # Secrets
  kubectl get secrets -n $ns -o yaml > $BACKUP_DIR/resources/$ns/secrets.yaml 2>/dev/null
done

echo "Kubernetes resources backed up to: $BACKUP_DIR/resources/" >> /opt/candidate/disaster-recovery-plan.txt
echo "" >> /opt/candidate/disaster-recovery-plan.txt

# 3. æ°¸ç¶šãƒœãƒªãƒ¥ãƒ¼ãƒ æƒ…å ±ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
echo "=== Persistent Volumes Backup Info ===" >> /opt/candidate/disaster-recovery-plan.txt
kubectl get pv -o yaml > $BACKUP_DIR/persistent-volumes.yaml
kubectl get pvc --all-namespaces -o yaml > $BACKUP_DIR/persistent-volume-claims.yaml

# PVæƒ…å ±ã‚’ãƒ¬ãƒãƒ¼ãƒˆã«è¨˜éŒ²
kubectl get pv >> /opt/candidate/disaster-recovery-plan.txt
echo "" >> /opt/candidate/disaster-recovery-plan.txt

# 4. è‡ªå‹•ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¹ã‚¯ãƒªãƒ—ãƒˆä½œæˆ
cat <<'EOF' > /opt/candidate/automated-backup.sh
#!/bin/bash

# Kubernetes Automated Backup Script
BACKUP_BASE_DIR="/opt/backups"
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
BACKUP_DIR="$BACKUP_BASE_DIR/$TIMESTAMP"
RETENTION_DAYS=7

echo "Starting automated backup at $(date)"

# Create backup directory
mkdir -p $BACKUP_DIR

# 1. ETCD Backup
echo "Creating ETCD backup..."
ETCD_POD=$(kubectl get pods -n kube-system -l component=etcd -o jsonpath='{.items[0].metadata.name}')

kubectl exec -n kube-system $ETCD_POD -- etcdctl \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  snapshot save /tmp/etcd-backup-$TIMESTAMP.db

kubectl cp kube-system/$ETCD_POD:/tmp/etcd-backup-$TIMESTAMP.db $BACKUP_DIR/etcd-backup.db

# 2. Kubernetes Resources
echo "Backing up Kubernetes resources..."
mkdir -p $BACKUP_DIR/resources

for ns in $(kubectl get ns -o jsonpath='{.items[*].metadata.name}'); do
  mkdir -p $BACKUP_DIR/resources/$ns
  kubectl get all,cm,secrets,pvc -n $ns -o yaml > $BACKUP_DIR/resources/$ns/all-resources.yaml 2>/dev/null
done

# 3. Cluster-wide resources
kubectl get nodes -o yaml > $BACKUP_DIR/nodes.yaml
kubectl get pv -o yaml > $BACKUP_DIR/persistent-volumes.yaml
kubectl get crd -o yaml > $BACKUP_DIR/custom-resource-definitions.yaml

# 4. Cleanup old backups
echo "Cleaning up backups older than $RETENTION_DAYS days..."
find $BACKUP_BASE_DIR -type d -mtime +$RETENTION_DAYS -exec rm -rf {} + 2>/dev/null

# 5. Verify backup
if [ -f "$BACKUP_DIR/etcd-backup.db" ]; then
  echo "Backup completed successfully: $BACKUP_DIR"
  # Send notification (add your notification logic here)
  # curl -X POST -H 'Content-Type: application/json' -d '{"text":"Backup completed: '$BACKUP_DIR'"}' $SLACK_WEBHOOK
else
  echo "ERROR: Backup failed!"
  exit 1
fi

echo "Backup completed at $(date)"
EOF

chmod +x /opt/candidate/automated-backup.sh

# 5. å¾©æ—§æ‰‹é †æ›¸ä½œæˆ
cat << RECOVERY_PROCEDURE >> /opt/candidate/disaster-recovery-plan.txt

=== Disaster Recovery Procedures ===

1. ETCD Cluster Recovery:
   a. Stop kube-apiserver on all master nodes
   b. Stop etcd on all etcd nodes
   c. Remove existing etcd data directory
   d. Restore from backup:
      etcdctl snapshot restore /path/to/backup.db \\
        --data-dir=/var/lib/etcd \\
        --name=<node-name> \\
        --initial-cluster=<cluster-info> \\
        --initial-cluster-token=<token> \\
        --initial-advertise-peer-urls=<peer-urls>
   e. Start etcd service
   f. Start kube-apiserver

2. Kubernetes Resources Recovery:
   kubectl apply -f /path/to/backup/resources/

3. Persistent Volume Recovery:
   - Restore underlying storage (depends on storage provider)
   - Recreate PV objects: kubectl apply -f persistent-volumes.yaml
   - Verify PVC binding

4. Verification Steps:
   - kubectl get nodes
   - kubectl get pods --all-namespaces
   - kubectl get pv,pvc
   - Verify application functionality

5. Automated Backup Schedule:
   Add to crontab: 0 2 * * * /opt/candidate/automated-backup.sh

=== Recovery Time Objectives ===
- ETCD Recovery: 15-30 minutes
- Application Recovery: 30-60 minutes
- Full Cluster Recovery: 1-2 hours

=== Recovery Point Objectives ===
- ETCD: Last snapshot (hourly backups recommended)
- Application Data: Depends on storage backup frequency

RECOVERY_PROCEDURE

echo "Automated backup script created: /opt/candidate/automated-backup.sh" >> /opt/candidate/disaster-recovery-plan.txt

# çµæœç¢ºèª
echo "" >> /opt/candidate/disaster-recovery-plan.txt
echo "=== Backup Summary ===" >> /opt/candidate/disaster-recovery-plan.txt
echo "ETCD Backup: $(ls -lh $BACKUP_DIR/etcd-backup-$TIMESTAMP.db)" >> /opt/candidate/disaster-recovery-plan.txt
echo "Resources Backup: $(du -sh $BACKUP_DIR/resources)" >> /opt/candidate/disaster-recovery-plan.txt
echo "Total Backup Size: $(du -sh $BACKUP_DIR)" >> /opt/candidate/disaster-recovery-plan.txt

cat /opt/candidate/disaster-recovery-plan.txt
```

**æ¡ç‚¹ãƒã‚¤ãƒ³ãƒˆ:**
- etcdãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®å®Ÿè¡Œ (30%)
- Kubernetesãƒªã‚½ãƒ¼ã‚¹ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ— (25%)
- å¾©æ—§æ‰‹é †æ›¸ã®ä½œæˆ (25%)
- è‡ªå‹•åŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ä½œæˆ (20%)
</details>

---

**æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—**: [Practice Exam 2](./practice-exam-02.md) ã§ã‚ˆã‚Šé«˜åº¦ãªå•é¡Œã«æŒ‘æˆ¦ã—ã¦ãã ã•ã„ã€‚