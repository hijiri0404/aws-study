#!/usr/bin/env python3
"""
å°èª¬å®¶ã«ãªã‚ã† - æ‹¡å¼µç‰ˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ä½œå“åˆ¥é€²æ—ç®¡ç†ãƒ»æ›´æ–°æ¤œçŸ¥ãƒ»å¢—åˆ†ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾å¿œ
"""

import requests
import time
import sys
import os
import re
import json
import argparse
from datetime import datetime
from urllib.parse import urljoin
from bs4 import BeautifulSoup

class EnhancedNovelDownloader:
    def __init__(self, work_id, output_dir="/home/hijiri/claude/claude/narou", start_chapter=1, end_chapter=None, force_update=False):
        self.work_id = work_id
        self.base_url = f"https://ncode.syosetu.com/{work_id}/"
        self.output_dir = output_dir
        self.start_chapter = start_chapter
        self.end_chapter = end_chapter
        self.force_update = force_update
        self.novel_title = None
        self.author_name = None
        self.failed_chapters = []
        
        # ãƒ‡ãƒ¼ã‚¿ç®¡ç†ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.data_dir = os.path.join(output_dir, "data")
        os.makedirs(self.data_dir, exist_ok=True)
        
        # ä½œå“ç®¡ç†ç”¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆdataãƒ•ã‚©ãƒ«ãƒ€å†…ï¼‰
        self.db_file = os.path.join(self.data_dir, "novels_database.json")
        self.work_progress_file = os.path.join(self.data_dir, f"progress_{work_id}.json")
        
        # åˆæœŸåŒ–
        self.ensure_database()
        
    def ensure_database(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ"""
        if not os.path.exists(self.db_file):
            initial_db = {
                "created_at": datetime.now().isoformat(),
                "works": {}
            }
            with open(self.db_file, 'w', encoding='utf-8') as f:
                json.dump(initial_db, f, ensure_ascii=False, indent=2)
    
    def load_database(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ãƒ­ãƒ¼ãƒ‰"""
        with open(self.db_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def save_database(self, db_data):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ä¿å­˜"""
        with open(self.db_file, 'w', encoding='utf-8') as f:
            json.dump(db_data, f, ensure_ascii=False, indent=2)
    
    def get_work_info(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ä½œå“æƒ…å ±ã‚’å–å¾—"""
        db = self.load_database()
        return db["works"].get(self.work_id, {})
    
    def update_work_info(self, info):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ä½œå“æƒ…å ±ã‚’æ›´æ–°"""
        db = self.load_database()
        if self.work_id not in db["works"]:
            db["works"][self.work_id] = {}
        
        db["works"][self.work_id].update(info)
        db["works"][self.work_id]["last_updated"] = datetime.now().isoformat()
        self.save_database(db)
    
    def get_novel_info(self):
        """ä½œå“æƒ…å ±ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ãƒ»ä½œè€…ãƒ»ç« æ•°ï¼‰ã‚’å–å¾—"""
        try:
            print(f"ä½œå“æƒ…å ±å–å¾—ä¸­: {self.base_url}")
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'ja,en-US;q=0.9,en;q=0.8',
            }
            
            response = requests.get(self.base_url, timeout=30, headers=headers)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ã‚¿ã‚¤ãƒˆãƒ«ã®æŠ½å‡º
            title_selectors = [
                'h1.p-novel__title',
                'h1',
                '.novel_title',
                'title',
            ]
            
            for selector in title_selectors:
                element = soup.select_one(selector)
                if element:
                    title_text = element.get_text(strip=True)
                    if selector == 'title':
                        title_text = title_text.split('å°èª¬å®¶ã«ãªã‚ã†')[0].strip()
                        title_text = title_text.split(' - ')[0].strip()
                    if title_text and title_text != 'å°èª¬å®¶ã«ãªã‚ã†':
                        self.novel_title = title_text
                        break
            
            if not self.novel_title:
                self.novel_title = f"ä½œå“ID_{self.work_id}"
            
            # ä½œè€…åã®æŠ½å‡º
            author_selectors = [
                '.p-novel__author a',
                '.novel_writername a',
                'a[href*="/user/"]',
            ]
            
            for selector in author_selectors:
                element = soup.select_one(selector)
                if element:
                    self.author_name = element.get_text(strip=True)
                    break
            
            if not self.author_name:
                self.author_name = "ä¸æ˜"
            
            # æœ€çµ‚æ›´æ–°æ—¥ã®å–å¾—
            last_update = None
            update_selectors = [
                '.p-novel__update',
                '.novel_update',
                '[class*="update"]'
            ]
            
            for selector in update_selectors:
                element = soup.select_one(selector)
                if element:
                    last_update = element.get_text(strip=True)
                    break
            
            # ç« æ•°ã®æ¤œå‡º
            if not self.end_chapter:
                # ç›®æ¬¡ãƒšãƒ¼ã‚¸ã‹ã‚‰ã®ç« æ•°æ¤œå‡ºï¼ˆåˆ¶é™ã‚ã‚Šï¼‰
                chapter_links = soup.select('a[href*="/' + self.work_id + '/"]')
                chapter_numbers = []
                
                for link in chapter_links:
                    href = link.get('href', '')
                    match = re.search(rf'/{self.work_id}/(\d+)/', href)
                    if match:
                        chapter_numbers.append(int(match.group(1)))
                
                # ç›®æ¬¡ã§100ç« ã—ã‹è¡¨ç¤ºã•ã‚Œãªã„å ´åˆãŒã‚ã‚‹ãŸã‚ã€äºŒåˆ†æ¢ç´¢ã§å®Ÿéš›ã®æœ€å¤§å€¤ã‚’æ¤œå‡º
                if chapter_numbers:
                    max_from_links = max(chapter_numbers)
                    # 100ç« ä¸åº¦ã®å ´åˆã¯ã€ãã‚Œã‚ˆã‚Šå¤šã„å¯èƒ½æ€§ãŒã‚ã‚‹ã®ã§äºŒåˆ†æ¢ç´¢å®Ÿè¡Œ
                    if max_from_links == 100:
                        print(f"ç›®æ¬¡ã‹ã‚‰æ¤œå‡º: {max_from_links}ç« ï¼ˆåˆ¶é™ã®å¯èƒ½æ€§ã‚ã‚Šï¼‰")
                        self.end_chapter = self.detect_max_chapter()
                    else:
                        self.end_chapter = max_from_links
                else:
                    self.end_chapter = self.detect_max_chapter()
            
            # ä½œå“æƒ…å ±ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
            work_info = {
                "title": self.novel_title,
                "author": self.author_name,
                "total_chapters": self.end_chapter,
                "web_last_update": last_update,
                "first_downloaded": datetime.now().isoformat()
            }
            
            # æ—¢å­˜ã®ä½œå“æƒ…å ±ãŒã‚ã‚‹å ´åˆã¯ä¸€éƒ¨æƒ…å ±ã‚’ä¿æŒ
            existing_info = self.get_work_info()
            if existing_info:
                work_info["first_downloaded"] = existing_info.get("first_downloaded", work_info["first_downloaded"])
            
            self.update_work_info(work_info)
            
            print(f"ä½œå“å: {self.novel_title}")
            print(f"ä½œè€…: {self.author_name}")
            print(f"æ¤œå‡ºç« æ•°: {self.end_chapter}ç« ")
            if last_update:
                print(f"WEBæœ€çµ‚æ›´æ–°: {last_update}")
            
            return True
            
        except Exception as e:
            print(f"ä½œå“æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šæ—¢å­˜ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰å–å¾—
            existing_info = self.get_work_info()
            if existing_info:
                self.novel_title = existing_info.get("title", f"ä½œå“ID_{self.work_id}")
                self.author_name = existing_info.get("author", "ä¸æ˜")
                if not self.end_chapter:
                    self.end_chapter = existing_info.get("total_chapters", 100)
                return True
            else:
                self.novel_title = f"ä½œå“ID_{self.work_id}"
                self.author_name = "ä¸æ˜"
                if not self.end_chapter:
                    self.end_chapter = 100
                return False
    
    def detect_max_chapter(self):
        """æœ€å¤§ç« æ•°ã‚’è‡ªå‹•æ¤œå‡º"""
        print("æœ€å¤§ç« æ•°ã‚’æ¤œå‡ºä¸­...")
        
        test_chapters = [1000, 500, 200, 100, 50, 20, 10]
        max_found = 1
        
        for test_num in test_chapters:
            if self.chapter_exists(test_num):
                max_found = test_num
                break
        
        # äºŒåˆ†æ¢ç´¢ã§æ­£ç¢ºãªå€¤ã‚’è¦‹ã¤ã‘ã‚‹
        low = max_found
        high = max_found * 2
        
        while self.chapter_exists(high):
            low = high
            high *= 2
            if high > 10000:
                break
        
        while low < high:
            mid = (low + high + 1) // 2
            if self.chapter_exists(mid):
                low = mid
            else:
                high = mid - 1
        
        print(f"æœ€å¤§ç« æ•°æ¤œå‡ºå®Œäº†: {low}ç« ")
        return low
    
    def chapter_exists(self, chapter_num):
        """æŒ‡å®šç« ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        try:
            url = urljoin(self.base_url, f"{chapter_num}/")
            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
            response = requests.head(url, headers=headers, timeout=10)
            return response.status_code == 200
        except:
            return False
    
    def load_progress(self):
        """é€²æ—çŠ¶æ³ã‚’ãƒ­ãƒ¼ãƒ‰"""
        if os.path.exists(self.work_progress_file):
            with open(self.work_progress_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {"completed": [], "failed": [], "download_sessions": []}
    
    def save_progress(self, completed, failed):
        """é€²æ—çŠ¶æ³ã‚’ä¿å­˜"""
        progress = self.load_progress()
        progress.update({
            "work_id": self.work_id,
            "novel_title": self.novel_title,
            "completed": completed,
            "failed": failed,
            "last_update": datetime.now().isoformat(),
            "total_chapters": self.end_chapter
        })
        
        # ä»Šå›ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±ã‚’è¨˜éŒ²
        current_session = {
            "session_date": datetime.now().isoformat(),
            "chapters_downloaded": len(set(completed) - set(progress.get("completed", []))),
            "total_chapters_at_time": self.end_chapter,
            "failed_chapters": failed
        }
        
        if "download_sessions" not in progress:
            progress["download_sessions"] = []
        progress["download_sessions"].append(current_session)
        
        with open(self.work_progress_file, 'w', encoding='utf-8') as f:
            json.dump(progress, f, ensure_ascii=False, indent=2)
    
    def check_for_updates(self):
        """æ›´æ–°ãƒã‚§ãƒƒã‚¯ï¼ˆæ–°ç« ãƒ»ç« æ•°å¢—åŠ ã®æ¤œå‡ºï¼‰"""
        work_info = self.get_work_info()
        progress = self.load_progress()
        
        if not work_info:
            print("åˆå›ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã§ã™")
            return True
        
        previous_total = work_info.get("total_chapters", 0)
        previous_completed = set(progress.get("completed", []))
        
        print(f"æ›´æ–°ãƒã‚§ãƒƒã‚¯:")
        print(f"  å‰å›ã®ç·ç« æ•°: {previous_total}ç« ")
        print(f"  å‰å›å®Œäº†ç« æ•°: {len(previous_completed)}ç« ")
        print(f"  ç¾åœ¨ã®ç·ç« æ•°: {self.end_chapter}ç« ")
        
        if self.end_chapter > previous_total:
            new_chapters = self.end_chapter - previous_total
            print(f"  ğŸ†• æ–°ç« ç™ºè¦‹: {new_chapters}ç« è¿½åŠ ã•ã‚Œã¦ã„ã¾ã™")
            return True
        elif len(previous_completed) < previous_total:
            incomplete = previous_total - len(previous_completed)
            print(f"  âš ï¸  æœªå®Œäº†ç« : {incomplete}ç« ãŒæœªãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã§ã™")
            return True
        else:
            print("  âœ… æ›´æ–°ãªã—ï¼ˆå…¨ç« ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ï¼‰")
            if not self.force_update:
                print("å¼·åˆ¶æ›´æ–°ã™ã‚‹ã«ã¯ --force ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„")
                return False
            else:
                print("--force ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã«ã‚ˆã‚Šå¼·åˆ¶å®Ÿè¡Œã—ã¾ã™")
                return True
    
    def get_output_filename(self):
        """å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ"""
        safe_title = re.sub(r'[<>:"/\\|?*]', '_', self.novel_title)
        timestamp = datetime.now().strftime('%Y%m%d')
        return os.path.join(self.output_dir, f"{self.work_id}_{safe_title}_{timestamp}.txt")
    
    def extract_chapter_content(self, url, chapter_num):
        """ç« ã®å†…å®¹ã‚’æŠ½å‡ºã™ã‚‹"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'ja,en-US;q=0.9,en;q=0.8',
            }
            
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ã‚¿ã‚¤ãƒˆãƒ«ã®æŠ½å‡º
            title = None
            title_patterns = [
                soup.find('h1'),
                soup.find('h2'),
                soup.find('div', class_='novel_title'),
                soup.find('p', class_='novel_subtitle'),
            ]
            
            for pattern in title_patterns:
                if pattern and pattern.get_text(strip=True):
                    title = pattern.get_text(strip=True)
                    break
            
            if not title:
                page_title = soup.find('title')
                if page_title:
                    title_text = page_title.get_text()
                    if 'å°èª¬å®¶ã«ãªã‚ã†' in title_text:
                        title = title_text.split('å°èª¬å®¶ã«ãªã‚ã†')[0].strip()
                    else:
                        title = title_text.strip()
                else:
                    title = f"ç¬¬{chapter_num}è©±"
            
            # æœ¬æ–‡ã®æŠ½å‡º
            content = None
            content_selectors = [
                'div.novel_view',
                'div#novel_honbun', 
                'div.p-novel__body',
                'div.novel_body',
                'div[id*="honbun"]',
                'div[class*="novel"]',
            ]
            
            for selector in content_selectors:
                element = soup.select_one(selector)
                if element:
                    content = element
                    break
            
            if not content:
                all_divs = soup.find_all('div')
                for div in all_divs:
                    div_text = div.get_text(strip=True)
                    if len(div_text) > 200 and re.search(r'[ã²ã‚‰ãŒãªã‚«ã‚¿ã‚«ãƒŠæ¼¢å­—]', div_text):
                        content = div
                        break
            
            if content:
                content_text = content.get_text(separator='\n', strip=True)
                
                lines = content_text.split('\n')
                cleaned_lines = []
                seen_lines = set()
                
                for line in lines:
                    line = line.strip()
                    if line and not self.is_noise_line(line) and line not in seen_lines:
                        cleaned_lines.append(line)
                        seen_lines.add(line)
                
                content_text = '\n\n'.join(cleaned_lines)
                return title, content_text
            else:
                return title, "æœ¬æ–‡ã®æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ"
                
        except requests.exceptions.RequestException as e:
            return f"ç¬¬{chapter_num}è©±", f"HTTPå–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}"
        except Exception as e:
            return f"ç¬¬{chapter_num}è©±", f"å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}"
    
    def is_noise_line(self, line):
        """ãƒã‚¤ã‚ºãƒ©ã‚¤ãƒ³åˆ¤å®š"""
        noise_patterns = [
            r'^(å‰ã¸|æ¬¡ã¸|ç›®æ¬¡|ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯)',
            r'^[0-9]+$',
            r'^(â€»|æ³¨æ„|è­¦å‘Š)',
            r'(åºƒå‘Š|AD|PR)',
            r'^(æ›´æ–°|æŠ•ç¨¿)',
            r'è©•ä¾¡|æ„Ÿæƒ³|ãƒ¬ãƒ“ãƒ¥ãƒ¼'
        ]
        
        for pattern in noise_patterns:
            if re.search(pattern, line):
                return True
        return False
    
    def initialize_output_file(self, output_file):
        """å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã®åˆæœŸåŒ–"""
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("===============================================\n")
            f.write(f"{self.novel_title}\n")
            f.write("===============================================\n\n")
            f.write("â€» æœ¬æ–‡ã¯ã€Œå°èª¬å®¶ã«ãªã‚ã†ã€ã‚ˆã‚Šå–å¾—\n")
            f.write(f"â€» ä½œå“ID: {self.work_id}\n")
            f.write(f"â€» URL: {self.base_url}\n")
            f.write(f"â€» ä½œè€…: {self.author_name}\n")
            f.write(f"â€» å–å¾—æ—¥: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"â€» ç« æ•°: {self.end_chapter}ç« \n")
            f.write("â€» æ‹¡å¼µç‰ˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ€ãƒ¼ä½¿ç”¨ï¼ˆé€²æ—ç®¡ç†ãƒ»æ›´æ–°æ¤œçŸ¥å¯¾å¿œï¼‰\n\n")
    
    def append_chapter(self, output_file, chapter_num, title, content):
        """ç« ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«è¿½è¨˜"""
        with open(output_file, 'a', encoding='utf-8') as f:
            f.write("===============================================\n")
            f.write(f"ç¬¬{chapter_num}ç«  - {title}\n")
            f.write("===============================================\n\n")
            f.write(content)
            f.write("\n\n")
    
    def show_progress(self, current, total, failed_count):
        """é€²æ—è¡¨ç¤º"""
        percentage = (current / total) * 100
        print(f"\ré€²æ—: {current}/{total} ({percentage:.1f}%) | å¤±æ•—: {failed_count}", end='', flush=True)
    
    def download_all(self):
        """å…¨ç« ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Ÿè¡Œ"""
        # ä½œå“æƒ…å ±ã‚’å–å¾—
        if not self.get_novel_info():
            print("ä½œå“æƒ…å ±ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸãŒã€å‡¦ç†ã‚’ç¶šè¡Œã—ã¾ã™")
        
        # æ›´æ–°ãƒã‚§ãƒƒã‚¯
        if not self.check_for_updates():
            print("ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
            return
        
        # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åã‚’æ±ºå®š
        output_file = self.get_output_filename()
        
        print(f"\nå°èª¬ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹: {self.novel_title}")
        print(f"ä½œå“ID: {self.work_id}")
        print(f"å–å¾—äºˆå®š: ç¬¬{self.start_chapter}ç« ï½ç¬¬{self.end_chapter}ç« ")
        print(f"å‡ºåŠ›å…ˆ: {output_file}")
        
        # é€²æ—ã‚’ãƒ­ãƒ¼ãƒ‰
        progress = self.load_progress()
        completed_chapters = set(progress.get("completed", []))
        failed_chapters = set(progress.get("failed", []))
        
        # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆæœŸåŒ–
        self.initialize_output_file(output_file)
        
        total_chapters = self.end_chapter - self.start_chapter + 1
        processed = 0
        new_downloads = 0
        
        for chapter_num in range(self.start_chapter, self.end_chapter + 1):
            # å¼·åˆ¶æ›´æ–°ã§ãªã„å ´åˆã€å®Œäº†æ¸ˆã¿ç« ã¯ã‚¹ã‚­ãƒƒãƒ—
            if not self.force_update and chapter_num in completed_chapters:
                processed += 1
                self.show_progress(processed, total_chapters, len(failed_chapters))
                continue
            
            chapter_url = urljoin(self.base_url, f"{chapter_num}/")
            title, content = self.extract_chapter_content(chapter_url, chapter_num)
            
            if "ã‚¨ãƒ©ãƒ¼" in content:
                failed_chapters.add(chapter_num)
                print(f"\nç¬¬{chapter_num}ç« ã®å–å¾—ã«å¤±æ•—")
            else:
                self.append_chapter(output_file, chapter_num, title, content)
                completed_chapters.add(chapter_num)
                new_downloads += 1
                print(f"\nç¬¬{chapter_num}ç« å®Œäº†: {title}")
            
            processed += 1
            self.show_progress(processed, total_chapters, len(failed_chapters))
            
            # é€²æ—ã‚’ä¿å­˜
            self.save_progress(list(completed_chapters), list(failed_chapters))
            
            # ã‚µã‚¤ãƒˆã«è² è·ã‚’ã‹ã‘ãªã„ã‚ˆã†å¾…æ©Ÿ
            time.sleep(2.5)
        
        print(f"\n\nå‡¦ç†å®Œäº†!")
        print(f"ä½œå“å: {self.novel_title}")
        print(f"ä»Šå›ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰: {new_downloads}ç« ")
        print(f"ç´¯è¨ˆå®Œäº†: {len(completed_chapters)}ç« ")
        print(f"å¤±æ•—: {len(failed_chapters)}ç« ")
        print(f"å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {output_file}")
        
        if failed_chapters:
            print(f"å¤±æ•—ã—ãŸç« : {sorted(failed_chapters)}")
        
        # ä½œå“æƒ…å ±ã‚’æ›´æ–°
        self.update_work_info({
            "total_chapters": self.end_chapter,
            "completed_chapters": len(completed_chapters),
            "last_download_session": datetime.now().isoformat()
        })
    
    def show_status(self):
        """ä½œå“ã®çŠ¶æ…‹ã‚’è¡¨ç¤º"""
        work_info = self.get_work_info()
        progress = self.load_progress()
        
        if not work_info:
            print(f"ä½œå“ID {self.work_id} ã®æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return
        
        print(f"=== ä½œå“çŠ¶æ…‹: {work_info.get('title', self.work_id)} ===")
        print(f"ä½œå“ID: {self.work_id}")
        print(f"ä½œè€…: {work_info.get('author', 'ä¸æ˜')}")
        print(f"ç·ç« æ•°: {work_info.get('total_chapters', 0)}ç« ")
        print(f"å®Œäº†ç« æ•°: {len(progress.get('completed', []))}ç« ")
        print(f"å¤±æ•—ç« æ•°: {len(progress.get('failed', []))}ç« ")
        print(f"åˆå›DL: {work_info.get('first_downloaded', 'ä¸æ˜')}")
        print(f"æœ€çµ‚DL: {work_info.get('last_download_session', 'ä¸æ˜')}")
        
        sessions = progress.get('download_sessions', [])
        if sessions:
            print(f"\nãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å±¥æ­´:")
            for i, session in enumerate(sessions[-5:], 1):  # æœ€æ–°5ä»¶
                print(f"  {i}. {session['session_date'][:10]} - {session['chapters_downloaded']}ç« å–å¾—")

def main():
    parser = argparse.ArgumentParser(description='å°èª¬å®¶ã«ãªã‚ã† æ‹¡å¼µç‰ˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ€ãƒ¼')
    parser.add_argument('work_id', help='ä½œå“ID (ä¾‹: n7069ds, n2627t, n5455cx)')
    parser.add_argument('--start', type=int, default=1, help='é–‹å§‹ç«  (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1)')
    parser.add_argument('--end', type=int, help='çµ‚äº†ç«  (æœªæŒ‡å®šæ™‚ã¯è‡ªå‹•æ¤œå‡º)')
    parser.add_argument('--output-dir', default='/home/hijiri/claude/claude/narou', help='å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
    parser.add_argument('--force', action='store_true', help='å¼·åˆ¶æ›´æ–°ï¼ˆæ—¢ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç« ã‚‚å†å–å¾—ï¼‰')
    parser.add_argument('--status', action='store_true', help='ä½œå“ã®çŠ¶æ…‹ã‚’è¡¨ç¤º')
    
    args = parser.parse_args()
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    os.makedirs(args.output_dir, exist_ok=True)
    
    downloader = EnhancedNovelDownloader(
        work_id=args.work_id,
        output_dir=args.output_dir,
        start_chapter=args.start,
        end_chapter=args.end,
        force_update=args.force
    )
    
    if args.status:
        # ä½œå“æƒ…å ±ã‚’å–å¾—ã—ã¦è¡¨ç¤º
        downloader.get_novel_info()
        downloader.show_status()
    else:
        downloader.download_all()

if __name__ == "__main__":
    main()