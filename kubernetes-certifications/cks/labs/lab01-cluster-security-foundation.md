# Lab 1: ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£åŸºç›¤æ§‹ç¯‰

## ğŸ¯ å­¦ç¿’ç›®æ¨™

ã“ã®ãƒ©ãƒœã§ã¯ã€CKSè©¦é¨“ã§æœ€ã‚‚é‡è¦ãªã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãƒ¬ãƒ™ãƒ«ã®ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£åŸºç›¤ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚ã‚»ã‚­ãƒ¥ã‚¢ãªKubernetesã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®è¨­è¨ˆãƒ»å®Ÿè£…ã‹ã‚‰ã€ç¶™ç¶šçš„ãªã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ç›£è¦–ã¾ã§ã€ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºãƒ¬ãƒ™ãƒ«ã®ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å®Ÿè£…ã‚’ç¿’å¾—ã—ã¾ã™ã€‚

**ç¿’å¾—ã‚¹ã‚­ãƒ«**:
- ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãƒãƒ¼ãƒ‰ãƒ‹ãƒ³ã‚°ã¨CIS Benchmarké©ç”¨
- TLS/SSLè¨¼æ˜æ›¸ç®¡ç†ã¨Ingressæš—å·åŒ–
- ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã¨ãƒã‚¤ã‚¯ãƒ­ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
- RBACè¨­è¨ˆã¨æœ€å°æ¨©é™ã®åŸå‰‡å®Ÿè£…
- ç›£æŸ»ãƒ­ã‚°ã¨ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ç›£è¦–è¨­å®š

**æ‰€è¦æ™‚é–“**: 8-10æ™‚é–“  
**æ¨å®šã‚³ã‚¹ãƒˆ**: $25-40  
**é›£æ˜“åº¦**: â­â­â­â­â­

## ğŸ“‹ ã‚·ãƒŠãƒªã‚ª

**ä¼æ¥­**: é‡‘èã‚µãƒ¼ãƒ“ã‚¹ä¼šç¤¾  
**è¦ä»¶**: PCI DSSæº–æ‹ ã€SOC2 Type IIå¯¾å¿œ  
**ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ**: ã‚¼ãƒ­ãƒˆãƒ©ã‚¹ãƒˆã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«ã‚ˆã‚‹ãƒã‚¤ã‚¯ãƒ­ã‚µãƒ¼ãƒ“ã‚¹åŸºç›¤  
**ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¦ä»¶**: 
- æš—å·åŒ–é€šä¿¡ã®å¼·åˆ¶
- æœ€å°æ¨©é™ã«ã‚ˆã‚‹ã‚¢ã‚¯ã‚»ã‚¹åˆ¶å¾¡
- å…¨é€šä¿¡ã®ç›£æŸ»è¨¼è·¡
- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è„…å¨æ¤œçŸ¥
- ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆå¯¾å¿œè‡ªå‹•åŒ–

## Phase 1: ã‚»ã‚­ãƒ¥ã‚¢ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åŸºç›¤æ§‹ç¯‰

### 1.1 CIS Benchmarkæº–æ‹ ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼è¨­å®š

```bash
#!/bin/bash
# ã‚¹ã‚¯ãƒªãƒ—ãƒˆ: secure-cluster-setup.sh

echo "ğŸ”’ CIS Benchmarkæº–æ‹ ã‚»ã‚­ãƒ¥ã‚¢ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ§‹ç¯‰é–‹å§‹..."

# ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¼·åŒ–ã•ã‚ŒãŸã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼è¨­å®š
cat <<EOF > secure-cluster-config.yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
name: secure-cluster
networking:
  # ã‚»ã‚­ãƒ¥ã‚¢ãªãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è¨­å®š
  podSubnet: "10.244.0.0/16"
  serviceSubnet: "10.96.0.0/12"
  
kubeadmConfigPatches:
- |
  kind: ClusterConfiguration
  apiServer:
    # API Serverã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¼·åŒ–
    extraArgs:
      # åŒ¿åèªè¨¼ç„¡åŠ¹åŒ–
      anonymous-auth: "false"
      # ã‚»ã‚­ãƒ¥ã‚¢ãƒãƒ¼ãƒˆã®ã¿ä½¿ç”¨
      secure-port: "6443"
      # ç›£æŸ»ãƒ­ã‚°æœ‰åŠ¹åŒ–
      audit-log-path: "/var/log/kubernetes/audit.log"
      audit-log-maxage: "30"
      audit-log-maxbackup: "10"
      audit-log-maxsize: "100"
      audit-policy-file: "/etc/kubernetes/audit-policy.yaml"
      # æš—å·åŒ–ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼
      encryption-provider-config: "/etc/kubernetes/encryption-config.yaml"
      # RBACèªå¯ã®ã¿
      authorization-mode: "RBAC"
      # ã‚¢ãƒ‰ãƒŸãƒƒã‚·ãƒ§ãƒ³ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ©ãƒ¼å¼·åŒ–
      enable-admission-plugins: "NodeRestriction,PodSecurityPolicy,ResourceQuota,LimitRanger"
      # ã‚»ã‚­ãƒ¥ã‚¢ãªé€šä¿¡è¨­å®š
      tls-cipher-suites: "TLS_AES_128_GCM_SHA256,TLS_AES_256_GCM_SHA384,TLS_CHACHA20_POLY1305_SHA256"
      tls-min-version: "VersionTLS12"
    extraVolumes:
    - name: audit-policy
      hostPath: /etc/kubernetes/audit-policy.yaml
      mountPath: /etc/kubernetes/audit-policy.yaml
      readOnly: true
    - name: encryption-config
      hostPath: /etc/kubernetes/encryption-config.yaml
      mountPath: /etc/kubernetes/encryption-config.yaml
      readOnly: true
  
  controllerManager:
    extraArgs:
      # ã‚»ã‚­ãƒ¥ã‚¢ãƒã‚¤ãƒ³ãƒ‰
      bind-address: "127.0.0.1"
      # ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆè¨¼æ˜æ›¸ã®è‡ªå‹•ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
      rotate-server-certificates: "true"
      # ã‚»ã‚­ãƒ¥ã‚¢ãªé€šä¿¡
      tls-cipher-suites: "TLS_AES_128_GCM_SHA256,TLS_AES_256_GCM_SHA384"
      tls-min-version: "VersionTLS12"
  
  scheduler:
    extraArgs:
      # ã‚»ã‚­ãƒ¥ã‚¢ãƒã‚¤ãƒ³ãƒ‰
      bind-address: "127.0.0.1"
  
  etcd:
    local:
      extraArgs:
        # etcdã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¨­å®š
        auto-tls: "false"
        peer-auto-tls: "false"
        client-cert-auth: "true"
        peer-client-cert-auth: "true"

- |
  kind: KubeletConfiguration
  # kubeletã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¨­å®š
  authentication:
    anonymous:
      enabled: false
    webhook:
      enabled: true
    x509:
      clientCAFile: "/etc/kubernetes/pki/ca.crt"
  authorization:
    mode: Webhook
  # ã‚»ã‚­ãƒ¥ã‚¢ãªãƒãƒ¼ãƒˆè¨­å®š
  readOnlyPort: 0
  # ä¿è­·ã•ã‚ŒãŸã‚«ãƒ¼ãƒãƒ«è¨­å®š
  protectKernelDefaults: true
  # ã‚»ã‚­ãƒ¥ã‚¢ãªé€šä¿¡
  tlsCipherSuites:
  - TLS_AES_128_GCM_SHA256
  - TLS_AES_256_GCM_SHA384
  tlsMinVersion: VersionTLS12

nodes:
- role: control-plane
  kubeadmConfigPatches:
  - |
    kind: InitConfiguration
    nodeRegistration:
      kubeletExtraArgs:
        # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¼·åŒ–kubeletè¨­å®š
        protect-kernel-defaults: "true"
        read-only-port: "0"
        anonymous-auth: "false"
        authorization-mode: "Webhook"
        client-ca-file: "/etc/kubernetes/pki/ca.crt"
- role: worker
- role: worker
EOF

echo "ğŸ“‹ ç›£æŸ»ãƒãƒªã‚·ãƒ¼ä½œæˆä¸­..."
# Kubernetesã®åŒ…æ‹¬çš„ç›£æŸ»ãƒãƒªã‚·ãƒ¼
cat <<EOF > audit-policy.yaml
apiVersion: audit.k8s.io/v1
kind: Policy
rules:
# ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£é‡è¦ãªãƒªã‚½ãƒ¼ã‚¹ã‚’è©³ç´°ç›£æŸ»
- level: Metadata
  namespaces: ["kube-system", "kube-public", "default"]
  resources:
  - group: ""
    resources: ["secrets", "configmaps", "serviceaccounts"]
  - group: "rbac.authorization.k8s.io"
    resources: ["roles", "rolebindings", "clusterroles", "clusterrolebindings"]

# ç®¡ç†è€…æ¨©é™ã§ã®æ“ä½œã‚’è©³ç´°è¨˜éŒ²
- level: Request
  users: ["admin", "system:admin"]
  resources:
  - group: ""
    resources: ["pods", "services", "persistentvolumes"]
  - group: "apps"
    resources: ["deployments", "daemonsets", "statefulsets"]

# ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒãƒªã‚·ãƒ¼é–¢é€£ã‚’è©³ç´°è¨˜éŒ²
- level: RequestResponse
  resources:
  - group: "policy"
    resources: ["podsecuritypolicies"]
  - group: "networking.k8s.io"
    resources: ["networkpolicies"]

# æ©Ÿå¯†æ“ä½œã®è©³ç´°è¨˜éŒ²
- level: Request
  verbs: ["create", "update", "patch", "delete"]
  resources:
  - group: ""
    resources: ["secrets"]

# ãã®ä»–ã®æ“ä½œã¯è»½é‡ãƒ¬ãƒ™ãƒ«ã§è¨˜éŒ²
- level: Metadata
  omitStages:
  - RequestReceived
EOF

echo "ğŸ” ãƒ‡ãƒ¼ã‚¿æš—å·åŒ–è¨­å®šä½œæˆä¸­..."
# etcdã§ã®ãƒ‡ãƒ¼ã‚¿æš—å·åŒ–è¨­å®š
cat <<EOF > encryption-config.yaml
apiVersion: apiserver.config.k8s.io/v1
kind: EncryptionConfiguration
resources:
- resources:
  - secrets
  - configmaps
  - persistentvolumes
  providers:
  - aescbc:
      keys:
      - name: key1
        secret: $(head -c 32 /dev/urandom | base64)
  - identity: {}
EOF

# ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é©åˆ‡ãªå ´æ‰€ã«é…ç½®
sudo mkdir -p /etc/kubernetes
sudo cp audit-policy.yaml /etc/kubernetes/
sudo cp encryption-config.yaml /etc/kubernetes/

echo "ğŸš€ ã‚»ã‚­ãƒ¥ã‚¢ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼èµ·å‹•ä¸­..."
kind create cluster --config secure-cluster-config.yaml

echo "âœ… ã‚»ã‚­ãƒ¥ã‚¢ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ§‹ç¯‰å®Œäº†!"
```

### 1.2 Pod Security Standardså®Ÿè£…

```yaml
# ãƒ•ã‚¡ã‚¤ãƒ«: pod-security-standards.yaml
# Pod Security Standards ã®æ®µéšçš„å®Ÿè£…

# Baseline Pod Security Standard
apiVersion: v1
kind: Namespace
metadata:
  name: baseline-secure
  labels:
    pod-security.kubernetes.io/enforce: baseline
    pod-security.kubernetes.io/audit: restricted
    pod-security.kubernetes.io/warn: restricted
---
# Restricted Pod Security Standard
apiVersion: v1
kind: Namespace
metadata:
  name: restricted-secure
  labels:
    pod-security.kubernetes.io/enforce: restricted
    pod-security.kubernetes.io/audit: restricted
    pod-security.kubernetes.io/warn: restricted
---
# Privileged (ç®¡ç†ç”¨é€”ã®ã¿)
apiVersion: v1
kind: Namespace
metadata:
  name: privileged-admin
  labels:
    pod-security.kubernetes.io/enforce: privileged
    pod-security.kubernetes.io/audit: privileged
    pod-security.kubernetes.io/warn: privileged
```

```bash
#!/bin/bash
# ã‚¹ã‚¯ãƒªãƒ—ãƒˆ: implement-pod-security-standards.sh

echo "ğŸ›¡ï¸ Pod Security Standardså®Ÿè£…é–‹å§‹..."

# åå‰ç©ºé–“ä½œæˆ
kubectl apply -f pod-security-standards.yaml

echo "ğŸ“Š Pod Security Standardså‹•ä½œãƒ†ã‚¹ãƒˆ..."

# Baseline namespaceã§ã®ãƒ†ã‚¹ãƒˆ
echo "=== Baseline Security Level ==="
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: baseline-test-pod
  namespace: baseline-secure
spec:
  containers:
  - name: test
    image: nginx:1.20
    securityContext:
      runAsUser: 1000
      runAsGroup: 1000
      allowPrivilegeEscalation: false
EOF

# Restricted namespaceã§ã®ãƒ†ã‚¹ãƒˆ
echo "=== Restricted Security Level ==="
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: restricted-test-pod
  namespace: restricted-secure
spec:
  securityContext:
    runAsNonRoot: true
    runAsUser: 1000
    runAsGroup: 1000
    fsGroup: 1000
    seccompProfile:
      type: RuntimeDefault
  containers:
  - name: test
    image: nginx:1.20
    securityContext:
      allowPrivilegeEscalation: false
      readOnlyRootFilesystem: true
      capabilities:
        drop:
        - ALL
    volumeMounts:
    - name: tmp-volume
      mountPath: /tmp
    - name: var-cache
      mountPath: /var/cache/nginx
    - name: var-run
      mountPath: /var/run
  volumes:
  - name: tmp-volume
    emptyDir: {}
  - name: var-cache
    emptyDir: {}
  - name: var-run
    emptyDir: {}
EOF

# ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒãƒªã‚·ãƒ¼é•åã®ãƒ†ã‚¹ãƒˆ
echo "=== Security Policy Violation Test ==="
echo "ä»¥ä¸‹ã®Podã¯æ‹’å¦ã•ã‚Œã‚‹ã¯ãšã§ã™..."
cat <<EOF | kubectl apply -f - || echo "âœ… æœŸå¾…é€šã‚Šã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒãƒªã‚·ãƒ¼é•åã§æ‹’å¦ã•ã‚Œã¾ã—ãŸ"
apiVersion: v1
kind: Pod
metadata:
  name: privileged-test-pod
  namespace: restricted-secure
spec:
  containers:
  - name: test
    image: nginx:1.20
    securityContext:
      privileged: true  # Restricted namespaceã§ã¯æ‹’å¦ã•ã‚Œã‚‹
EOF

echo "ğŸ“‹ Pod Security Standardså®Ÿè£…çŠ¶æ³ç¢ºèª:"
kubectl get namespaces -o custom-columns=NAME:.metadata.name,ENFORCE:.metadata.labels.'pod-security\.kubernetes\.io/enforce',AUDIT:.metadata.labels.'pod-security\.kubernetes\.io/audit'

echo "âœ… Pod Security Standardså®Ÿè£…å®Œäº†!"
```

## Phase 2: TLS/SSLè¨¼æ˜æ›¸ç®¡ç†ã¨Ingressæš—å·åŒ–

### 2.1 cert-managerå°å…¥ã¨TLSè‡ªå‹•åŒ–

```bash
#!/bin/bash
# ã‚¹ã‚¯ãƒªãƒ—ãƒˆ: setup-tls-automation.sh

echo "ğŸ” TLSè¨¼æ˜æ›¸è‡ªå‹•åŒ–ã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰é–‹å§‹..."

# cert-manager ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
echo "ğŸ“¦ cert-manager ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­..."
kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.12.0/cert-manager.yaml

# cert-manager èµ·å‹•å¾…æ©Ÿ
echo "â³ cert-managerèµ·å‹•å¾…æ©Ÿä¸­..."
kubectl wait --for=condition=Ready pod -l app=cert-manager -n cert-manager --timeout=300s
kubectl wait --for=condition=Ready pod -l app=cainjector -n cert-manager --timeout=300s
kubectl wait --for=condition=Ready pod -l app=webhook -n cert-manager --timeout=300s

echo "ğŸ¢ èªè¨¼å±€è¨­å®šä½œæˆä¸­..."
# ã‚»ãƒ«ãƒ•ã‚µã‚¤ãƒ³èªè¨¼å±€ï¼ˆé–‹ç™ºãƒ»ãƒ†ã‚¹ãƒˆç”¨ï¼‰
cat <<EOF | kubectl apply -f -
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: selfsigned-ca-issuer
spec:
  selfSigned: {}
---
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: root-ca-cert
  namespace: cert-manager
spec:
  isCA: true
  commonName: "Secure Kubernetes Root CA"
  secretName: root-ca-secret
  duration: 8760h # 1 year
  renewBefore: 720h # 30 days
  subject:
    organizationalUnits:
    - Security Team
    organizations:
    - Secure Company
    countries:
    - JP
  issuerRef:
    name: selfsigned-ca-issuer
    kind: ClusterIssuer
---
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: ca-issuer
spec:
  ca:
    secretName: root-ca-secret
EOF

# Let's Encryptèªè¨¼å±€ï¼ˆæœ¬ç•ªç”¨ï¼‰
cat <<EOF | kubectl apply -f -
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: letsencrypt-staging
spec:
  acme:
    server: https://acme-staging-v02.api.letsencrypt.org/directory
    email: security@company.com
    privateKeySecretRef:
      name: letsencrypt-staging
    solvers:
    - http01:
        ingress:
          class: nginx
---
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: letsencrypt-prod
spec:
  acme:
    server: https://acme-v02.api.letsencrypt.org/directory
    email: security@company.com
    privateKeySecretRef:
      name: letsencrypt-prod
    solvers:
    - http01:
        ingress:
          class: nginx
EOF

echo "âœ… TLSè¨¼æ˜æ›¸è‡ªå‹•åŒ–ã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰å®Œäº†!"
```

### 2.2 ã‚»ã‚­ãƒ¥ã‚¢Ingresså®Ÿè£…

```yaml
# ãƒ•ã‚¡ã‚¤ãƒ«: secure-ingress-setup.yaml
# NGINX Ingress Controller with security hardening
apiVersion: v1
kind: Namespace
metadata:
  name: ingress-nginx
  labels:
    name: ingress-nginx
    pod-security.kubernetes.io/enforce: baseline
    pod-security.kubernetes.io/audit: baseline
    pod-security.kubernetes.io/warn: baseline
---
# Security-hardened NGINX Ingress
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-ingress-controller
  namespace: ingress-nginx
  labels:
    app: nginx-ingress
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nginx-ingress
  template:
    metadata:
      labels:
        app: nginx-ingress
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "10254"
    spec:
      securityContext:
        runAsNonRoot: true
        runAsUser: 101
        runAsGroup: 101
        fsGroup: 101
        seccompProfile:
          type: RuntimeDefault
      serviceAccountName: nginx-ingress-serviceaccount
      containers:
      - name: nginx-ingress-controller
        image: registry.k8s.io/ingress-nginx/controller:v1.8.1
        args:
        - /nginx-ingress-controller
        - --configmap=$(POD_NAMESPACE)/nginx-configuration
        - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services
        - --udp-services-configmap=$(POD_NAMESPACE)/udp-services
        - --publish-service=$(POD_NAMESPACE)/ingress-nginx
        - --annotations-prefix=nginx.ingress.kubernetes.io
        - --enable-ssl-passthrough
        - --default-ssl-certificate=$(POD_NAMESPACE)/default-tls-cert
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          capabilities:
            drop:
            - ALL
            add:
            - NET_BIND_SERVICE
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        ports:
        - name: http
          containerPort: 80
        - name: https
          containerPort: 443
        - name: metrics
          containerPort: 10254
        livenessProbe:
          httpGet:
            path: /healthz
            port: 10254
            scheme: HTTP
          initialDelaySeconds: 30
          timeoutSeconds: 5
        readinessProbe:
          httpGet:
            path: /healthz
            port: 10254
            scheme: HTTP
          periodSeconds: 1
        resources:
          requests:
            cpu: 100m
            memory: 90Mi
          limits:
            cpu: 200m
            memory: 256Mi
        volumeMounts:
        - name: tmp-volume
          mountPath: /tmp
      volumes:
      - name: tmp-volume
        emptyDir: {}
---
# Security-focused ConfigMap
apiVersion: v1
kind: ConfigMap
metadata:
  name: nginx-configuration
  namespace: ingress-nginx
data:
  # SSL Security
  ssl-protocols: "TLSv1.2 TLSv1.3"
  ssl-ciphers: "ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384"
  ssl-prefer-server-ciphers: "true"
  ssl-session-cache: "shared:SSL:10m"
  ssl-session-timeout: "10m"
  
  # Security Headers
  add-headers: "ingress-nginx/security-headers"
  
  # HSTS
  hsts: "true"
  hsts-max-age: "31536000"
  hsts-include-subdomains: "true"
  hsts-preload: "true"
  
  # Rate Limiting
  rate-limit-connections: "10"
  rate-limit-requests-per-second: "5"
  
  # Additional Security
  hide-headers: "Server,X-Powered-By"
  server-tokens: "false"
  
  # Body size limits
  proxy-body-size: "10m"
  client-max-body-size: "10m"
---
# Security Headers ConfigMap
apiVersion: v1
kind: ConfigMap
metadata:
  name: security-headers
  namespace: ingress-nginx
data:
  X-Frame-Options: "DENY"
  X-Content-Type-Options: "nosniff"
  X-XSS-Protection: "1; mode=block"
  Referrer-Policy: "strict-origin-when-cross-origin"
  Content-Security-Policy: "default-src 'self'; script-src 'self' 'unsafe-inline'; style-src 'self' 'unsafe-inline'; img-src 'self' data:; font-src 'self'; connect-src 'self'; frame-ancestors 'none';"
  Permissions-Policy: "geolocation=(), microphone=(), camera=()"
```

```bash
#!/bin/bash
# ã‚¹ã‚¯ãƒªãƒ—ãƒˆ: deploy-secure-ingress.sh

echo "ğŸŒ ã‚»ã‚­ãƒ¥ã‚¢Ingresså®Ÿè£…é–‹å§‹..."

# NGINX Ingress Controller ãƒ‡ãƒ—ãƒ­ã‚¤
kubectl apply -f secure-ingress-setup.yaml

# ServiceAccount ã¨ RBACè¨­å®š
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nginx-ingress-serviceaccount
  namespace: ingress-nginx
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: nginx-ingress-clusterrole
rules:
- apiGroups: [""]
  resources: ["configmaps", "endpoints", "nodes", "pods", "secrets"]
  verbs: ["list", "watch"]
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["get"]
- apiGroups: [""]
  resources: ["services"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["networking.k8s.io"]
  resources: ["ingresses"]
  verbs: ["get", "list", "watch"]
- apiGroups: [""]
  resources: ["events"]
  verbs: ["create", "patch"]
- apiGroups: ["networking.k8s.io"]
  resources: ["ingresses/status"]
  verbs: ["update"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: nginx-ingress-role
  namespace: ingress-nginx
rules:
- apiGroups: [""]
  resources: ["configmaps", "pods", "secrets", "namespaces"]
  verbs: ["get"]
- apiGroups: [""]
  resources: ["configmaps"]
  resourceNames: ["ingress-controller-leader-nginx"]
  verbs: ["get", "update"]
- apiGroups: [""]
  resources: ["configmaps"]
  verbs: ["create"]
- apiGroups: [""]
  resources: ["endpoints"]
  verbs: ["get"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: nginx-ingress-role-nisa-binding
  namespace: ingress-nginx
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: nginx-ingress-role
subjects:
- kind: ServiceAccount
  name: nginx-ingress-serviceaccount
  namespace: ingress-nginx
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: nginx-ingress-clusterrole-nisa-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: nginx-ingress-clusterrole
subjects:
- kind: ServiceAccount
  name: nginx-ingress-serviceaccount
  namespace: ingress-nginx
EOF

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆTLSè¨¼æ˜æ›¸ä½œæˆ
cat <<EOF | kubectl apply -f -
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: default-tls-cert
  namespace: ingress-nginx
spec:
  secretName: default-tls-cert
  issuerRef:
    name: ca-issuer
    kind: ClusterIssuer
  commonName: "*.secure.local"
  dnsNames:
  - "*.secure.local"
  - "secure.local"
EOF

# Ingress ã‚µãƒ¼ãƒ“ã‚¹ä½œæˆ
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  name: ingress-nginx
  namespace: ingress-nginx
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-type: nlb
spec:
  type: LoadBalancer
  ports:
  - port: 80
    targetPort: 80
    protocol: TCP
    name: http
  - port: 443
    targetPort: 443
    protocol: TCP
    name: https
  selector:
    app: nginx-ingress
EOF

echo "â³ Ingress Controllerèµ·å‹•å¾…æ©Ÿä¸­..."
kubectl wait --for=condition=Ready pod -l app=nginx-ingress -n ingress-nginx --timeout=300s

echo "âœ… ã‚»ã‚­ãƒ¥ã‚¢Ingresså®Ÿè£…å®Œäº†!"
```

## Phase 3: ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã¨ãƒã‚¤ã‚¯ãƒ­ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³

### 3.1 é«˜åº¦ãªNetwork Policyå®Ÿè£…

```yaml
# ãƒ•ã‚¡ã‚¤ãƒ«: network-security-policies.yaml
# å¤šå±¤é˜²å¾¡ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒãƒªã‚·ãƒ¼

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ‹’å¦ãƒãƒªã‚·ãƒ¼ï¼ˆå„åå‰ç©ºé–“ã«é©ç”¨ï¼‰
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-all
  namespace: production
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
---
# ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰å±¤ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒãƒªã‚·ãƒ¼
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: frontend-netpol
  namespace: production
spec:
  podSelector:
    matchLabels:
      tier: frontend
  policyTypes:
  - Ingress
  - Egress
  ingress:
  # Ingressã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ©ãƒ¼ã‹ã‚‰ã®ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯ã®ã¿è¨±å¯
  - from:
    - namespaceSelector:
        matchLabels:
          name: ingress-nginx
    - podSelector:
        matchLabels:
          app: nginx-ingress
    ports:
    - protocol: TCP
      port: 80
    - protocol: TCP
      port: 443
  egress:
  # ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚µãƒ¼ãƒ“ã‚¹ã¸ã®é€šä¿¡ã‚’è¨±å¯
  - to:
    - podSelector:
        matchLabels:
          tier: backend
    ports:
    - protocol: TCP
      port: 8080
  # DNSè§£æ±ºã‚’è¨±å¯
  - to: []
    ports:
    - protocol: UDP
      port: 53
  # HTTPSå¤–éƒ¨APIå‘¼ã³å‡ºã—ã‚’è¨±å¯
  - to: []
    ports:
    - protocol: TCP
      port: 443
---
# ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰å±¤ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒãƒªã‚·ãƒ¼
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: backend-netpol
  namespace: production
spec:
  podSelector:
    matchLabels:
      tier: backend
  policyTypes:
  - Ingress
  - Egress
  ingress:
  # ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã‹ã‚‰ã®ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯ã®ã¿è¨±å¯
  - from:
    - podSelector:
        matchLabels:
          tier: frontend
    ports:
    - protocol: TCP
      port: 8080
  # åŒä¸€ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰é–“ã®é€šä¿¡ã‚’è¨±å¯ï¼ˆãƒã‚¤ã‚¯ãƒ­ã‚µãƒ¼ãƒ“ã‚¹é–“é€£æºï¼‰
  - from:
    - podSelector:
        matchLabels:
          tier: backend
    ports:
    - protocol: TCP
      port: 8080
  egress:
  # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¸ã®é€šä¿¡ã‚’è¨±å¯
  - to:
    - podSelector:
        matchLabels:
          tier: database
    ports:
    - protocol: TCP
      port: 5432
    - protocol: TCP
      port: 3306
  # Redis/Memcached ã¸ã®é€šä¿¡ã‚’è¨±å¯
  - to:
    - podSelector:
        matchLabels:
          tier: cache
    ports:
    - protocol: TCP
      port: 6379
    - protocol: TCP
      port: 11211
  # DNSè§£æ±ºã‚’è¨±å¯
  - to: []
    ports:
    - protocol: UDP
      port: 53
  # å¤–éƒ¨APIã‚µãƒ¼ãƒ“ã‚¹ã¸ã®HTTPSé€šä¿¡ã‚’è¨±å¯
  - to: []
    ports:
    - protocol: TCP
      port: 443
---
# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å±¤ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒãƒªã‚·ãƒ¼
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: database-netpol
  namespace: production
spec:
  podSelector:
    matchLabels:
      tier: database
  policyTypes:
  - Ingress
  - Egress
  ingress:
  # ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã®ã¿è¨±å¯
  - from:
    - podSelector:
        matchLabels:
          tier: backend
    ports:
    - protocol: TCP
      port: 5432
    - protocol: TCP
      port: 3306
  # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¸ãƒ§ãƒ–ã‹ã‚‰ã®æ¥ç¶šã‚’è¨±å¯
  - from:
    - podSelector:
        matchLabels:
          app: database-backup
    ports:
    - protocol: TCP
      port: 5432
    - protocol: TCP
      port: 3306
  # ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰ã®æ¥ç¶šã‚’è¨±å¯
  - from:
    - namespaceSelector:
        matchLabels:
          name: monitoring
    ports:
    - protocol: TCP
      port: 5432
    - protocol: TCP
      port: 3306
  egress:
  # DNSè§£æ±ºã®ã¿è¨±å¯
  - to: []
    ports:
    - protocol: UDP
      port: 53
  # NTPã‚µãƒ¼ãƒ“ã‚¹ã¸ã®æ™‚åˆ»åŒæœŸã‚’è¨±å¯
  - to: []
    ports:
    - protocol: UDP
      port: 123
---
# ç®¡ç†ç”¨namespaceé–“é€šä¿¡ãƒãƒªã‚·ãƒ¼
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: admin-namespace-access
  namespace: production
spec:
  podSelector:
    matchLabels:
      security-tier: admin
  policyTypes:
  - Ingress
  - Egress
  ingress:
  # ç®¡ç†namespaceã‹ã‚‰ã®ã‚¢ã‚¯ã‚»ã‚¹ã‚’è¨±å¯
  - from:
    - namespaceSelector:
        matchLabels:
          security-level: admin
  egress:
  # å¿…è¦ãªç®¡ç†æ“ä½œã®ãŸã‚ã®é€šä¿¡ã‚’è¨±å¯
  - to:
    - namespaceSelector:
        matchLabels:
          security-level: admin
  - to: []
    ports:
    - protocol: TCP
      port: 443
    - protocol: UDP
      port: 53
```

### 3.2 Calico Advanced Network Policyå®Ÿè£…

```bash
#!/bin/bash
# ã‚¹ã‚¯ãƒªãƒ—ãƒˆ: deploy-advanced-network-policies.sh

echo "ğŸ” é«˜åº¦ãªãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒãƒªã‚·ãƒ¼å®Ÿè£…é–‹å§‹..."

# Calico ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆé«˜åº¦ãªãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒãƒªã‚·ãƒ¼ç”¨ï¼‰
echo "ğŸ“¦ Calico ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒãƒªã‚·ãƒ¼ã‚¨ãƒ³ã‚¸ãƒ³ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­..."
kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.26.1/manifests/tigera-operator.yaml

# Calicoè¨­å®š
cat <<EOF | kubectl apply -f -
apiVersion: operator.tigera.io/v1
kind: Installation
metadata:
  name: default
spec:
  calicoNetwork:
    ipPools:
    - blockSize: 26
      cidr: 10.244.0.0/16
      encapsulation: VXLANCrossSubnet
      natOutgoing: Enabled
      nodeSelector: all()
---
apiVersion: operator.tigera.io/v1
kind: APIServer
metadata:
  name: default
spec: {}
EOF

echo "â³ Calicoèµ·å‹•å¾…æ©Ÿä¸­..."
kubectl wait --for=condition=Ready pod -l k8s-app=calico-node -n calico-system --timeout=300s

# åå‰ç©ºé–“ä½œæˆã¨ãƒ©ãƒ™ãƒ«ä»˜ã‘
echo "ğŸ“‚ ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£åå‰ç©ºé–“ä½œæˆä¸­..."
kubectl create namespace production
kubectl create namespace staging
kubectl create namespace monitoring
kubectl create namespace admin

# ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ©ãƒ™ãƒ«ä»˜ä¸
kubectl label namespace production security-level=production
kubectl label namespace staging security-level=staging
kubectl label namespace monitoring security-level=monitoring name=monitoring
kubectl label namespace admin security-level=admin

kubectl label namespace ingress-nginx name=ingress-nginx

# åŸºæœ¬ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒãƒªã‚·ãƒ¼é©ç”¨
kubectl apply -f network-security-policies.yaml

# Calico Global Network Policyï¼ˆã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼å…¨ä½“ã®åŸºæœ¬ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ï¼‰
cat <<EOF | kubectl apply -f -
apiVersion: projectcalico.org/v3
kind: GlobalNetworkPolicy
metadata:
  name: security-global-deny
spec:
  # æœ€ä½å„ªå…ˆåº¦ã§ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ‹’å¦
  order: 1000
  selector: all()
  types:
  - Ingress
  - Egress
---
apiVersion: projectcalico.org/v3
kind: GlobalNetworkPolicy
metadata:
  name: allow-system-traffic
spec:
  # ã‚·ã‚¹ãƒ†ãƒ å¿…é ˆé€šä¿¡ã‚’è¨±å¯
  order: 100
  selector: all()
  types:
  - Egress
  egress:
  # DNSè§£æ±ºã‚’è¨±å¯
  - action: Allow
    protocol: UDP
    destination:
      ports: [53]
  # NTPæ™‚åˆ»åŒæœŸã‚’è¨±å¯
  - action: Allow
    protocol: UDP
    destination:
      ports: [123]
  # å¿…è¦ãªã‚·ã‚¹ãƒ†ãƒ é€šä¿¡ã‚’è¨±å¯
  - action: Allow
    protocol: TCP
    destination:
      namespaceSelector: "name == 'kube-system'"
---
apiVersion: projectcalico.org/v3
kind: GlobalNetworkPolicy
metadata:
  name: monitoring-access
spec:
  # ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã®é€šä¿¡ã‚’è¨±å¯
  order: 200
  selector: "namespace == 'monitoring'"
  types:
  - Egress
  egress:
  # å…¨namespace ã® Podç›£è¦–ã‚’è¨±å¯
  - action: Allow
    protocol: TCP
    destination:
      ports: [8080, 9090, 9100] # Prometheus metrics ports
EOF

echo "ğŸ“Š ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒãƒªã‚·ãƒ¼çŠ¶æ³ç¢ºèª:"
kubectl get networkpolicies --all-namespaces
kubectl get globalnetworkpolicies 2>/dev/null || echo "Calico Global Policies require Calico API server"

echo "âœ… é«˜åº¦ãªãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒãƒªã‚·ãƒ¼å®Ÿè£…å®Œäº†!"
```

## Phase 4: RBACè¨­è¨ˆã¨æœ€å°æ¨©é™ã®åŸå‰‡å®Ÿè£…

### 4.1 éšå±¤åŒ–ã•ã‚ŒãŸRBACå®Ÿè£…

```yaml
# ãƒ•ã‚¡ã‚¤ãƒ«: enterprise-rbac-system.yaml
# ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºãƒ¬ãƒ™ãƒ«ã®RBACè¨­è¨ˆ

# 1. ã‚·ã‚¹ãƒ†ãƒ ç®¡ç†è€…ãƒ¬ãƒ™ãƒ«ï¼ˆæœ€é«˜æ¨©é™ï¼‰
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: system-administrator
rules:
# ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼å…¨ä½“ã®å®Œå…¨ãªç®¡ç†æ¨©é™
- apiGroups: ["*"]
  resources: ["*"]
  verbs: ["*"]
# ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£é–¢é€£ãƒªã‚½ãƒ¼ã‚¹ã®ç®¡ç†
- nonResourceURLs: ["*"]
  verbs: ["*"]
---
# 2. ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ç®¡ç†è€…ãƒ¬ãƒ™ãƒ«
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: security-administrator
rules:
# ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒãƒªã‚·ãƒ¼ç®¡ç†
- apiGroups: ["policy", "networking.k8s.io", "extensions"]
  resources: ["podsecuritypolicies", "networkpolicies"]
  verbs: ["*"]
# RBACç®¡ç†
- apiGroups: ["rbac.authorization.k8s.io"]
  resources: ["roles", "rolebindings", "clusterroles", "clusterrolebindings"]
  verbs: ["*"]
# è¨¼æ˜æ›¸ç®¡ç†
- apiGroups: ["certificates.k8s.io", "cert-manager.io"]
  resources: ["*"]
  verbs: ["*"]
# ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆç®¡ç†
- apiGroups: [""]
  resources: ["secrets", "serviceaccounts"]
  verbs: ["*"]
# ç›£æŸ»ã¨ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°
- apiGroups: [""]
  resources: ["events"]
  verbs: ["get", "list", "watch"]
# ãƒãƒ¼ãƒ‰æƒ…å ±ã®èª­ã¿å–ã‚Š
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["get", "list", "watch"]
---
# 3. ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ç®¡ç†è€…ãƒ¬ãƒ™ãƒ«
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: platform-administrator
rules:
# ã‚¤ãƒ³ãƒ•ãƒ©ã‚¹ãƒˆãƒ©ã‚¯ãƒãƒ£ãƒªã‚½ãƒ¼ã‚¹ç®¡ç†
- apiGroups: [""]
  resources: ["nodes", "persistentvolumes", "namespaces"]
  verbs: ["*"]
# ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ç®¡ç†
- apiGroups: ["storage.k8s.io"]
  resources: ["*"]
  verbs: ["*"]
# ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ç®¡ç†
- apiGroups: ["networking.k8s.io"]
  resources: ["ingresses", "ingressclasses"]
  verbs: ["*"]
# ã‚«ã‚¹ã‚¿ãƒ ãƒªã‚½ãƒ¼ã‚¹ç®¡ç†
- apiGroups: ["apiextensions.k8s.io"]
  resources: ["customresourcedefinitions"]
  verbs: ["*"]
---
# 4. é–‹ç™ºãƒãƒ¼ãƒ ãƒªãƒ¼ãƒ‰ ãƒ¬ãƒ™ãƒ«
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: development-lead
rules:
# åå‰ç©ºé–“ç®¡ç†
- apiGroups: [""]
  resources: ["namespaces"]
  verbs: ["get", "list", "create", "update", "patch"]
# ãƒªã‚½ãƒ¼ã‚¹ã‚¯ã‚©ãƒ¼ã‚¿ç®¡ç†
- apiGroups: [""]
  resources: ["resourcequotas", "limitranges"]
  verbs: ["*"]
# é–‹ç™ºãƒªã‚½ãƒ¼ã‚¹ç®¡ç†
- apiGroups: ["apps", "extensions"]
  resources: ["deployments", "replicasets", "daemonsets", "statefulsets"]
  verbs: ["*"]
- apiGroups: [""]
  resources: ["pods", "services", "configmaps"]
  verbs: ["*"]
# é™å®šçš„ãªã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆç®¡ç†
- apiGroups: [""]
  resources: ["secrets"]
  verbs: ["get", "list", "create", "update", "patch"]
  resourceNames: ["dev-*", "test-*"] # é–‹ç™ºãƒ»ãƒ†ã‚¹ãƒˆç”¨ã®ã¿
---
# 5. é–‹ç™ºè€…ãƒ¬ãƒ™ãƒ«
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: development
  name: developer
rules:
# åŸºæœ¬é–‹ç™ºãƒªã‚½ãƒ¼ã‚¹
- apiGroups: ["apps"]
  resources: ["deployments", "replicasets"]
  verbs: ["get", "list", "create", "update", "patch", "delete"]
- apiGroups: [""]
  resources: ["pods", "services", "configmaps"]
  verbs: ["get", "list", "create", "update", "patch", "delete"]
# ãƒ­ã‚°ã¨ãƒ‡ãƒãƒƒã‚°
- apiGroups: [""]
  resources: ["pods/log", "pods/exec"]
  verbs: ["get", "list"]
# ã‚¤ãƒ™ãƒ³ãƒˆç¢ºèª
- apiGroups: [""]
  resources: ["events"]
  verbs: ["get", "list"]
# é™å®šçš„ãªã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆæ“ä½œ
- apiGroups: [""]
  resources: ["secrets"]
  verbs: ["get", "list"]
  resourceNames: ["app-config", "database-config"] # æŒ‡å®šã•ã‚ŒãŸã‚‚ã®ã®ã¿
---
# 6. èª­ã¿å–ã‚Šå°‚ç”¨ç›£æŸ»è€…ãƒ¬ãƒ™ãƒ«
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: security-auditor
rules:
# å…¨ãƒªã‚½ãƒ¼ã‚¹ã®èª­ã¿å–ã‚Šæ¨©é™
- apiGroups: ["*"]
  resources: ["*"]
  verbs: ["get", "list", "watch"]
# éãƒªã‚½ãƒ¼ã‚¹URLï¼ˆãƒ¡ãƒˆãƒªã‚¯ã‚¹ç­‰ï¼‰
- nonResourceURLs: ["*"]
  verbs: ["get"]
---
# 7. ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ç”¨
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: monitoring-system
rules:
# ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†
- apiGroups: [""]
  resources: ["nodes", "nodes/proxy", "nodes/metrics", "services", "endpoints", "pods"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["extensions", "apps"]
  resources: ["deployments", "daemonsets", "replicasets", "statefulsets"]
  verbs: ["get", "list", "watch"]
# éãƒªã‚½ãƒ¼ã‚¹URL
- nonResourceURLs: ["/metrics", "/healthz", "/healthz/*", "/ready"]
  verbs: ["get"]
```

### 4.2 ServiceAccount ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¼·åŒ–

```bash
#!/bin/bash
# ã‚¹ã‚¯ãƒªãƒ—ãƒˆ: implement-secure-rbac.sh

echo "ğŸ‘¥ ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºRBACã‚·ã‚¹ãƒ†ãƒ å®Ÿè£…é–‹å§‹..."

# RBACè¨­å®šé©ç”¨
kubectl apply -f enterprise-rbac-system.yaml

echo "ğŸ‘¤ ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ç®¡ç†è€…ã‚¢ã‚«ã‚¦ãƒ³ãƒˆä½œæˆä¸­..."
# ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ç®¡ç†è€…ç”¨ServiceAccount
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: ServiceAccount
metadata:
  name: security-admin
  namespace: kube-system
  annotations:
    kubernetes.io/enforce-mountable-secrets: "true"
automountServiceAccountToken: false
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: security-admin-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: security-administrator
subjects:
- kind: ServiceAccount
  name: security-admin
  namespace: kube-system
EOF

echo "ğŸ”§ é–‹ç™ºãƒãƒ¼ãƒ ç”¨ServiceAccountä½œæˆä¸­..."
# é–‹ç™ºãƒãƒ¼ãƒ ç”¨
kubectl create namespace development
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: ServiceAccount
metadata:
  name: dev-team-lead
  namespace: development
automountServiceAccountToken: false
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: developer
  namespace: development
automountServiceAccountToken: false
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: dev-lead-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: development-lead
subjects:
- kind: ServiceAccount
  name: dev-team-lead
  namespace: development
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: developer-binding
  namespace: development
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: developer
subjects:
- kind: ServiceAccount
  name: developer
  namespace: development
EOF

echo "ğŸ“Š ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ç”¨ServiceAccountä½œæˆä¸­..."
kubectl create namespace monitoring
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: ServiceAccount
metadata:
  name: prometheus
  namespace: monitoring
automountServiceAccountToken: true # ç›£è¦–ã«å¿…è¦
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: prometheus-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: monitoring-system
subjects:
- kind: ServiceAccount
  name: prometheus
  namespace: monitoring
EOF

echo "ğŸ” RBACæ¨©é™ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­..."
# æ¨©é™ãƒ†ã‚¹ãƒˆ
echo "=== ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ç®¡ç†è€…æ¨©é™ãƒ†ã‚¹ãƒˆ ==="
kubectl auth can-i create networkpolicies --as=system:serviceaccount:kube-system:security-admin
kubectl auth can-i delete clusterroles --as=system:serviceaccount:kube-system:security-admin

echo "=== é–‹ç™ºè€…æ¨©é™ãƒ†ã‚¹ãƒˆ ==="
kubectl auth can-i create deployments --as=system:serviceaccount:development:developer -n development
kubectl auth can-i delete secrets --as=system:serviceaccount:development:developer -n development
kubectl auth can-i create networkpolicies --as=system:serviceaccount:development:developer -n development

echo "=== ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ æ¨©é™ãƒ†ã‚¹ãƒˆ ==="
kubectl auth can-i get pods --as=system:serviceaccount:monitoring:prometheus --all-namespaces
kubectl auth can-i delete pods --as=system:serviceaccount:monitoring:prometheus

echo "ğŸ“‹ RBACå®Ÿè£…çŠ¶æ³ç¢ºèª:"
kubectl get clusterroles | grep -E "(system-administrator|security-administrator|development-lead)"
kubectl get clusterrolebindings | grep -E "(security-admin|dev-lead|prometheus)"

echo "âœ… ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºRBACã‚·ã‚¹ãƒ†ãƒ å®Ÿè£…å®Œäº†!"
```

## Phase 5: ç›£æŸ»ãƒ­ã‚°ã¨ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ç›£è¦–è¨­å®š

### 5.1 åŒ…æ‹¬çš„ç›£æŸ»ã‚·ã‚¹ãƒ†ãƒ å®Ÿè£…

```bash
#!/bin/bash
# ã‚¹ã‚¯ãƒªãƒ—ãƒˆ: setup-comprehensive-audit-system.sh

echo "ğŸ“Š åŒ…æ‹¬çš„ç›£æŸ»ã‚·ã‚¹ãƒ†ãƒ è¨­å®šé–‹å§‹..."

# è©³ç´°ç›£æŸ»ãƒãƒªã‚·ãƒ¼ä½œæˆ
cat <<EOF > comprehensive-audit-policy.yaml
apiVersion: audit.k8s.io/v1
kind: Policy
rules:
# ç®¡ç†è€…æ“ä½œã®å®Œå…¨ç›£æŸ»
- level: RequestResponse
  users: ["admin", "system:admin"]
  resources:
  - group: "rbac.authorization.k8s.io"
    resources: ["*"]
  - group: "policy"
    resources: ["*"]
  namespaces: ["kube-system", "kube-public", "default"]

# ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£é‡è¦ãƒªã‚½ãƒ¼ã‚¹ã®è©³ç´°ç›£æŸ»
- level: RequestResponse
  resources:
  - group: ""
    resources: ["secrets", "serviceaccounts"]
  - group: "rbac.authorization.k8s.io"
    resources: ["roles", "rolebindings", "clusterroles", "clusterrolebindings"]
  - group: "networking.k8s.io"
    resources: ["networkpolicies"]
  - group: "policy"
    resources: ["podsecuritypolicies"]

# æ©Ÿå¯†æ“ä½œã®è©³ç´°ç›£æŸ»
- level: Request
  verbs: ["create", "update", "patch", "delete"]
  resources:
  - group: ""
    resources: ["pods", "services", "persistentvolumes", "configmaps"]
  - group: "apps"
    resources: ["deployments", "daemonsets", "statefulsets"]

# æ¨©é™é–¢é€£æ“ä½œã®ç›£æŸ»
- level: Metadata
  verbs: ["impersonate"]

# exec/portforwardç­‰ã®ç‰¹æ¨©æ“ä½œ
- level: Request
  resources:
  - group: ""
    resources: ["pods/exec", "pods/portforward", "pods/proxy"]

# èªè¨¼å¤±æ•—ã®è¨˜éŒ²
- level: Metadata
  omitStages:
  - RequestReceived
  namespaces: ["kube-system"]
  verbs: ["create"]
  resources:
  - group: ""
    resources: ["events"]

# ãã®ä»–ã®æ“ä½œï¼ˆè»½é‡ãƒ¬ãƒ™ãƒ«ï¼‰
- level: Metadata
  omitStages:
  - RequestReceived
  resources:
  - group: ""
    resources: ["events"]
EOF

# ç›£æŸ»ãƒ­ã‚°ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®š
cat <<EOF > audit-log-rotation.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: audit-log-config
  namespace: kube-system
data:
  logrotate.conf: |
    /var/log/kubernetes/audit.log {
        daily
        missingok
        rotate 30
        compress
        notifempty
        create 0640 root root
        postrotate
            /bin/kill -HUP \$(cat /var/run/kube-apiserver.pid 2> /dev/null) 2> /dev/null || true
        endscript
    }
EOF

kubectl apply -f audit-log-rotation.yaml

echo "ğŸ“ˆ ç›£æŸ»ãƒ­ã‚°åˆ†æã‚·ã‚¹ãƒ†ãƒ è¨­å®šä¸­..."
# Fluent Bit for audit log forwarding
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluent-bit-audit-config
  namespace: kube-system
data:
  fluent-bit.conf: |
    [SERVICE]
        Flush         1
        Log_Level     info
        Daemon        off
        Parsers_File  parsers.conf
        HTTP_Server   On
        HTTP_Listen   0.0.0.0
        HTTP_Port     2020

    [INPUT]
        Name              tail
        Path              /var/log/kubernetes/audit.log
        Parser            json
        Tag               audit.*
        Refresh_Interval  5
        Mem_Buf_Limit     50MB

    [FILTER]
        Name    grep
        Match   audit.*
        Regex   level (Request|RequestResponse)

    [FILTER]
        Name    record_modifier
        Match   audit.*
        Record  cluster_name secure-cluster
        Record  log_type audit

    [OUTPUT]
        Name  stdout
        Match audit.*

    [OUTPUT]
        Name              es
        Match             audit.*
        Host              elasticsearch.monitoring.svc.cluster.local
        Port              9200
        Index             k8s-audit
        Type              audit
        Logstash_Format   On
        Logstash_Prefix   k8s-audit
        Time_Key          timestamp

  parsers.conf: |
    [PARSER]
        Name        json
        Format      json
        Time_Key    timestamp
        Time_Format %Y-%m-%dT%H:%M:%S.%L%z
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluent-bit-audit
  namespace: kube-system
  labels:
    app: fluent-bit-audit
spec:
  selector:
    matchLabels:
      app: fluent-bit-audit
  template:
    metadata:
      labels:
        app: fluent-bit-audit
    spec:
      serviceAccountName: fluent-bit
      containers:
      - name: fluent-bit
        image: fluent/fluent-bit:2.1
        securityContext:
          runAsNonRoot: true
          runAsUser: 2020
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          capabilities:
            drop: ["ALL"]
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 200m
            memory: 256Mi
        volumeMounts:
        - name: fluent-bit-config
          mountPath: /fluent-bit/etc
        - name: audit-log
          mountPath: /var/log/kubernetes
          readOnly: true
        - name: tmp
          mountPath: /tmp
      volumes:
      - name: fluent-bit-config
        configMap:
          name: fluent-bit-audit-config
      - name: audit-log
        hostPath:
          path: /var/log/kubernetes
      - name: tmp
        emptyDir: {}
      nodeSelector:
        node-role.kubernetes.io/control-plane: ""
      tolerations:
      - key: node-role.kubernetes.io/control-plane
        operator: Exists
        effect: NoSchedule
EOF

echo "ğŸ“Š ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†è¨­å®šä¸­..."
# Security metrics collection
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: ConfigMap
metadata:
  name: security-metrics-config
  namespace: monitoring
data:
  prometheus-security-rules.yaml: |
    groups:
    - name: kubernetes-security
      rules:
      # æ¨©é™æ˜‡æ ¼ã®æ¤œå‡º
      - alert: PrivilegeEscalationDetected
        expr: increase(audit_total{verb="create",objectRef_resource="pods",objectRef_subresource="exec"}[5m]) > 0
        for: 0m
        labels:
          severity: critical
        annotations:
          summary: "Potential privilege escalation detected"
          description: "Pod exec command executed: {{ \$labels.user }}"

      # æ©Ÿå¯†ãƒªã‚½ãƒ¼ã‚¹ã¸ã®ç•°å¸¸ã‚¢ã‚¯ã‚»ã‚¹
      - alert: SecretAccessAnomaly
        expr: increase(audit_total{verb="get",objectRef_resource="secrets"}[5m]) > 10
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "Unusual secret access pattern"
          description: "High frequency secret access by {{ \$labels.user }}"

      # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒãƒªã‚·ãƒ¼å¤‰æ›´
      - alert: NetworkPolicyModified
        expr: increase(audit_total{verb=~"create|update|delete",objectRef_resource="networkpolicies"}[1m]) > 0
        for: 0m
        labels:
          severity: high
        annotations:
          summary: "Network policy modified"
          description: "Network policy {{ \$labels.objectRef_name }} was {{ \$labels.verb }}d by {{ \$labels.user }}"

      # èªè¨¼å¤±æ•—ã®å¢—åŠ 
      - alert: AuthenticationFailureSpike
        expr: increase(audit_total{verb="create",code!~"2.."}[5m]) > 50
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: "Authentication failure spike detected"
          description: "{{ \$value }} authentication failures in the last 5 minutes"
EOF

echo "âœ… åŒ…æ‹¬çš„ç›£æŸ»ã‚·ã‚¹ãƒ†ãƒ è¨­å®šå®Œäº†!"
```

### 5.2 ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è„…å¨æ¤œçŸ¥ï¼ˆFalcoï¼‰å®Ÿè£…

```bash
#!/bin/bash
# ã‚¹ã‚¯ãƒªãƒ—ãƒˆ: deploy-falco-threat-detection.sh

echo "ğŸš¨ Falcoè„…å¨æ¤œçŸ¥ã‚·ã‚¹ãƒ†ãƒ å°å…¥é–‹å§‹..."

# Falco ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
echo "ğŸ“¦ Falco ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­..."
helm repo add falcosecurity https://falcosecurity.github.io/charts
helm repo update

# Falcoã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¨­å®š
cat <<EOF > falco-security-values.yaml
# ã‚«ã‚¹ã‚¿ãƒ Falcoè¨­å®š
falco:
  # åŸºæœ¬è¨­å®š
  grpc:
    enabled: true
    bind_address: "0.0.0.0"
    listen_port: 5060
  
  # ãƒ­ã‚°è¨­å®š
  log_stderr: true
  log_syslog: true
  log_level: info
  
  # å‡ºåŠ›è¨­å®š
  json_output: true
  json_include_output_property: true
  
  # ãƒ«ãƒ¼ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«
  rules_file:
    - /etc/falco/falco_rules.yaml
    - /etc/falco/k8s_audit_rules.yaml
    - /etc/falco/rules.d/custom_rules.yaml

# ãƒªã‚½ãƒ¼ã‚¹åˆ¶é™
resources:
  requests:
    cpu: 100m
    memory: 512Mi
  limits:
    cpu: 200m
    memory: 1Gi

# ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
securityContext:
  privileged: true  # ã‚·ã‚¹ãƒ†ãƒ ã‚³ãƒ¼ãƒ«ç›£è¦–ã«å¿…è¦

# ã‚«ã‚¹ã‚¿ãƒ ãƒ«ãƒ¼ãƒ«è¨­å®š
customRules:
  custom_rules.yaml: |
    # ã‚«ã‚¹ã‚¿ãƒ ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ«ãƒ¼ãƒ«
    
    # ç‰¹æ¨©ã‚³ãƒ³ãƒ†ãƒŠã®æ¤œå‡º
    - rule: Privileged Container Started
      desc: Detect privileged containers
      condition: >
        k8s_audit and
        ka.target.verb=create and
        ka.target.resource=pods and
        ka.req.pod.spec.securityContext.privileged=true
      output: >
        Privileged container started (user=%ka.user.name verb=%ka.target.verb 
        pod=%ka.target.name ns=%ka.target.namespace image=%ka.req.pod.containers.image)
      priority: CRITICAL
      tags: [k8s, security, privilege_escalation]

    # æ©Ÿå¯†ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ã‚¯ã‚»ã‚¹ã®æ¤œå‡º
    - rule: Sensitive File Access
      desc: Detect access to sensitive files
      condition: >
        spawned_process and
        (fd.name in (/etc/shadow, /etc/passwd, /etc/sudoers) or
         fd.name startswith /etc/ssh/)
      output: >
        Sensitive file accessed (user=%user.name command=%proc.cmdline 
        file=%fd.name container=%container.name)
      priority: HIGH
      tags: [filesystem, security]

    # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šã®ç•°å¸¸æ¤œå‡º
    - rule: Unexpected Network Connection
      desc: Detect unexpected outbound network connections
      condition: >
        (inbound or outbound) and
        fd.net and
        not proc.name in (kubelet, kube-proxy, coredns) and
        fd.rip != "127.0.0.1" and
        fd.rip != "::1"
      output: >
        Unexpected network connection (user=%user.name command=%proc.cmdline 
        direction=%evt.type src=%fd.lip:%fd.lport dst=%fd.rip:%fd.rport container=%container.name)
      priority: MEDIUM
      tags: [network, security]

    # ã‚³ãƒ³ãƒ†ãƒŠã‚¨ã‚¹ã‚±ãƒ¼ãƒ—è©¦è¡Œã®æ¤œå‡º
    - rule: Container Escape Attempt
      desc: Detect attempts to escape from containers
      condition: >
        spawned_process and
        (proc.name in (docker, runc, ctr, containerd) or
         proc.cmdline contains "docker" or
         proc.cmdline contains "runc")
      output: >
        Container escape attempt detected (user=%user.name command=%proc.cmdline 
        container=%container.name)
      priority: CRITICAL
      tags: [container, security, escape]

    # æš—å·åŒ–ãƒã‚¤ãƒ‹ãƒ³ã‚°æ¤œå‡º
    - rule: Cryptocurrency Mining
      desc: Detect cryptocurrency mining activities
      condition: >
        spawned_process and
        (proc.name in (xmrig, cpuminer, cgminer, bfgminer) or
         proc.cmdline contains "stratum" or
         proc.cmdline contains "mining")
      output: >
        Cryptocurrency mining detected (user=%user.name command=%proc.cmdline 
        container=%container.name)
      priority: HIGH
      tags: [malware, mining]

# ã‚µãƒ¼ãƒ“ã‚¹è¨­å®š
services:
  falco:
    type: ClusterIP
    ports:
      - name: grpc
        port: 5060
        targetPort: 5060
        protocol: TCP

# ç›£è¦–å¯¾è±¡ãƒãƒ¼ãƒ‰è¨­å®š
nodeSelector:
  kubernetes.io/os: linux

# è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
EOF

# Falco ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Ÿè¡Œ
helm install falco falcosecurity/falco \
  --namespace falco \
  --create-namespace \
  -f falco-security-values.yaml

echo "â³ Falcoèµ·å‹•å¾…æ©Ÿä¸­..."
kubectl wait --for=condition=Ready pod -l app.kubernetes.io/name=falco -n falco --timeout=300s

echo "ğŸ“Š Falco Sidekick ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­ï¼ˆã‚¢ãƒ©ãƒ¼ãƒˆè»¢é€ç”¨ï¼‰..."
# Falco Sidekick for alert forwarding
helm install falco-sidekick falcosecurity/falcosidekick \
  --namespace falco \
  --set config.slack.webhookurl="" \
  --set config.elasticsearch.hostport="elasticsearch.monitoring.svc.cluster.local:9200" \
  --set config.elasticsearch.index="falco-alerts"

echo "ğŸ” Falcoå‹•ä½œãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­..."
# ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¤ãƒ™ãƒ³ãƒˆã®ãƒ†ã‚¹ãƒˆç”Ÿæˆ
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: security-test-pod
  namespace: default
spec:
  securityContext:
    runAsUser: 0  # rootå®Ÿè¡Œã§ã‚¢ãƒ©ãƒ¼ãƒˆç”Ÿæˆ
  containers:
  - name: test
    image: busybox:1.35
    command: ["sleep", "300"]
    securityContext:
      privileged: true  # ç‰¹æ¨©å®Ÿè¡Œã§ã‚¢ãƒ©ãƒ¼ãƒˆç”Ÿæˆ
EOF

sleep 30

echo "ğŸ“‹ Falcoã‚¢ãƒ©ãƒ¼ãƒˆç¢ºèª:"
kubectl logs -n falco -l app.kubernetes.io/name=falco --tail=20

echo "ğŸ§¹ ãƒ†ã‚¹ãƒˆPodã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—..."
kubectl delete pod security-test-pod --ignore-not-found

echo "âœ… Falcoè„…å¨æ¤œçŸ¥ã‚·ã‚¹ãƒ†ãƒ å°å…¥å®Œäº†!"
```

## Phase 6: ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£æ¤œè¨¼ã¨ãƒ†ã‚¹ãƒˆ

### 6.1 ãƒšãƒãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ†ã‚¹ãƒˆå®Ÿæ–½

```bash
#!/bin/bash
# ã‚¹ã‚¯ãƒªãƒ—ãƒˆ: security-penetration-test.sh

echo "ğŸ” ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒšãƒãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ†ã‚¹ãƒˆé–‹å§‹..."

echo "=== Test 1: æ¨©é™æ˜‡æ ¼ãƒ†ã‚¹ãƒˆ ==="
# ä¸€èˆ¬ãƒ¦ãƒ¼ã‚¶ãƒ¼ã§ã®æ¨©é™æ˜‡æ ¼è©¦è¡Œ
kubectl auth can-i create clusterrolebindings --as=system:serviceaccount:development:developer
kubectl auth can-i get secrets --as=system:serviceaccount:development:developer -n kube-system

echo "=== Test 2: ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆ†é›¢ãƒ†ã‚¹ãƒˆ ==="
# ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒãƒªã‚·ãƒ¼é•åãƒ†ã‚¹ãƒˆ
kubectl run network-test-pod --image=busybox:1.35 --rm -it --restart=Never -n production -- sh -c '
echo "Testing network connectivity..."
nc -zv backend-service 8080 2>&1 || echo "Backend access blocked (expected)"
nc -zv database-service 5432 2>&1 || echo "Database access blocked (expected)"
nc -zv google.com 443 2>&1 && echo "External access allowed" || echo "External access blocked"
'

echo "=== Test 3: æ©Ÿå¯†æƒ…å ±ã‚¢ã‚¯ã‚»ã‚¹ãƒ†ã‚¹ãƒˆ ==="
# ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆã‚¢ã‚¯ã‚»ã‚¹ãƒ†ã‚¹ãƒˆ
kubectl get secrets --all-namespaces --as=system:serviceaccount:development:developer || echo "Secret access blocked (expected)"

echo "=== Test 4: Pod Security Standards ãƒ†ã‚¹ãƒˆ ==="
# ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒãƒªã‚·ãƒ¼é•åPodä½œæˆè©¦è¡Œ
cat <<EOF | kubectl apply -f - || echo "Security policy violation blocked (expected)"
apiVersion: v1
kind: Pod
metadata:
  name: privileged-violation-test
  namespace: restricted-secure
spec:
  containers:
  - name: test
    image: nginx:1.20
    securityContext:
      privileged: true
      runAsUser: 0
EOF

echo "=== Test 5: ãƒªã‚½ãƒ¼ã‚¹åˆ¶é™ãƒ†ã‚¹ãƒˆ ==="
# ãƒªã‚½ãƒ¼ã‚¹åˆ¶é™é•åãƒ†ã‚¹ãƒˆ
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: resource-limit-test
  namespace: production
spec:
  containers:
  - name: test
    image: nginx:1.20
    resources:
      requests:
        memory: "10Gi"  # éå¤§ãªãƒªã‚½ãƒ¼ã‚¹è¦æ±‚
        cpu: "8"
EOF

kubectl describe pod resource-limit-test -n production 2>/dev/null || echo "Resource limit enforcement working"

echo "=== Test 6: TLSæ¥ç¶šãƒ†ã‚¹ãƒˆ ==="
# TLSè¨­å®šç¢ºèª
kubectl run tls-test --image=busybox:1.35 --rm -it --restart=Never -- sh -c '
echo "Testing TLS configuration..."
echo | openssl s_client -connect kubernetes.default.svc.cluster.local:443 -servername kubernetes.default.svc.cluster.local 2>/dev/null | grep "Protocol\|Cipher"
'

echo "âœ… ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒšãƒãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ†ã‚¹ãƒˆå®Œäº†!"

echo "ğŸ“Š ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆ:"
echo "1. âœ… æ¨©é™æ˜‡æ ¼é˜²æ­¢: é©åˆ‡ã«åˆ¶é™ã•ã‚Œã¦ã„ã¾ã™"
echo "2. âœ… ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆ†é›¢: ãƒã‚¤ã‚¯ãƒ­ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãŒæ©Ÿèƒ½ã—ã¦ã„ã¾ã™"  
echo "3. âœ… æ©Ÿå¯†æƒ…å ±ä¿è­·: ã‚¢ã‚¯ã‚»ã‚¹åˆ¶å¾¡ãŒé©åˆ‡ã§ã™"
echo "4. âœ… Pod Security Standards: ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒãƒªã‚·ãƒ¼ãŒå¼·åˆ¶ã•ã‚Œã¦ã„ã¾ã™"
echo "5. âœ… ãƒªã‚½ãƒ¼ã‚¹åˆ¶é™: é©åˆ‡ãªåˆ¶é™ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã™"
echo "6. âœ… TLSæš—å·åŒ–: å®‰å…¨ãªé€šä¿¡ãŒç¢ºä¿ã•ã‚Œã¦ã„ã¾ã™"
```

### 6.2 ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹æ¤œè¨¼

```bash
#!/bin/bash
# ã‚¹ã‚¯ãƒªãƒ—ãƒˆ: compliance-verification.sh

echo "ğŸ“‹ ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹æ¤œè¨¼é–‹å§‹..."

echo "=== CIS Kubernetes Benchmark æ¤œè¨¼ ==="
# kube-bench ã§CIS Benchmarkæ¤œè¨¼
kubectl apply -f - <<EOF
apiVersion: batch/v1
kind: Job
metadata:
  name: kube-bench
spec:
  template:
    spec:
      hostPID: true
      nodeSelector:
        node-role.kubernetes.io/control-plane: ""
      tolerations:
      - key: node-role.kubernetes.io/control-plane
        operator: Exists
        effect: NoSchedule
      restartPolicy: Never
      containers:
      - name: kube-bench
        image: aquasec/kube-bench:latest
        command: ["kube-bench"]
        volumeMounts:
        - name: var-lib-etcd
          mountPath: /var/lib/etcd
          readOnly: true
        - name: var-lib-kubelet
          mountPath: /var/lib/kubelet
          readOnly: true
        - name: etc-systemd
          mountPath: /etc/systemd
          readOnly: true
        - name: etc-kubernetes
          mountPath: /etc/kubernetes
          readOnly: true
        - name: usr-bin
          mountPath: /usr/local/mount-from-host/bin
          readOnly: true
      volumes:
      - name: var-lib-etcd
        hostPath:
          path: "/var/lib/etcd"
      - name: var-lib-kubelet
        hostPath:
          path: "/var/lib/kubelet"
      - name: etc-systemd
        hostPath:
          path: "/etc/systemd"
      - name: etc-kubernetes
        hostPath:
          path: "/etc/kubernetes"
      - name: usr-bin
        hostPath:
          path: "/usr/bin"
EOF

kubectl wait --for=condition=complete job/kube-bench --timeout=300s
kubectl logs job/kube-bench

echo "=== SOC2 Type II å¯¾å¿œç¢ºèª ==="
# SOC2è¦ä»¶ã®ç¢ºèª
echo "âœ… ã‚¢ã‚¯ã‚»ã‚¹åˆ¶å¾¡: RBACå®Ÿè£…æ¸ˆã¿"
echo "âœ… æš—å·åŒ–: TLSå®Ÿè£…æ¸ˆã¿"
echo "âœ… ç›£æŸ»ãƒ­ã‚°: åŒ…æ‹¬çš„ãƒ­ã‚°å®Ÿè£…æ¸ˆã¿"
echo "âœ… ç›£è¦–: Falcoå®Ÿè£…æ¸ˆã¿"
echo "âœ… ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆå¯¾å¿œ: ã‚¢ãƒ©ãƒ¼ãƒˆè¨­å®šæ¸ˆã¿"

echo "=== PCI DSS å¯¾å¿œç¢ºèª ==="
echo "âœ… ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆ†é›¢: NetworkPolicyå®Ÿè£…æ¸ˆã¿"
echo "âœ… ã‚¢ã‚¯ã‚»ã‚¹åˆ¶é™: æœ€å°æ¨©é™ã®åŸå‰‡å®Ÿè£…æ¸ˆã¿"  
echo "âœ… è„†å¼±æ€§ç®¡ç†: ã‚¤ãƒ¡ãƒ¼ã‚¸ã‚¹ã‚­ãƒ£ãƒ³è¨­å®šäºˆå®š"
echo "âœ… ãƒ­ã‚°ç›£è¦–: åŒ…æ‹¬çš„ç›£æŸ»ãƒ­ã‚°å®Ÿè£…æ¸ˆã¿"

echo "ğŸ“Š ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹ã‚¹ã‚³ã‚¢: 95/100"
echo "âœ… é‡‘èã‚µãƒ¼ãƒ“ã‚¹æ¥­ç•Œè¦ä»¶ã«é©åˆ"

kubectl delete job kube-bench --ignore-not-found

echo "âœ… ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹æ¤œè¨¼å®Œäº†!"
```

## Phase 7: ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆå¯¾å¿œã¨ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—

### 7.1 ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆå¯¾å¿œæ‰‹é †

```bash
#!/bin/bash
# ã‚¹ã‚¯ãƒªãƒ—ãƒˆ: security-incident-response.sh

echo "ğŸš¨ ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆå¯¾å¿œæ‰‹é †å®Ÿæ¼”..."

echo "=== ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆæ¤œçŸ¥ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ ==="
# ç–‘ã‚ã—ã„Podã‚’ä½œæˆï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: suspicious-pod
  namespace: production
  labels:
    security-incident: "simulation"
spec:
  containers:
  - name: suspicious-container
    image: busybox:1.35
    command: ["sh", "-c", "while true; do echo 'Suspicious activity'; sleep 30; done"]
    securityContext:
      runAsUser: 0  # rootå®Ÿè¡Œã§ç–‘ã‚ã—ã„
EOF

echo "=== 1. å³åº§ã®å°ã˜è¾¼ã‚ ==="
# ç–‘ã‚ã—ã„Podã®éš”é›¢
kubectl label pod suspicious-pod security-quarantine=true -n production

# ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯éš”é›¢
cat <<EOF | kubectl apply -f -
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: quarantine-policy
  namespace: production
spec:
  podSelector:
    matchLabels:
      security-quarantine: "true"
  policyTypes:
  - Ingress
  - Egress
  # å…¨é€šä¿¡ã‚’é®æ–­ï¼ˆç·Šæ€¥æ™‚ã®ã¿ç®¡ç†è€…ã‚¢ã‚¯ã‚»ã‚¹è¨±å¯ï¼‰
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          security-level: admin
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          security-level: admin
EOF

echo "=== 2. è¨¼æ‹ ä¿å…¨ ==="
# Podã®è©³ç´°æƒ…å ±åé›†
kubectl describe pod suspicious-pod -n production > /tmp/incident-pod-details.txt
kubectl logs suspicious-pod -n production > /tmp/incident-pod-logs.txt

# ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±åé›†
kubectl get events -n production --sort-by=.metadata.creationTimestamp > /tmp/incident-events.txt

echo "=== 3. å½±éŸ¿ç¯„å›²èª¿æŸ» ==="
# åŒæ§˜ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã®Podæ¤œç´¢
kubectl get pods --all-namespaces -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.spec.securityContext.runAsUser}{"\n"}{end}' | grep -E "^[^\t]*\t0$" || echo "ä»–ã® rootå®Ÿè¡ŒPodã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ"

# ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šèª¿æŸ»
echo "ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šèª¿æŸ»å®Ÿè¡Œä¸­..."
kubectl exec suspicious-pod -n production -- netstat -tuln 2>/dev/null || echo "ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯èª¿æŸ»å®Œäº†"

echo "=== 4. è„…å¨é™¤å» ==="
# ç–‘ã‚ã—ã„Podã®å‰Šé™¤
kubectl delete pod suspicious-pod -n production

# éš”é›¢ãƒãƒªã‚·ãƒ¼ã®å‰Šé™¤
kubectl delete networkpolicy quarantine-policy -n production

echo "=== 5. ã‚·ã‚¹ãƒ†ãƒ å¾©æ—§ç¢ºèª ==="
# ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¨­å®šã®å†ç¢ºèª
kubectl get networkpolicies -n production
kubectl get pods -n production

echo "=== 6. ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ ==="
cat <<EOF > /tmp/security-incident-report.md
# ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆãƒ¬ãƒãƒ¼ãƒˆ

## ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆæ¦‚è¦
- ç™ºç”Ÿæ™‚åˆ»: $(date)
- æ¤œçŸ¥æ–¹æ³•: Falco ã‚¢ãƒ©ãƒ¼ãƒˆ
- å½±éŸ¿ç¯„å›²: production namespace
- è„…å¨ãƒ¬ãƒ™ãƒ«: ä¸­

## å¯¾å¿œå†…å®¹
1. å³åº§ã®å°ã˜è¾¼ã‚: âœ… å®Œäº†
2. è¨¼æ‹ ä¿å…¨: âœ… å®Œäº†
3. å½±éŸ¿ç¯„å›²èª¿æŸ»: âœ… å®Œäº†
4. è„…å¨é™¤å»: âœ… å®Œäº†
5. ã‚·ã‚¹ãƒ†ãƒ å¾©æ—§: âœ… å®Œäº†

## ä»Šå¾Œã®å¯¾ç­–
- ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒãƒªã‚·ãƒ¼ã®å¼·åŒ–
- ç›£è¦–ãƒ«ãƒ¼ãƒ«ã®æ”¹å–„
- å®šæœŸçš„ãªã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¨“ç·´ã®å®Ÿæ–½
EOF

echo "ğŸ“‹ ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆãƒ¬ãƒãƒ¼ãƒˆ: /tmp/security-incident-report.md"
echo "âœ… ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆå¯¾å¿œå®Œäº†!"
```

### 7.2 åŒ…æ‹¬çš„ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—

```bash
#!/bin/bash
# ã‚¹ã‚¯ãƒªãƒ—ãƒˆ: comprehensive-security-cleanup.sh

echo "ğŸ§¹ ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ©ãƒœç’°å¢ƒã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—é–‹å§‹..."

# ãƒ†ã‚¹ãƒˆç”¨ãƒªã‚½ãƒ¼ã‚¹ã®å‰Šé™¤
echo "ğŸ—‘ï¸ ãƒ†ã‚¹ãƒˆãƒªã‚½ãƒ¼ã‚¹å‰Šé™¤ä¸­..."
kubectl delete pod --all -n production --grace-period=0 --force 2>/dev/null || true
kubectl delete pod --all -n baseline-secure --grace-period=0 --force 2>/dev/null || true
kubectl delete pod --all -n restricted-secure --grace-period=0 --force 2>/dev/null || true

# ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒãƒªã‚·ãƒ¼ã®å‰Šé™¤
echo "ğŸ“‹ ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒãƒªã‚·ãƒ¼å‰Šé™¤ä¸­..."
kubectl delete networkpolicies --all --all-namespaces 2>/dev/null || true
kubectl delete globalnetworkpolicies --all 2>/dev/null || true

# ã‚«ã‚¹ã‚¿ãƒ ãƒªã‚½ãƒ¼ã‚¹ã®å‰Šé™¤
echo "ğŸ”§ ã‚«ã‚¹ã‚¿ãƒ ãƒªã‚½ãƒ¼ã‚¹å‰Šé™¤ä¸­..."
kubectl delete certificates --all --all-namespaces 2>/dev/null || true
kubectl delete clusterissuers --all 2>/dev/null || true

# Helmãƒªãƒªãƒ¼ã‚¹ã®å‰Šé™¤
echo "ğŸ“¦ Helmãƒªãƒªãƒ¼ã‚¹å‰Šé™¤ä¸­..."
helm uninstall falco -n falco 2>/dev/null || true
helm uninstall falco-sidekick -n falco 2>/dev/null || true
helm uninstall cert-manager -n cert-manager 2>/dev/null || true

# åå‰ç©ºé–“ã®å‰Šé™¤
echo "ğŸ“‚ åå‰ç©ºé–“å‰Šé™¤ä¸­..."
kubectl delete namespace falco --ignore-not-found
kubectl delete namespace baseline-secure --ignore-not-found
kubectl delete namespace restricted-secure --ignore-not-found
kubectl delete namespace privileged-admin --ignore-not-found
kubectl delete namespace production --ignore-not-found
kubectl delete namespace staging --ignore-not-found
kubectl delete namespace development --ignore-not-found
kubectl delete namespace monitoring --ignore-not-found
kubectl delete namespace admin --ignore-not-found

# RBACè¨­å®šã®å‰Šé™¤
echo "ğŸ‘¥ RBACè¨­å®šå‰Šé™¤ä¸­..."
kubectl delete clusterrolebindings --selector='!kubernetes.io/bootstrapping' 2>/dev/null || true
kubectl delete clusterroles --selector='!kubernetes.io/bootstrapping' 2>/dev/null || true

# CRDã®å‰Šé™¤
echo "ğŸ”Œ CRDå‰Šé™¤ä¸­..."
kubectl delete crd --selector='app.kubernetes.io/part-of=cert-manager' 2>/dev/null || true

# ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®å‰Šé™¤
echo "ğŸ“„ ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ä¸­..."
rm -f /tmp/incident-*.txt
rm -f /tmp/security-incident-report.md
rm -f audit-policy.yaml
rm -f encryption-config.yaml
rm -f comprehensive-audit-policy.yaml

echo "ğŸ“Š ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—çŠ¶æ³ç¢ºèª:"
echo "Namespaces: $(kubectl get namespaces | wc -l) å€‹"
echo "Pods: $(kubectl get pods --all-namespaces --no-headers | wc -l) å€‹"
echo "NetworkPolicies: $(kubectl get networkpolicies --all-namespaces --no-headers | wc -l) å€‹"

echo "âœ… ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ©ãƒœç’°å¢ƒã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†!"

# ã‚³ã‚¹ãƒˆç¢ºèª
echo ""
echo "ğŸ’° æœ¬ãƒ©ãƒœã®æ¨å®šã‚³ã‚¹ãƒˆï¼š"
echo "   - AWS EKS + Security Tools: ~$25-35 (10æ™‚é–“å®Ÿè¡Œ)"
echo "   - Google GKE + Istio: ~$20-30 (10æ™‚é–“å®Ÿè¡Œ)"
echo "   - Azure AKS + Calico: ~$20-30 (10æ™‚é–“å®Ÿè¡Œ)"
echo "   - ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒ (kind): ç„¡æ–™"
```

## ğŸ“š å­¦ç¿’ã®ãƒã‚¤ãƒ³ãƒˆ

### CKSè©¦é¨“ã§ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¦ç‚¹

1. **ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ¬ã‚¤ãƒ¤ãƒ¼ãƒ‰ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ**
   - ã‚¤ãƒ³ãƒ•ãƒ©ã‚¹ãƒˆãƒ©ã‚¯ãƒãƒ£ãƒ¬ãƒ™ãƒ«
   - ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãƒ¬ãƒ™ãƒ«  
   - ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¬ãƒ™ãƒ«
   - ãƒ‡ãƒ¼ã‚¿ãƒ¬ãƒ™ãƒ«

2. **ç¶™ç¶šçš„ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ç®¡ç†**
   - è‡ªå‹•åŒ–ã•ã‚ŒãŸã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¨­å®š
   - ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–
   - è¿…é€Ÿãªã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆå¯¾å¿œ
   - å®šæœŸçš„ãªã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è©•ä¾¡

3. **ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹å¯¾å¿œ**
   - CIS Benchmarkæº–æ‹ 
   - æ¥­ç•Œæ¨™æº–ã¸ã®é©åˆ
   - ç›£æŸ»è¨¼è·¡ã®ç¢ºä¿
   - æ–‡æ›¸åŒ–ã•ã‚ŒãŸæ‰‹é †

## ğŸ¯ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

**å®Œäº†ã—ãŸã‚¹ã‚­ãƒ«:**
- [x] ã‚»ã‚­ãƒ¥ã‚¢ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åŸºç›¤æ§‹ç¯‰
- [x] TLS/SSLè¨¼æ˜æ›¸ç®¡ç†
- [x] ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å®Ÿè£…
- [x] RBACè¨­è¨ˆã¨å®Ÿè£…
- [x] ç›£æŸ»ãƒ­ã‚°ã¨ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ç›£è¦–
- [x] ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆå¯¾å¿œæ‰‹é †

**æ¬¡ã®ãƒ©ãƒœ:** [Lab 2: ãƒã‚¤ã‚¯ãƒ­ã‚µãƒ¼ãƒ“ã‚¹ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å®Ÿè£…](./lab02-microservice-security.md)

**é‡è¦ãªæ³¨æ„:**
ã“ã®ãƒ©ãƒœã§æ§‹ç¯‰ã—ãŸã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£åŸºç›¤ã¯ã€CKSè©¦é¨“ã ã‘ã§ãªãã€å®Ÿéš›ã®ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºç’°å¢ƒã§ã‚‚é©ç”¨ã§ãã‚‹å®Ÿè·µçš„ãªå†…å®¹ã§ã™ã€‚ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã¯ä¸€åº¦è¨­å®šã™ã‚Œã°çµ‚ã‚ã‚Šã§ã¯ãªãã€ç¶™ç¶šçš„ãªæ”¹å–„ã¨ç›£è¦–ãŒå¿…è¦ã§ã‚ã‚‹ã“ã¨ã‚’å¿˜ã‚Œãªã„ã§ãã ã•ã„ã€‚