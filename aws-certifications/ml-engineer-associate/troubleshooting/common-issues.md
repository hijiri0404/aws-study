# ML Engineer Associate - ã‚ˆãã‚ã‚‹å•é¡Œã¨è§£æ±ºç­–

## ðŸ“‹ æ¦‚è¦

MLA-C01è©¦é¨“ã¨MLå®Ÿå‹™ã§ã‚ˆãé­é‡ã™ã‚‹å•é¡Œã¨ãã®è§£æ±ºç­–ã‚’ã¾ã¨ã‚ã¦ã„ã¾ã™ã€‚SageMakerã€MLOpsã€æ©Ÿæ¢°å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é–¢é€£ã®ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°æ‰‹é †ã‚’å«ã‚ã¦è§£èª¬ã—ã¾ã™ã€‚

## ðŸš¨ Domain 1: Data Engineering - ãƒ‡ãƒ¼ã‚¿é–¢é€£

### å•é¡Œ1: S3ã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ãŒé…ã„

#### ç—‡çŠ¶
- SageMaker Training Job ã§ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ãŒç•°å¸¸ã«é…ã„
- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒå¤§ãã„å ´åˆã®å‡¦ç†æ™‚é–“ãŒé•·ã„

#### åŽŸå› åˆ†æž
- S3ã®ãƒžãƒ«ãƒãƒ‘ãƒ¼ãƒˆèª­ã¿è¾¼ã¿æœªè¨­å®š
- ä¸é©åˆ‡ãªãƒ‡ãƒ¼ã‚¿å½¢å¼ï¼ˆCSV vs Parquetï¼‰
- S3ã¨SageMakerã®ç•°ãªã‚‹ãƒªãƒ¼ã‚¸ãƒ§ãƒ³

#### è§£æ±ºæ‰‹é †
```python
# 1. S3 ãƒ‡ãƒ¼ã‚¿ã®æœ€é©åŒ–
import boto3
import pandas as pd
from concurrent.futures import ThreadPoolExecutor

def optimize_s3_data_loading():
    # ãƒžãƒ«ãƒãƒ‘ãƒ¼ãƒˆèª­ã¿è¾¼ã¿è¨­å®š
    s3_client = boto3.client('s3')
    config = boto3.session.Config(
        max_pool_connections=50,
        retries={'max_attempts': 3}
    )
    
    # Parquetå½¢å¼ã§ã®ä¿å­˜ï¼ˆé«˜é€Ÿèª­ã¿è¾¼ã¿ï¼‰
    df = pd.read_csv('s3://my-bucket/data.csv')
    df.to_parquet('s3://my-bucket/data.parquet', index=False)
    
    return "ãƒ‡ãƒ¼ã‚¿æœ€é©åŒ–å®Œäº†"

# 2. SageMaker ã§ã®åŠ¹çŽ‡çš„ãªãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
import sagemaker
from sagemaker.inputs import TrainingInput

def setup_efficient_data_input():
    # S3 Input Mode ã‚’ File ã‹ã‚‰ Pipe ã«å¤‰æ›´
    train_input = TrainingInput(
        s3_data='s3://my-bucket/train/',
        content_type='text/csv',
        input_mode='Pipe',  # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°èª­ã¿è¾¼ã¿
        distribution='ShardedByS3Key'
    )
    
    return train_input
```

#### äºˆé˜²ç­–
- ãƒ‡ãƒ¼ã‚¿ã¯Parquetå½¢å¼ã§ä¿å­˜
- SageMakerã¨åŒã˜ãƒªãƒ¼ã‚¸ãƒ§ãƒ³ã«ãƒ‡ãƒ¼ã‚¿é…ç½®
- é©åˆ‡ãªS3ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚¯ãƒ©ã‚¹ã®é¸æŠž

### å•é¡Œ2: ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãŒå¤±æ•—ã™ã‚‹

#### ç—‡çŠ¶
```
ProcessingJobError: Job failed with the following error: OutOfMemoryError
```

#### åŽŸå› åˆ†æž
- ãƒ¡ãƒ¢ãƒªä¸è¶³
- éžåŠ¹çŽ‡ãªãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
- ä¸é©åˆ‡ãªã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚¿ã‚¤ãƒ—

#### è§£æ±ºæ‰‹é †
```python
# 1. ãƒ¡ãƒ¢ãƒªåŠ¹çŽ‡çš„ãªãƒ‡ãƒ¼ã‚¿å‡¦ç†
from sagemaker.processing import ProcessingInput, ProcessingOutput
from sagemaker.sklearn.processing import SKLearnProcessor

def optimize_data_processing():
    # ã‚ˆã‚Šå¤§ããªã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½¿ç”¨
    processor = SKLearnProcessor(
        framework_version='0.23-1',
        instance_type='ml.m5.2xlarge',  # ãƒ¡ãƒ¢ãƒªå¢—é‡
        instance_count=2,  # ä¸¦åˆ—å‡¦ç†
        base_job_name='data-processing'
    )
    
    # ãƒãƒ£ãƒ³ã‚¯å‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
    processing_script = '''
import pandas as pd
import numpy as np

def process_in_chunks(input_path, output_path, chunk_size=10000):
    """ãƒãƒ£ãƒ³ã‚¯å˜ä½ã§ãƒ‡ãƒ¼ã‚¿å‡¦ç†"""
    chunks = []
    for chunk in pd.read_csv(input_path, chunksize=chunk_size):
        # ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        chunk = chunk.dropna()
        chunk = chunk[chunk['value'] > 0]
        chunks.append(chunk)
    
    # çµæžœã‚’ã¾ã¨ã‚ã¦ä¿å­˜
    result = pd.concat(chunks, ignore_index=True)
    result.to_csv(output_path, index=False)
    '''
    
    return processor, processing_script

# 2. åˆ†æ•£å‡¦ç†ã®è¨­å®š
from sagemaker.spark.processing import PySparkProcessor

def setup_distributed_processing():
    spark_processor = PySparkProcessor(
        base_job_name='spark-preprocessing',
        framework_version='3.1',
        instance_type='ml.m5.xlarge',
        instance_count=3,  # åˆ†æ•£å‡¦ç†
        max_runtime_in_seconds=3600
    )
    
    return spark_processor
```

## ðŸ¤– Domain 2: Exploratory Data Analysis - åˆ†æžé–¢é€£

### å•é¡Œ3: SageMaker Studio ãŒèµ·å‹•ã—ãªã„

#### ç—‡çŠ¶
- Studio ã‚¢ãƒ—ãƒªãŒ "Pending" çŠ¶æ…‹ã®ã¾ã¾
- Jupyter Kernel ãŒèµ·å‹•ã—ãªã„

#### è§£æ±ºæ‰‹é †
```bash
# 1. SageMaker Studio ã®çŠ¶æ…‹ç¢ºèª
aws sagemaker describe-user-profile \
  --domain-id d-xxxxxxxxxxxx \
  --user-profile-name my-user-profile

# 2. VPCè¨­å®šã®ç¢ºèª
aws ec2 describe-subnets --subnet-ids subnet-xxxxxxxxx
aws ec2 describe-security-groups --group-ids sg-xxxxxxxxx

# 3. IAMæ¨©é™ã®ç¢ºèª
aws iam get-role --role-name SageMakerExecutionRole
aws iam list-attached-role-policies --role-name SageMakerExecutionRole
```

#### ä¿®æ­£ä¾‹
```python
# SageMaker Studio Domain ã®å†ä½œæˆ
import boto3

def recreate_studio_domain():
    sagemaker_client = boto3.client('sagemaker')
    
    # Domainè¨­å®š
    domain_settings = {
        'DomainName': 'my-ml-domain',
        'AuthMode': 'IAM',
        'DefaultUserSettings': {
            'ExecutionRole': 'arn:aws:iam::123456789012:role/SageMakerExecutionRole',
            'SecurityGroups': ['sg-xxxxxxxxx'],
            'SharingSettings': {
                'NotebookOutputOption': 'Allowed'
            }
        },
        'SubnetIds': ['subnet-xxxxxxxxx', 'subnet-yyyyyyyyy'],
        'VpcId': 'vpc-xxxxxxxxx'
    }
    
    response = sagemaker_client.create_domain(**domain_settings)
    return response['DomainArn']
```

### å•é¡Œ4: Feature Store ã®æ›¸ãè¾¼ã¿ãŒå¤±æ•—ã™ã‚‹

#### ç—‡çŠ¶
- Feature Group ã¸ã®æ›¸ãè¾¼ã¿ã‚¨ãƒ©ãƒ¼
- ãƒ‡ãƒ¼ã‚¿åž‹ä¸æ•´åˆã‚¨ãƒ©ãƒ¼

#### è§£æ±ºæ‰‹é †
```python
# 1. Feature Store ã®è¨­å®šç¢ºèª
import boto3
from sagemaker.feature_store.feature_group import FeatureGroup

def debug_feature_store():
    sagemaker_client = boto3.client('sagemaker')
    
    # Feature Group ã®çŠ¶æ…‹ç¢ºèª
    response = sagemaker_client.describe_feature_group(
        FeatureGroupName='my-feature-group'
    )
    
    print(f"Status: {response['FeatureGroupStatus']}")
    print(f"Schema: {response['FeatureDefinitions']}")
    
    return response

# 2. ãƒ‡ãƒ¼ã‚¿åž‹ã®ä¿®æ­£
import pandas as pd
from sagemaker.feature_store.inputs import FeatureValue

def fix_data_types_for_feature_store():
    # ãƒ‡ãƒ¼ã‚¿åž‹ã‚’æ˜Žç¤ºçš„ã«æŒ‡å®š
    df = pd.DataFrame({
        'customer_id': [1, 2, 3],
        'feature_1': [1.0, 2.0, 3.0],  # float64
        'feature_2': ['A', 'B', 'C'],  # string
        'event_time': pd.to_datetime(['2024-01-01', '2024-01-02', '2024-01-03'])
    })
    
    # Event Time ã‚’é©åˆ‡ãªå½¢å¼ã«å¤‰æ›
    df['event_time'] = df['event_time'].dt.strftime('%Y-%m-%dT%H:%M:%SZ')
    
    return df
```

## ðŸ—ï¸ Domain 3: Model Building - ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰é–¢é€£

### å•é¡Œ5: SageMaker Training Job ãŒå¤±æ•—ã™ã‚‹

#### ç—‡çŠ¶
```
AlgorithmError: ExecuteUserScriptError: 
Command "/opt/ml/code/train.py" died with exit code 1
```

#### åŽŸå› åˆ†æž
- ä¾å­˜é–¢ä¿‚ã®å•é¡Œ
- ä¸é©åˆ‡ãªãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
- GPU/CPU ãƒªã‚½ãƒ¼ã‚¹ä¸è¶³

#### è§£æ±ºæ‰‹é †
```python
# 1. ä¾å­˜é–¢ä¿‚ã®æ˜Žç¤ºçš„ãªæŒ‡å®š
# requirements.txt
'''
scikit-learn==1.0.2
pandas==1.3.3
numpy==1.21.2
boto3==1.18.12
'''

# 2. ä¿®æ­£ã•ã‚ŒãŸãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
import argparse
import joblib
import os
import logging

def setup_training_script():
    """ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã‚’å«ã‚€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆ"""
    script = '''
import argparse
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
import joblib
import logging

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def main():
    try:
        parser = argparse.ArgumentParser()
        parser.add_argument('--n-estimators', type=int, default=100)
        parser.add_argument('--max-depth', type=int, default=3)
        args = parser.parse_args()
        
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        logger.info("ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–‹å§‹")
        train_data = pd.read_csv('/opt/ml/input/data/train/train.csv')
        
        # ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®åˆ†é›¢
        X = train_data.drop('target', axis=1)
        y = train_data['target']
        
        # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
        logger.info("ãƒ¢ãƒ‡ãƒ«å­¦ç¿’é–‹å§‹")
        model = RandomForestClassifier(
            n_estimators=args.n_estimators,
            max_depth=args.max_depth,
            random_state=42
        )
        model.fit(X, y)
        
        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        logger.info("ãƒ¢ãƒ‡ãƒ«ä¿å­˜é–‹å§‹")
        joblib.dump(model, '/opt/ml/model/model.pkl')
        logger.info("ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†")
        
    except Exception as e:
        logger.error(f"ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {str(e)}")
        raise

if __name__ == "__main__":
    main()
    '''
    
    return script

# 3. SageMaker Estimator ã®è¨­å®š
from sagemaker.sklearn.estimator import SKLearn

def setup_robust_estimator():
    estimator = SKLearn(
        entry_point='train.py',
        framework_version='0.23-1',
        instance_type='ml.m5.large',
        instance_count=1,
        hyperparameters={
            'n-estimators': 100,
            'max-depth': 5
        },
        max_run=3600,  # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®š
        use_spot_instances=True,  # ã‚³ã‚¹ãƒˆå‰Šæ¸›
        max_wait=7200
    )
    
    return estimator
```

### å•é¡Œ6: ãƒ¢ãƒ‡ãƒ«ã®ç²¾åº¦ãŒä½Žã„

#### ç—‡çŠ¶
- æœŸå¾…ã™ã‚‹ç²¾åº¦ãŒå‡ºãªã„
- éŽå­¦ç¿’ã¾ãŸã¯æœªå­¦ç¿’
- æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã§ã®æ€§èƒ½ä½Žä¸‹

#### è§£æ±ºæ‰‹é †
```python
# 1. è©³ç´°ãª ãƒ¢ãƒ‡ãƒ«è©•ä¾¡
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.model_selection import learning_curve

def comprehensive_model_evaluation():
    """åŒ…æ‹¬çš„ãªãƒ¢ãƒ‡ãƒ«è©•ä¾¡"""
    
    # å­¦ç¿’æ›²ç·šã®æç”»
    def plot_learning_curve(estimator, X, y, cv=5):
        train_sizes, train_scores, val_scores = learning_curve(
            estimator, X, y, cv=cv, n_jobs=-1, 
            train_sizes=np.linspace(0.1, 1.0, 10)
        )
        
        plt.figure(figsize=(10, 6))
        plt.plot(train_sizes, np.mean(train_scores, axis=1), 'o-', label='Training Score')
        plt.plot(train_sizes, np.mean(val_scores, axis=1), 'o-', label='Validation Score')
        plt.xlabel('Training Size')
        plt.ylabel('Score')
        plt.legend()
        plt.title('Learning Curve')
        plt.show()
    
    # ç‰¹å¾´é‡é‡è¦åº¦ã®åˆ†æž
    def analyze_feature_importance(model, feature_names):
        importances = model.feature_importances_
        indices = np.argsort(importances)[::-1]
        
        plt.figure(figsize=(12, 6))
        plt.bar(range(len(importances)), importances[indices])
        plt.xticks(range(len(importances)), [feature_names[i] for i in indices], rotation=45)
        plt.title('Feature Importance')
        plt.tight_layout()
        plt.show()
    
    return plot_learning_curve, analyze_feature_importance

# 2. ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–
from sagemaker.tuner import HyperparameterTuner, IntegerParameter, ContinuousParameter

def setup_hyperparameter_tuning():
    # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç¯„å›²å®šç¾©
    hyperparameter_ranges = {
        'n-estimators': IntegerParameter(50, 500),
        'max-depth': IntegerParameter(3, 20),
        'min-samples-split': IntegerParameter(2, 20),
        'min-samples-leaf': IntegerParameter(1, 10)
    }
    
    # ãƒãƒ¥ãƒ¼ãƒŠãƒ¼è¨­å®š
    tuner = HyperparameterTuner(
        estimator=estimator,
        objective_metric_name='validation:accuracy',
        hyperparameter_ranges=hyperparameter_ranges,
        max_jobs=20,
        max_parallel_jobs=3,
        objective_type='Maximize'
    )
    
    return tuner
```

## ðŸš€ Domain 4: Model Deployment - ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆé–¢é€£

### å•é¡Œ7: SageMaker Endpoint ã®èµ·å‹•ãŒé…ã„

#### ç—‡çŠ¶
- Endpoint ã®èµ·å‹•ã«10åˆ†ä»¥ä¸Šã‹ã‹ã‚‹
- Cold Start ã«ã‚ˆã‚‹åˆå›žæŽ¨è«–ã®é…å»¶

#### è§£æ±ºæ‰‹é †
```python
# 1. Multi-Model Endpoint ã®æ´»ç”¨
from sagemaker.multidatamodel import MultiDataModel

def setup_multi_model_endpoint():
    # Multi-Model Endpoint ã§è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã‚’åŠ¹çŽ‡çš„ã«ç®¡ç†
    multi_model = MultiDataModel(
        name='multi-model-endpoint',
        model_data_prefix='s3://my-bucket/models/',
        model=model,
        sagemaker_session=sagemaker_session
    )
    
    # Auto Scaling è¨­å®š
    predictor = multi_model.deploy(
        initial_instance_count=1,
        instance_type='ml.m5.large',
        endpoint_name='multi-model-endpoint'
    )
    
    return predictor

# 2. Serverless Inference ã®åˆ©ç”¨
from sagemaker.serverless import ServerlessInferenceConfig

def setup_serverless_inference():
    # ã‚³ãƒ¼ãƒ«ãƒ‰ã‚¹ã‚¿ãƒ¼ãƒˆæ™‚é–“ã‚’çŸ­ç¸®
    serverless_config = ServerlessInferenceConfig(
        memory_size_in_mb=3008,
        max_concurrency=10
    )
    
    predictor = model.deploy(
        serverless_inference_config=serverless_config,
        endpoint_name='serverless-endpoint'
    )
    
    return predictor
```

### å•é¡Œ8: æŽ¨è«–çµæžœã®ç²¾åº¦ãŒå­¦ç¿’æ™‚ã¨ç•°ãªã‚‹

#### ç—‡çŠ¶
- å­¦ç¿’æ™‚ã¯é«˜ç²¾åº¦ã ãŒæŽ¨è«–æ™‚ã¯ä½Žç²¾åº¦
- ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆã®ç™ºç”Ÿ

#### è§£æ±ºæ‰‹é †
```python
# 1. Model Monitor ã®è¨­å®š
from sagemaker.model_monitor import DefaultModelMonitor
from sagemaker.model_monitor.dataset_format import DatasetFormat

def setup_model_monitoring():
    # ãƒ‡ãƒ¼ã‚¿å“è³ªç›£è¦–
    data_quality_monitor = DefaultModelMonitor(
        role=role,
        instance_count=1,
        instance_type='ml.m5.xlarge',
        volume_size_in_gb=20,
        max_runtime_in_seconds=3600,
    )
    
    # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ä½œæˆ
    baseline_job = data_quality_monitor.suggest_baseline(
        baseline_dataset='s3://my-bucket/baseline/baseline.csv',
        dataset_format=DatasetFormat.csv(header=True),
        output_s3_uri='s3://my-bucket/baseline-results'
    )
    
    # ç›£è¦–ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«è¨­å®š
    data_quality_monitor.create_monitoring_schedule(
        monitor_schedule_name='data-quality-schedule',
        endpoint_input=endpoint_name,
        output_s3_uri='s3://my-bucket/monitoring-results',
        statistics=baseline_job.baseline_statistics(),
        constraints=baseline_job.suggested_constraints(),
        schedule_cron_expression='cron(0 * * * * ?)'  # æ¯Žæ™‚å®Ÿè¡Œ
    )
    
    return data_quality_monitor

# 2. A/B ãƒ†ã‚¹ãƒˆã®å®Ÿè£…
from sagemaker.model_monitor import ModelQualityMonitor

def setup_ab_testing():
    # è¤‡æ•°ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ‡ãƒ—ãƒ­ã‚¤
    variant_1 = {
        'VariantName': 'model-v1',
        'ModelName': 'model-v1',
        'InitialInstanceCount': 1,
        'InstanceType': 'ml.m5.large',
        'InitialVariantWeight': 70  # 70%ã®ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯
    }
    
    variant_2 = {
        'VariantName': 'model-v2', 
        'ModelName': 'model-v2',
        'InitialInstanceCount': 1,
        'InstanceType': 'ml.m5.large',
        'InitialVariantWeight': 30  # 30%ã®ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯
    }
    
    # Endpoint è¨­å®š
    endpoint_config = {
        'EndpointConfigName': 'ab-test-config',
        'ProductionVariants': [variant_1, variant_2]
    }
    
    return endpoint_config
```

## ðŸ”„ Domain 5: ML Operations and ML Engineering - MLOpsé–¢é€£

### å•é¡Œ9: SageMaker Pipeline ãŒå¤±æ•—ã™ã‚‹

#### ç—‡çŠ¶
- Pipeline ã®é€”ä¸­ã§ã‚¹ãƒ†ãƒƒãƒ—ãŒå¤±æ•—
- ä¾å­˜é–¢ä¿‚ã®å•é¡Œ

#### è§£æ±ºæ‰‹é †
```python
# 1. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã‚’å«ã‚€Pipelineè¨­è¨ˆ
from sagemaker.workflow.pipeline import Pipeline
from sagemaker.workflow.steps import ProcessingStep, TrainingStep
from sagemaker.workflow.step_collections import RegisterModel
from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo
from sagemaker.workflow.condition_step import ConditionStep

def create_robust_pipeline():
    # ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä»˜ãProcessingStep
    processing_step = ProcessingStep(
        name="ProcessingStep",
        processor=processor,
        inputs=[ProcessingInput(source=input_data, destination="/opt/ml/processing/input")],
        outputs=[ProcessingOutput(output_name="train", source="/opt/ml/processing/train")],
        code="preprocessing.py",
        job_arguments=["--log-level", "INFO"]
    )
    
    # æ¡ä»¶ä»˜ãTrainingStep
    training_step = TrainingStep(
        name="TrainingStep",
        estimator=estimator,
        inputs={"train": TrainingInput(s3_data=processing_step.properties.ProcessingOutputConfig.Outputs["train"].S3Output.S3Uri)}
    )
    
    # ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã«ã‚ˆã‚‹æ¡ä»¶åˆ†å²
    evaluation_condition = ConditionGreaterThanOrEqualTo(
        left=JsonGet(
            step_name=training_step.name,
            property_file=PropertyFile(name="EvaluationReport", output_name="evaluation", path="evaluation.json"),
            json_path="classification_metrics.accuracy.value"
        ),
        right=0.8  # ç²¾åº¦80%ä»¥ä¸Šã®å ´åˆã®ã¿ç™»éŒ²
    )
    
    # æ¡ä»¶ä»˜ããƒ¢ãƒ‡ãƒ«ç™»éŒ²
    register_step = RegisterModel(
        name="RegisterModel",
        estimator=estimator,
        model_data=training_step.properties.ModelArtifacts.S3ModelArtifacts,
        content_types=["text/csv"],
        response_types=["text/csv"],
        inference_instances=["ml.t2.medium", "ml.m5.large"],
        transform_instances=["ml.m5.large"],
        model_package_group_name="my-model-group"
    )
    
    condition_step = ConditionStep(
        name="CheckAccuracy",
        conditions=[evaluation_condition],
        if_steps=[register_step],
        else_steps=[]
    )
    
    # Pipelineæ§‹ç¯‰
    pipeline = Pipeline(
        name="robust-ml-pipeline",
        steps=[processing_step, training_step, condition_step],
        sagemaker_session=sagemaker_session
    )
    
    return pipeline

# 2. Pipelineå®Ÿè¡ŒçŠ¶æ³ã®ç›£è¦–
def monitor_pipeline_execution(pipeline_name):
    """Pipelineå®Ÿè¡ŒçŠ¶æ³ã®ç›£è¦–"""
    import boto3
    
    sagemaker_client = boto3.client('sagemaker')
    
    # æœ€æ–°ã®å®Ÿè¡Œæƒ…å ±ã‚’å–å¾—
    response = sagemaker_client.list_pipeline_executions(
        PipelineName=pipeline_name,
        MaxResults=1
    )
    
    execution_arn = response['PipelineExecutionSummaries'][0]['PipelineExecutionArn']
    
    # å®Ÿè¡Œè©³ç´°ã‚’å–å¾—
    execution_details = sagemaker_client.describe_pipeline_execution(
        PipelineExecutionArn=execution_arn
    )
    
    print(f"Status: {execution_details['PipelineExecutionStatus']}")
    
    # å„ã‚¹ãƒ†ãƒƒãƒ—ã®çŠ¶æ³ç¢ºèª
    steps = sagemaker_client.list_pipeline_execution_steps(
        PipelineExecutionArn=execution_arn
    )
    
    for step in steps['PipelineExecutionSteps']:
        print(f"Step: {step['StepName']}, Status: {step['StepStatus']}")
        if step['StepStatus'] == 'Failed':
            print(f"Failure Reason: {step.get('FailureReason', 'N/A')}")
    
    return execution_details
```

### å•é¡Œ10: Feature Store ã®æ€§èƒ½å•é¡Œ

#### ç—‡çŠ¶
- Feature Store ã‹ã‚‰ã®èª­ã¿è¾¼ã¿ãŒé…ã„
- å¤§é‡ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†ã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ

#### è§£æ±ºæ‰‹é †
```python
# 1. åŠ¹çŽ‡çš„ãªFeatureå–å¾—
from sagemaker.feature_store.feature_group import FeatureGroup

def optimize_feature_retrieval():
    # ãƒãƒƒãƒå–å¾—ã®æœ€é©åŒ–
    feature_group = FeatureGroup(
        name='optimized-feature-group',
        sagemaker_session=sagemaker_session
    )
    
    # å¿…è¦ãªç‰¹å¾´é‡ã®ã¿å–å¾—
    query = f"""
    SELECT customer_id, feature_1, feature_2, feature_3
    FROM "{feature_group.name}"
    WHERE event_time >= '2024-01-01T00:00:00Z'
    AND event_time < '2024-12-31T23:59:59Z'
    """
    
    # AthenaçµŒç”±ã§ã®é«˜é€Ÿã‚¯ã‚¨ãƒª
    athena_client = boto3.client('athena')
    query_execution = athena_client.start_query_execution(
        QueryString=query,
        QueryExecutionContext={'Database': 'sagemaker_featurestore'},
        ResultConfiguration={'OutputLocation': 's3://my-bucket/athena-results/'}
    )
    
    return query_execution

# 2. ã‚­ãƒ£ãƒƒã‚·ãƒ¥æˆ¦ç•¥ã®å®Ÿè£…
import redis

def implement_feature_caching():
    # Redis ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®š
    redis_client = redis.Redis(host='localhost', port=6379, db=0)
    
    def get_cached_features(customer_id):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸç‰¹å¾´é‡ã‚’å–å¾—"""
        cache_key = f"features:{customer_id}"
        cached_features = redis_client.get(cache_key)
        
        if cached_features:
            return json.loads(cached_features)
        else:
            # Feature Store ã‹ã‚‰å–å¾—
            features = feature_group.get_record(
                identifier_value_as_string=str(customer_id)
            )
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜ï¼ˆ1æ™‚é–“ã®æœ‰åŠ¹æœŸé™ï¼‰
            redis_client.setex(cache_key, 3600, json.dumps(features))
            return features
    
    return get_cached_features
```

## ðŸ› ï¸ ä¸€èˆ¬çš„ãªãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°æ‰‹æ³•

### ãƒ‡ãƒãƒƒã‚°ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

#### 1. CloudWatch Logs ã®æ´»ç”¨
```python
# CloudWatch Logs ã§ã®ã‚¨ãƒ©ãƒ¼è¿½è·¡
import boto3

def analyze_sagemaker_logs():
    logs_client = boto3.client('logs')
    
    # SageMaker Job ã®ãƒ­ã‚°ç¢ºèª
    response = logs_client.filter_log_events(
        logGroupName='/aws/sagemaker/TrainingJobs',
        startTime=int((datetime.now() - timedelta(hours=1)).timestamp() * 1000),
        filterPattern='ERROR'
    )
    
    for event in response['events']:
        print(f"Time: {event['timestamp']}, Message: {event['message']}")
```

#### 2. ã‚³ã‚¹ãƒˆæœ€é©åŒ–
```python
# Spot Instance ã®æ´»ç”¨
from sagemaker.estimator import Estimator

def cost_optimized_training():
    estimator = Estimator(
        image_uri='your-image-uri',
        role=role,
        instance_count=1,
        instance_type='ml.m5.large',
        use_spot_instances=True,  # Spot Instance ä½¿ç”¨
        max_wait=7200,  # æœ€å¤§å¾…æ©Ÿæ™‚é–“
        max_run=3600,   # æœ€å¤§å®Ÿè¡Œæ™‚é–“
        checkpoint_s3_uri='s3://my-bucket/checkpoints/'  # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
    )
    
    return estimator
```

## ðŸ“š äºˆé˜²ç­–ã¨ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

### 1. ã‚¨ãƒ©ãƒ¼ç›£è¦–
- CloudWatch Alarms ã®è¨­å®š
- SageMaker Model Monitor ã®æ´»ç”¨
- ç•°å¸¸æ¤œçŸ¥ã®è‡ªå‹•åŒ–

### 2. ãƒ‡ãƒ¼ã‚¿å“è³ªç®¡ç†
- ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®æ§‹ç¯‰
- ã‚¹ã‚­ãƒ¼ãƒžæ¤œè¨¼ã®å®Ÿè£…
- çµ±è¨ˆçš„å“è³ªãƒã‚§ãƒƒã‚¯

### 3. ãƒ¢ãƒ‡ãƒ«ç®¡ç†
- MLflow ã«ã‚ˆã‚‹å®Ÿé¨“ç®¡ç†
- Model Registry ã®æ´»ç”¨
- ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã®å¾¹åº•

### 4. ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£
- VPC è¨­å®šã®é©åˆ‡ãªæ§‹æˆ
- IAM æ¨©é™ã®æœ€å°åŒ–
- æš—å·åŒ–ã®å®Ÿè£…

## ðŸŽ¯ è©¦é¨“å¯¾ç­–ã®ãƒã‚¤ãƒ³ãƒˆ

### ã‚ˆãå‡ºé¡Œã•ã‚Œã‚‹å•é¡Œãƒ‘ã‚¿ãƒ¼ãƒ³
1. **SageMakerè¨­å®šã‚¨ãƒ©ãƒ¼**: IAMæ¨©é™ã€VPCè¨­å®š
2. **ãƒ‡ãƒ¼ã‚¿å‡¦ç†å•é¡Œ**: ãƒ¡ãƒ¢ãƒªä¸è¶³ã€å½¢å¼ã‚¨ãƒ©ãƒ¼
3. **ãƒ¢ãƒ‡ãƒ«æ€§èƒ½å•é¡Œ**: éŽå­¦ç¿’ã€ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆ
4. **ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆå•é¡Œ**: ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆè¨­å®šã€ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
5. **MLOpså•é¡Œ**: ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³è¨­è¨ˆã€ç›£è¦–è¨­å®š

### é‡è¦ãªè¨ºæ–­ã‚³ãƒžãƒ³ãƒ‰
```bash
# AWS CLI ã«ã‚ˆã‚‹è¨ºæ–­
aws sagemaker describe-training-job --training-job-name my-job
aws sagemaker describe-endpoint --endpoint-name my-endpoint
aws logs describe-log-groups --log-group-name-prefix /aws/sagemaker
aws s3 ls s3://my-bucket/data/ --recursive
```

---

**é‡è¦**: ML Engineer Associate ã§ã¯ã€ç†è«–çŸ¥è­˜ã¨ã¨ã‚‚ã«å®Ÿéš›ã®SageMakeræ“ä½œçµŒé¨“ãŒé‡è¦ã§ã™ã€‚æ§˜ã€…ãªã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’çµŒé¨“ã—ã€é©åˆ‡ãªå¯¾å‡¦æ³•ã‚’èº«ã«ã¤ã‘ã‚‹ã“ã¨ãŒåˆæ ¼ã®éµã¨ãªã‚Šã¾ã™ã€‚