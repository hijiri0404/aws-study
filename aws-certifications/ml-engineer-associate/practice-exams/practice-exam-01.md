# MLE-A æƒ³å®šå•é¡Œé›† 01 - ãƒ‡ãƒ¼ã‚¿æº–å‚™ã¨Feature Engineering

## ğŸ“‹ è©¦é¨“æƒ…å ±

**Domain Focus**: Domain 1 - Data Preparation for Machine Learning (28%)  
**å•é¡Œæ•°**: 20å•  
**åˆ¶é™æ™‚é–“**: 30åˆ†  
**åˆæ ¼ç‚¹**: 14/20 (70%)

---

## ğŸ”§ å•é¡Œ 1
ã‚ãªãŸã¯ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å°å£²æ¥­ã®æ©Ÿæ¢°å­¦ç¿’ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã§ã™ã€‚é¡§å®¢ã®è³¼è²·å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦å•†å“æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã—ã¦ã„ã¾ã™ã€‚é¡§å®¢ã®è³¼è²·ãƒ‡ãƒ¼ã‚¿ã«ã¯ä»¥ä¸‹ã®ç‰¹å¾´ãŒã‚ã‚Šã¾ã™ï¼š

- ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: 10TB
- æ›´æ–°é »åº¦: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ 
- ç‰¹å¾´é‡æ•°: 500+
- äºˆæ¸¬ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·è¦ä»¶: 100msä»¥ä¸‹

ã“ã®ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ã«æœ€é©ãªAWSã‚µãƒ¼ãƒ“ã‚¹ã®çµ„ã¿åˆã‚ã›ã¯ï¼Ÿ

**A)** Amazon S3 + Amazon Athena + SageMaker Batch Transform  
**B)** Amazon SageMaker Feature Store + SageMaker Real-time Inference  
**C)** Amazon Redshift + AWS Lambda + API Gateway  
**D)** Amazon DynamoDB + Amazon Kinesis + SageMaker Multi-Model Endpoints

<details>
<summary>è§£ç­”ã¨è§£èª¬</summary>

**æ­£è§£: B**

**è§£èª¬:**
- **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ›´æ–°**: Feature Store ã®ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã‚¹ãƒˆã‚¢ãŒãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ›´æ–°ã‚’ã‚µãƒãƒ¼ãƒˆ
- **å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿**: Feature Store ã¯å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã®åŠ¹ç‡çš„ãªç®¡ç†ãŒå¯èƒ½
- **ä½ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·**: ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã‚¹ãƒˆã‚¢ + ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¨è«–ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã§100msè¦ä»¶ã‚’æº€ãŸã›ã‚‹
- **å¤šæ•°ã®ç‰¹å¾´é‡**: Feature Store ã¯æ•°ç™¾ã®ç‰¹å¾´é‡ã‚’åŠ¹ç‡çš„ã«ç®¡ç†

**ä»–ã®é¸æŠè‚¢ãŒä¸é©åˆ‡ãªç†ç”±:**
- A: Batch Transform ã¯ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¨è«–ã«ä¸é©åˆ‡
- C: Redshift ã¯åˆ†æç”¨é€”ã§ã€100ms ã®ä½ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·è¦ä»¶ã«ä¸å‘ã
- D: DynamoDB ã¯ç‰¹å¾´é‡ã‚¹ãƒˆã‚¢ã¨ã—ã¦ã®æœ€é©åŒ–ãŒä¸ååˆ†
</details>

---

## ğŸ”§ å•é¡Œ 2
SageMaker Data Wrangler ã‚’ä½¿ç”¨ã—ã¦ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã‚’è¡Œã£ã¦ã„ã¾ã™ã€‚ä»¥ä¸‹ã®å‰å‡¦ç†ã‚¹ãƒ†ãƒƒãƒ—ã®ã†ã¡ã€Data Wrangler ã§ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ã‚‚ã®ã¯ï¼Ÿ

**A)** ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã®One-hot encoding  
**B)** æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®å·®åˆ†è¨ˆç®—  
**C)** ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†  
**D)** å¤–ã‚Œå€¤ã®æ¤œå‡ºã¨å‰Šé™¤

<details>
<summary>è§£ç­”ã¨è§£èª¬</summary>

**æ­£è§£: C**

**è§£èª¬:**
Data Wrangler ã¯**ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿å‡¦ç†**ã«ç‰¹åŒ–ã—ãŸã‚µãƒ¼ãƒ“ã‚¹ã§ã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†ã¯ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã›ã‚“ã€‚

**Data Wrangler ãŒã‚µãƒãƒ¼ãƒˆã™ã‚‹æ©Ÿèƒ½:**
- 300+ ã®çµ„ã¿è¾¼ã¿å¤‰æ›
- ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆOne-hot, Label encodingç­‰ï¼‰
- æ™‚ç³»åˆ—å¤‰æ›ï¼ˆãƒ©ã‚°ã€å·®åˆ†ã€ç§»å‹•å¹³å‡ç­‰ï¼‰
- å¤–ã‚Œå€¤æ¤œå‡ºã¨å‡¦ç†
- ãƒ‡ãƒ¼ã‚¿ã®å¯è¦–åŒ–ã¨å“è³ªãƒã‚§ãƒƒã‚¯

**ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†ã«ã¯:**
- Amazon Kinesis Analytics
- AWS Glue Streaming
- Amazon EMR with Spark Streaming
ã‚’ä½¿ç”¨ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
</details>

---

## ğŸ”§ å•é¡Œ 3
Feature Store ã§ç‰¹å¾´é‡ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ä½œæˆã™ã‚‹éš›ã®å¿…é ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ï¼Ÿ

**A)** `FeatureGroupName` ã®ã¿  
**B)** `FeatureGroupName`, `RecordIdentifierFeatureName`, `EventTimeFeatureName`  
**C)** `FeatureGroupName`, `S3Uri`, `RoleArn`  
**D)** `FeatureGroupName`, `OnlineStoreConfig`, `OfflineStoreConfig`

<details>
<summary>è§£ç­”ã¨è§£èª¬</summary>

**æ­£è§£: B**

**è§£èª¬:**
Feature Group ä½œæˆã®**å¿…é ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**ã¯ï¼š

1. **FeatureGroupName**: ç‰¹å¾´é‡ã‚°ãƒ«ãƒ¼ãƒ—ã®åå‰
2. **RecordIdentifierFeatureName**: ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ä¸€æ„ã«è­˜åˆ¥ã™ã‚‹ç‰¹å¾´é‡å
3. **EventTimeFeatureName**: ã‚¤ãƒ™ãƒ³ãƒˆç™ºç”Ÿæ™‚åˆ»ã‚’ç¤ºã™ç‰¹å¾´é‡å

```python
feature_group.create(
    s3_uri=s3_uri,
    record_identifier_name='customer_id',  # å¿…é ˆ
    event_time_feature_name='event_time',  # å¿…é ˆ
    role_arn=role,
    feature_definitions=feature_definitions
)
```

**ãã®ä»–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**:
- `S3Uri`, `RoleArn`: é€šå¸¸å¿…è¦ã ãŒã€ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‹ã‚‰æ¨è«–å¯èƒ½
- `OnlineStoreConfig`, `OfflineStoreConfig`: ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ä½¿ç”¨å¯èƒ½ï¼‰
</details>

---

## ğŸ”§ å•é¡Œ 4
å¤§è¦æ¨¡ãªCSVãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ100GBï¼‰ã‚’SageMaker Processing ã§åŠ¹ç‡çš„ã«å‡¦ç†ã—ãŸã„å ´åˆã®æ¨å¥¨ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯ï¼Ÿ

**A)** å˜ä¸€ã®ml.m5.24xlarge ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½¿ç”¨  
**B)** è¤‡æ•°ã®ml.m5.large ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†å‰²å‡¦ç†  
**C)** SageMaker Batch Transform ã‚’ä½¿ç”¨  
**D)** AWS Lambda ã§ä¸¦åˆ—å‡¦ç†

<details>
<summary>è§£ç­”ã¨è§£èª¬</summary>

**æ­£è§£: B**

**è§£èª¬:**
å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã®åŠ¹ç‡çš„ãªå‡¦ç†ã«ã¯**æ°´å¹³ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°**ãŒæ¨å¥¨ã•ã‚Œã¾ã™ã€‚

**æœ€é©ãªæ§‹æˆ:**
```python
sklearn_processor = SKLearnProcessor(
    framework_version='1.0-1',
    instance_type='ml.m5.large',
    instance_count=10,  # è¤‡æ•°ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
    volume_size_in_gb=100
)
```

**åˆ©ç‚¹:**
- ä¸¦åˆ—å‡¦ç†ã«ã‚ˆã‚‹é«˜é€ŸåŒ–
- ã‚³ã‚¹ãƒˆåŠ¹ç‡ï¼ˆå°ã•ã„ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ Ã— è¤‡æ•° < å¤§ãã„ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ Ã— 1ï¼‰
- éšœå®³è€æ€§ã®å‘ä¸Š
- æŸ”è»Ÿãªã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°

**ä»–ã®é¸æŠè‚¢ãŒä¸é©åˆ‡ãªç†ç”±:**
- A: å˜ä¸€ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã¯ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã«ãªã‚Šã‚„ã™ã„
- C: Batch Transform ã¯æ¨è«–ç”¨ã€å‰å‡¦ç†ã«ã¯ä¸é©åˆ‡
- D: Lambda ã®å®Ÿè¡Œæ™‚é–“åˆ¶é™ï¼ˆ15åˆ†ï¼‰ã«ã‚ˆã‚Šå¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã«ä¸å‘ã
</details>

---

## ğŸ”§ å•é¡Œ 5
æ™‚ç³»åˆ—äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã®ãŸã‚ã«ã€éå»30æ—¥é–“ã®å£²ä¸Šãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç‰¹å¾´é‡ã‚’ä½œæˆã—ã¦ã„ã¾ã™ã€‚ä»¥ä¸‹ã®ã†ã¡ã€ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã‚’é¿ã‘ã‚‹ãŸã‚ã«æœ€ã‚‚é‡è¦ãªè€ƒæ…®äº‹é …ã¯ï¼Ÿ

**A)** ç‰¹å¾´é‡ã®æ­£è¦åŒ–ã‚’è¡Œã†  
**B)** å°†æ¥ã®æƒ…å ±ã‚’å«ã‚€ç‰¹å¾´é‡ã‚’é™¤å¤–ã™ã‚‹  
**C)** æ¬ æå€¤ã‚’é©åˆ‡ã«å‡¦ç†ã™ã‚‹  
**D)** ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã™ã‚‹

<details>
<summary>è§£ç­”ã¨è§£èª¬</summary>

**æ­£è§£: B**

**è§£èª¬:**
**ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ï¼ˆData Leakageï¼‰**ã¯æ©Ÿæ¢°å­¦ç¿’ã§æœ€ã‚‚é‡è¦ãªå•é¡Œã®ä¸€ã¤ã§ã™ã€‚

**æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã§ã®ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ä¾‹:**
- äºˆæ¸¬æ™‚ç‚¹ã‚ˆã‚Šå¾Œã®æƒ…å ±ã‚’ç‰¹å¾´é‡ã«å«ã‚ã‚‹
- é›†è¨ˆæœŸé–“ãŒäºˆæ¸¬å¯¾è±¡æœŸé–“ã¨é‡è¤‡
- æœªæ¥ã®çµ±è¨ˆé‡ï¼ˆæœªæ¥ã®å¹³å‡å€¤ç­‰ï¼‰ã‚’ä½¿ç”¨

**äºˆé˜²ç­–:**
```python
# âŒ æ‚ªã„ä¾‹: å°†æ¥ã®æƒ…å ±ã‚’å«ã‚€
future_sales_avg = df['sales'].rolling(window=7).mean().shift(-3)  # 3æ—¥å¾Œã®æƒ…å ±

# âœ… è‰¯ã„ä¾‹: éå»ã®æƒ…å ±ã®ã¿
past_sales_avg = df['sales'].rolling(window=7).mean().shift(1)    # 1æ—¥å‰ã¾ã§ã®æƒ…å ±
```

**æ™‚ç³»åˆ—åˆ†å‰²ã§ã®æ³¨æ„ç‚¹:**
```python
# âŒ ãƒ©ãƒ³ãƒ€ãƒ åˆ†å‰²ï¼ˆæ™‚ç³»åˆ—ã§ã¯ä¸é©åˆ‡ï¼‰
train_test_split(X, y, test_size=0.2, random_state=42)

# âœ… æ™‚ç³»åˆ—åˆ†å‰²
train_data = data[data['date'] < '2024-01-01']
test_data = data[data['date'] >= '2024-01-01']
```
</details>

---

## ğŸ”§ å•é¡Œ 6
SageMaker Feature Store ã§ã€ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã‚¹ãƒˆã‚¢ã¨ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ã‚¹ãƒˆã‚¢ã®é©åˆ‡ãªä½¿ã„åˆ†ã‘ã¯ï¼Ÿ

**A)** ã‚ªãƒ³ãƒ©ã‚¤ãƒ³: ãƒãƒƒãƒäºˆæ¸¬ã€ã‚ªãƒ•ãƒ©ã‚¤ãƒ³: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ äºˆæ¸¬  
**B)** ã‚ªãƒ³ãƒ©ã‚¤ãƒ³: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ äºˆæ¸¬ã€ã‚ªãƒ•ãƒ©ã‚¤ãƒ³: ãƒ¢ãƒ‡ãƒ«è¨“ç·´ãƒ»ãƒãƒƒãƒäºˆæ¸¬  
**C)** ã‚ªãƒ³ãƒ©ã‚¤ãƒ³: ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã€ã‚ªãƒ•ãƒ©ã‚¤ãƒ³: ãƒ‡ãƒ¼ã‚¿åˆ†æ  
**D)** ä¸¡æ–¹ã¨ã‚‚åŒã˜ç”¨é€”ã§ä½¿ç”¨å¯èƒ½

<details>
<summary>è§£ç­”ã¨è§£èª¬</summary>

**æ­£è§£: B**

**è§£èª¬:**

**ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã‚¹ãƒˆã‚¢:**
- **ç”¨é€”**: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ äºˆæ¸¬ã€ä½ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·æ¤œç´¢
- **ç‰¹å¾´**: ã‚¤ãƒ³ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã€ãƒŸãƒªç§’ãƒ¬ãƒ™ãƒ«ã®å¿œç­”
- **åˆ¶é™**: é™å®šçš„ãªã‚¯ã‚¨ãƒªæ©Ÿèƒ½ã€é«˜ã„ã‚³ã‚¹ãƒˆ

**ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ã‚¹ãƒˆã‚¢:**
- **ç”¨é€”**: ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã€ãƒãƒƒãƒäºˆæ¸¬ã€ãƒ‡ãƒ¼ã‚¿åˆ†æ
- **ç‰¹å¾´**: S3ãƒ™ãƒ¼ã‚¹ã€Athenaã§è¤‡é›‘ãªã‚¯ã‚¨ãƒªå¯èƒ½
- **åˆ©ç‚¹**: å¤§å®¹é‡ã€ä½ã‚³ã‚¹ãƒˆã€é•·æœŸä¿å­˜

```python
# ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã‚¹ãƒˆã‚¢ä½¿ç”¨ä¾‹ï¼ˆãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ äºˆæ¸¬ï¼‰
online_response = feature_group.get_record(
    record_identifier_value_as_string='customer_123'
)

# ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ã‚¹ãƒˆã‚¢ä½¿ç”¨ä¾‹ï¼ˆåˆ†æãƒ»è¨“ç·´ï¼‰
query = """
SELECT customer_id, age, income, churn_probability
FROM my_feature_group
WHERE event_time >= '2024-01-01'
"""
training_data = feature_store.athena_query(query)
```

**å®Ÿè£…ã«ãŠã‘ã‚‹æ³¨æ„ç‚¹:**
- åŒã˜ãƒ‡ãƒ¼ã‚¿ãŒä¸¡æ–¹ã«æ ¼ç´ã•ã‚Œã‚‹
- ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã‚¹ãƒˆã‚¢ã¯æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã®ã¿ï¼ˆTTLè¨­å®šå¯èƒ½ï¼‰
- ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ã‚¹ãƒˆã‚¢ã¯å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã™ã¹ã¦ä¿æŒ
</details>

---

## ğŸ”§ å•é¡Œ 7
ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆé™½æ€§ã‚¯ãƒ©ã‚¹2%ã€é™°æ€§ã‚¯ãƒ©ã‚¹98%ï¼‰ã‚’æ‰±ã†éš›ã®é©åˆ‡ãªå‰å‡¦ç†æ‰‹æ³•ã¯ï¼Ÿ

**A)** é™°æ€§ã‚¯ãƒ©ã‚¹ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«å‰Šé™¤ã—ã¦ãƒãƒ©ãƒ³ã‚¹ã‚’å–ã‚‹  
**B)** SMOTEï¼ˆSynthetic Minority Over-sampling Techniqueï¼‰ã‚’é©ç”¨  
**C)** é‡ã¿ä»˜ãæå¤±é–¢æ•°ã‚’ä½¿ç”¨ã—ã€ãƒ‡ãƒ¼ã‚¿ã¯ãã®ã¾ã¾ä¿æŒ  
**D)** B ã¨ C ã®ä¸¡æ–¹ã‚’æ¤œè¨ã—ã€æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã§æ€§èƒ½æ¯”è¼ƒ

<details>
<summary>è§£ç­”ã¨è§£èª¬</summary>

**æ­£è§£: D**

**è§£èª¬:**
ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿å¯¾ç­–ã«ã¯è¤‡æ•°ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒã‚ã‚Šã€**ãƒ‡ãƒ¼ã‚¿ã¨ãƒ“ã‚¸ãƒã‚¹è¦ä»¶ã«å¿œã˜ã¦æœ€é©è§£ãŒç•°ãªã‚‹**ãŸã‚ã€è¤‡æ•°æ‰‹æ³•ã®æ¯”è¼ƒæ¤œè¨ãŒé‡è¦ã§ã™ã€‚

**ä¸»è¦ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒ:**

1. **ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ‰‹æ³•:**
   ```python
   from imblearn.over_sampling import SMOTE
   smote = SMOTE(random_state=42)
   X_resampled, y_resampled = smote.fit_resample(X, y)
   ```

2. **é‡ã¿ä»˜ãå­¦ç¿’:**
   ```python
   from sklearn.ensemble import RandomForestClassifier
   clf = RandomForestClassifier(class_weight='balanced')
   ```

3. **é–¾å€¤èª¿æ•´:**
   ```python
   from sklearn.metrics import precision_recall_curve
   precision, recall, thresholds = precision_recall_curve(y_true, y_scores)
   optimal_threshold = thresholds[np.argmax(precision + recall)]
   ```

**è©•ä¾¡æŒ‡æ¨™ã®é¸æŠ:**
- Accuracy ã¯ä¸é©åˆ‡ï¼ˆ98%ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼‰
- Precision, Recall, F1-score, AUC-PR ã‚’ä½¿ç”¨

**SageMaker ã§ã®å®Ÿè£…ä¾‹:**
```python
# AutoMLã§ã®ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿å‡¦ç†
automl_job = sagemaker.AutoML(
    target_attribute_name='target',
    problem_type='BinaryClassification',
    objective={'MetricName': 'F1'}  # ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿ç”¨ã®æŒ‡æ¨™
)
```
</details>

---

## ğŸ”§ å•é¡Œ 8
SageMaker Processing ã§å¤§é‡ã®ç”»åƒãƒ‡ãƒ¼ã‚¿ï¼ˆ10ä¸‡æšã€å„5MBï¼‰ã‚’å‰å‡¦ç†ã™ã‚‹éš›ã®æ¨å¥¨æ§‹æˆã¯ï¼Ÿ

**A)** CPUæœ€é©åŒ–ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆml.c5.xlargeï¼‰ã‚’è¤‡æ•°ä½¿ç”¨  
**B)** GPUæ­è¼‰ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆml.p3.2xlargeï¼‰ã‚’å˜ä¸€ä½¿ç”¨  
**C)** ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆml.r5.largeï¼‰ã‚’è¤‡æ•°ä½¿ç”¨  
**D)** GPUæ­è¼‰ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆml.p3.xlargeï¼‰ã‚’è¤‡æ•°ä½¿ç”¨

<details>
<summary>è§£ç­”ã¨è§£èª¬</summary>

**æ­£è§£: D**

**è§£èª¬:**
ç”»åƒå‰å‡¦ç†ã®ã‚ˆã†ãª**è¨ˆç®—é›†ç´„çš„ã‚¿ã‚¹ã‚¯**ã«ã¯**GPUä¸¦åˆ—å‡¦ç†**ãŒæœ€é©ã§ã™ã€‚

**ãƒ‡ãƒ¼ã‚¿é‡ã®è¨ˆç®—:**
- 10ä¸‡æš Ã— 5MB = 500GB
- ä¸¦åˆ—å‡¦ç†ãŒå¿…é ˆ

**æ¨å¥¨æ§‹æˆã®ç†ç”±:**
1. **GPUæ´»ç”¨**: ç”»åƒå‡¦ç†ï¼ˆãƒªã‚µã‚¤ã‚ºã€æ­£è¦åŒ–ã€æ‹¡å¼µï¼‰ã¯GPUã§é«˜é€ŸåŒ–
2. **è¤‡æ•°ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹**: ãƒ‡ãƒ¼ã‚¿åˆ†æ•£å‡¦ç†ã§æ™‚é–“çŸ­ç¸®
3. **é©åˆ‡ãªGPUã‚µã‚¤ã‚º**: p3.xlarge ã¯ 1GPUã€ã‚³ã‚¹ãƒˆåŠ¹ç‡ãŒè‰¯ã„

```python
# ç”»åƒå‡¦ç†ç”¨ã®Processorã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
image_processor = PyTorchProcessor(
    framework_version='1.12',
    py_version='py38',
    instance_type='ml.p3.xlarge',
    instance_count=4,  # 4ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã§ä¸¦åˆ—å‡¦ç†
    volume_size_in_gb=200,
    role=role
)

# å‡¦ç†æ™‚é–“ã®æ¨å®š
# CPU: ~20æ™‚é–“, GPU(å˜ä¸€): ~4æ™‚é–“, GPU(ä¸¦åˆ—): ~1æ™‚é–“
```

**ä»–ã®é¸æŠè‚¢ã®å•é¡Œ:**
- A: CPUå‡¦ç†ã¯ç”»åƒå‡¦ç†ã«éåŠ¹ç‡
- B: å˜ä¸€ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã§ã¯ãƒœãƒˆãƒ«ãƒãƒƒã‚¯
- C: ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ã¯ä¸è¦ã€GPUå‡¦ç†èƒ½åŠ›ãŒé‡è¦
</details>

---

## ğŸ”§ å•é¡Œ 9
ä»¥ä¸‹ã®Pythonã‚³ãƒ¼ãƒ‰ã§SageMaker Feature Storeã‹ã‚‰ç‰¹å¾´é‡ã‚’å–å¾—ã—ã¦ã„ã¾ã™ã€‚ã“ã®å®Ÿè£…ã®å•é¡Œç‚¹ã¯ï¼Ÿ

```python
def get_customer_features(customer_ids):
    features = []
    for customer_id in customer_ids:
        response = feature_group.get_record(
            record_identifier_value_as_string=str(customer_id)
        )
        features.append(response.record)
    return features
```

**A)** ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãŒãªã„  
**B)** ãƒ‡ãƒ¼ã‚¿å‹å¤‰æ›ãŒä¸é©åˆ‡  
**C)** ãƒãƒƒãƒå‡¦ç†ã‚’ä½¿ç”¨ã—ã¦ã„ãªã„  
**D)** ä¸Šè¨˜ã™ã¹ã¦

<details>
<summary>è§£ç­”ã¨è§£èª¬</summary>

**æ­£è§£: D**

**è§£èª¬:**
æä¾›ã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰ã«ã¯è¤‡æ•°ã®å•é¡ŒãŒã‚ã‚Šã¾ã™ã€‚

**å•é¡Œç‚¹ã¨æ”¹å–„æ¡ˆ:**

1. **ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä¸è¶³:**
   ```python
   try:
       response = feature_group.get_record(...)
   except ClientError as e:
       if e.response['Error']['Code'] == 'ResourceNotFound':
           # ãƒ¬ã‚³ãƒ¼ãƒ‰ãŒå­˜åœ¨ã—ãªã„å ´åˆã®å‡¦ç†
           return None
       else:
           raise e
   ```

2. **éåŠ¹ç‡ãªé€æ¬¡å‡¦ç†:**
   ```python
   # âŒ æ‚ªã„ä¾‹: ãƒ«ãƒ¼ãƒ—ã§é€æ¬¡å‡¦ç†
   for customer_id in customer_ids:
       response = feature_group.get_record(...)
   
   # âœ… æ”¹å–„æ¡ˆ: ãƒãƒƒãƒå‡¦ç†
   from concurrent.futures import ThreadPoolExecutor
   
   def get_batch_features(customer_ids, batch_size=10):
       with ThreadPoolExecutor(max_workers=batch_size) as executor:
           futures = [executor.submit(get_single_record, cid) 
                     for cid in customer_ids]
           return [f.result() for f in futures]
   ```

3. **ãƒ‡ãƒ¼ã‚¿å‹å¤‰æ›ã®å•é¡Œ:**
   ```python
   # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®é©åˆ‡ãªå‡¦ç†
   def parse_feature_record(record):
       features = {}
       for feature in record:
           feature_name = feature.feature_name
           value = feature.value_as_string
           # é©åˆ‡ãªãƒ‡ãƒ¼ã‚¿å‹ã«å¤‰æ›
           if feature_name in numeric_features:
               features[feature_name] = float(value)
           else:
               features[feature_name] = value
       return features
   ```

**æœ€é©åŒ–ã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰ä¾‹:**
```python
import asyncio
from concurrent.futures import ThreadPoolExecutor
import logging

async def get_customer_features_optimized(customer_ids, max_workers=10):
    async def get_single_feature(customer_id):
        try:
            response = await feature_group.get_record(
                record_identifier_value_as_string=str(customer_id)
            )
            return parse_feature_record(response.record)
        except Exception as e:
            logging.warning(f"Failed to get features for {customer_id}: {e}")
            return None
    
    tasks = [get_single_feature(cid) for cid in customer_ids]
    return await asyncio.gather(*tasks)
```
</details>

---

## ğŸ”§ å•é¡Œ 10
æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã§ã®ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã«ãŠã„ã¦ã€ã€Œãƒ©ã‚°ç‰¹å¾´é‡ã€ã‚’ä½œæˆã™ã‚‹éš›ã®é‡è¦ãªè€ƒæ…®äº‹é …ã¯ï¼Ÿ

**A)** ãƒ©ã‚°ã®æ•°ã¯å¤šã„ã»ã©è‰¯ã„  
**B)** äºˆæ¸¬æ™‚ã«åˆ©ç”¨å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’ä½¿ç”¨  
**C)** æ¬ æå€¤ã¯å˜ç´”ã«å‰Šé™¤  
**D)** ã™ã¹ã¦ã®ãƒ©ã‚°ç‰¹å¾´é‡ã‚’åŒã˜é‡ã¿ã§æ‰±ã†

<details>
<summary>è§£ç­”ã¨è§£èª¬</summary>

**æ­£è§£: B**

**è§£èª¬:**
æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã§ã®ãƒ©ã‚°ç‰¹å¾´é‡ä½œæˆã§ã¯ã€**å®Ÿéš›ã®äºˆæ¸¬æ™‚ã«åˆ©ç”¨å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ã®ã¿**ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ãŒæœ€é‡è¦ã§ã™ã€‚

**é©åˆ‡ãªãƒ©ã‚°ç‰¹å¾´é‡ã®å®Ÿè£…:**
```python
def create_lag_features(df, target_col, lags=[1, 7, 30]):
    """
    é©åˆ‡ãªãƒ©ã‚°ç‰¹å¾´é‡ã®ä½œæˆ
    """
    df_lagged = df.copy()
    
    for lag in lags:
        # lagæ—¥å‰ã®å€¤ã‚’ç‰¹å¾´é‡ã¨ã—ã¦è¿½åŠ 
        df_lagged[f'{target_col}_lag_{lag}'] = df[target_col].shift(lag)
    
    # äºˆæ¸¬æ™‚ã«åˆ©ç”¨ä¸å¯èƒ½ãªæœŸé–“ã‚’å‰Šé™¤
    df_lagged = df_lagged.dropna()
    
    return df_lagged

# âŒ æ‚ªã„ä¾‹: æœªæ¥ã®æƒ…å ±ã‚’å«ã‚€
df['sales_lag_minus1'] = df['sales'].shift(-1)  # 1æ—¥å¾Œã®å£²ä¸Š

# âœ… è‰¯ã„ä¾‹: éå»ã®æƒ…å ±ã®ã¿
df['sales_lag_1'] = df['sales'].shift(1)        # 1æ—¥å‰ã®å£²ä¸Š
df['sales_lag_7'] = df['sales'].shift(7)        # 7æ—¥å‰ã®å£²ä¸Š
```

**ãã®ä»–ã®é‡è¦ãªè€ƒæ…®äº‹é …:**

1. **é©åˆ‡ãªãƒ©ã‚°æ•°ã®é¸æŠ:**
   ```python
   # ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ã«åŸºã¥ãé¸æŠ
   # å°å£²æ¥­: é€±æ¬¡ï¼ˆ7æ—¥ï¼‰ã€æœˆæ¬¡ï¼ˆ30æ—¥ï¼‰ãƒ‘ã‚¿ãƒ¼ãƒ³
   # é‡‘è: æ—¥æ¬¡ã€é€±æ¬¡ãƒ‘ã‚¿ãƒ¼ãƒ³
   ```

2. **æ¬ æå€¤ã®é©åˆ‡ãªå‡¦ç†:**
   ```python
   # forward fillï¼ˆå‰æ–¹è£œå®Œï¼‰
   df['sales_lag_1'] = df['sales'].shift(1).fillna(method='ffill')
   
   # ç§»å‹•å¹³å‡ã§ã®è£œå®Œ
   df['sales_lag_1'] = df['sales'].shift(1).fillna(
       df['sales'].rolling(window=7).mean()
   )
   ```

3. **çµ±è¨ˆçš„ç‰¹å¾´é‡ã®çµ„ã¿åˆã‚ã›:**
   ```python
   # ç§»å‹•å¹³å‡
   df['sales_ma_7'] = df['sales'].rolling(window=7).mean().shift(1)
   
   # ç§»å‹•æ¨™æº–åå·®
   df['sales_std_7'] = df['sales'].rolling(window=7).std().shift(1)
   
   # å­£ç¯€æ€§èª¿æ•´
   df['sales_seasonal'] = df['sales'] / df.groupby(df.index.dayofweek)['sales'].transform('mean')
   ```

**SageMaker ã§ã®æ™‚ç³»åˆ—ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°:**
```python
from sagemaker.tensorflow import TensorFlow

# TensorFlow ã§ã®æ™‚ç³»åˆ—ãƒ¢ãƒ‡ãƒ«
tf_estimator = TensorFlow(
    entry_point='time_series_model.py',
    framework_version='2.8',
    py_version='py39',
    instance_type='ml.m5.large',
    hyperparameters={
        'sequence_length': 30,  # 30æ—¥åˆ†ã®å±¥æ­´ã‚’ä½¿ç”¨
        'prediction_horizon': 7  # 7æ—¥å¾Œã‚’äºˆæ¸¬
    }
)
```
</details>

---

## ğŸ“Š è§£ç­”ä¸€è¦§

| å•é¡Œ | æ­£è§£ | Domain | é‡è¦åº¦ |
|------|------|--------|--------|
| 1 | B | Data Preparation | â­â­â­ |
| 2 | C | Data Preparation | â­â­ |
| 3 | B | Feature Store | â­â­â­ |
| 4 | B | Data Processing | â­â­â­ |
| 5 | B | Data Quality | â­â­â­â­ |
| 6 | B | Feature Store | â­â­â­ |
| 7 | D | Data Preparation | â­â­â­â­ |
| 8 | D | Processing | â­â­ |
| 9 | D | Implementation | â­â­â­ |
| 10 | B | Time Series | â­â­â­â­ |

## ğŸ¯ å­¦ç¿’ã®ãƒã‚¤ãƒ³ãƒˆ

### é«˜å¾—ç‚¹ã®ã‚³ãƒ„
1. **Feature Store ã®ç†è§£**: ã‚ªãƒ³ãƒ©ã‚¤ãƒ³/ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ã‚¹ãƒˆã‚¢ã®ä½¿ã„åˆ†ã‘
2. **ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯é˜²æ­¢**: æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã§ã®æ³¨æ„ç‚¹
3. **ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æˆ¦ç•¥**: å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
4. **å®Ÿè£…è©³ç´°**: SageMaker ã®å…·ä½“çš„ãªè¨­å®šæ–¹æ³•

### å¾©ç¿’ã™ã¹ãé ˜åŸŸ
- **70%æœªæº€ã®å ´åˆ**: åŸºæœ¬æ¦‚å¿µã‹ã‚‰å†å­¦ç¿’
- **70-85%ã®å ´åˆ**: å®Ÿè£…è©³ç´°ã‚’é‡ç‚¹çš„ã«
- **85%ä»¥ä¸Šã®å ´åˆ**: æ¬¡ã®Domainã®å­¦ç¿’ã¸

### æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—
ã“ã®Domain 1ã§8å‰²ä»¥ä¸Šæ­£è§£ã§ããŸã‚‰ã€**Domain 2: ML Model Development** ã®å­¦ç¿’ã«é€²ã‚“ã§ãã ã•ã„ã€‚

---
**æ³¨æ„**: ã“ã®å•é¡Œé›†ã¯MLE-Aè©¦é¨“ã®å‡ºé¡Œå‚¾å‘ã‚’åŸºã«ä½œæˆã•ã‚Œã¦ãŠã‚Šã€å®Ÿéš›ã®è©¦é¨“å•é¡Œã¨ã¯ç•°ãªã‚Šã¾ã™ã€‚AWSå…¬å¼ã®ç·´ç¿’å•é¡Œã‚‚ä½µç”¨ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚