# Lab 4: MLOpsãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

## ðŸŽ¯ å­¦ç¿’ç›®æ¨™

ã“ã®ãƒ©ãƒœã§ã¯ã€MLOpsï¼ˆMachine Learning Operationsï¼‰ã®åŒ…æ‹¬çš„ãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ã—ã€æ©Ÿæ¢°å­¦ç¿’ã®ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«å…¨ä½“ã‚’è‡ªå‹•åŒ–ã—ã¾ã™ï¼š

- SageMaker Pipelines ã«ã‚ˆã‚‹ ML ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼è‡ªå‹•åŒ–
- CI/CD ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®æ§‹ç¯‰
- ãƒ¢ãƒ‡ãƒ«ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã¨ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†
- ãƒ¢ãƒ‡ãƒ«ç›£è¦–ã¨è‡ªå‹•å†å­¦ç¿’
- Infrastructure as Code (IaC) ã®å®Ÿè£…

## ðŸ“‹ å‰ææ¡ä»¶

- AWS CLI ãŒè¨­å®šæ¸ˆã¿
- CodeCommitã€CodeBuildã€CodePipeline ã®åŸºæœ¬çŸ¥è­˜
- [Lab 3: ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ](./lab03-model-deployment.md) ã®å®Œäº†æŽ¨å¥¨

## ðŸ—ï¸ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ¦‚è¦

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        MLOps Pipeline                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚   Source    â”‚    â”‚   Build     â”‚    â”‚   Deploy    â”‚     â”‚
â”‚  â”‚ CodeCommit  â”‚â”€â”€â”€â–¶â”‚ CodeBuild   â”‚â”€â”€â”€â–¶â”‚ CodePipelineâ”‚     â”‚
â”‚  â”‚   GitHub    â”‚    â”‚   Testing   â”‚    â”‚  SageMaker  â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚              SageMaker Pipeline                         â”‚ â”‚
â”‚  â”‚  Data Processing â”€â–¶ Training â”€â–¶ Evaluation â”€â–¶ Deploy  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚        â”‚                       â”‚                             â”‚
â”‚        â–¼                       â–¼                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚  â”‚   Model     â”‚         â”‚   Monitor   â”‚                     â”‚
â”‚  â”‚  Registry   â”‚         â”‚  & Retrain  â”‚                     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ðŸš€ Step 1: SageMaker Pipelines ã®æ§‹ç¯‰

### 1.1 Pipeline å®šç¾©ã¨ã‚¹ãƒ†ãƒƒãƒ—ä½œæˆ

```python
import boto3
import sagemaker
from sagemaker.workflow.pipeline import Pipeline
from sagemaker.workflow.steps import ProcessingStep, TrainingStep
from sagemaker.workflow.step_collections import RegisterModel
from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo
from sagemaker.workflow.condition_step import ConditionStep
from sagemaker.workflow.functions import JsonGet
from sagemaker.workflow.properties import PropertyFile
from sagemaker.workflow.parameters import ParameterString, ParameterFloat

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å®šç¾©
input_data = ParameterString(
    name="InputData",
    default_value="s3://your-bucket/input-data/",
    description="Input data location"
)

model_approval_status = ParameterString(
    name="ModelApprovalStatus", 
    default_value="Approved",
    description="Model approval status"
)

accuracy_threshold = ParameterFloat(
    name="AccuracyThreshold",
    default_value=0.8,
    description="Minimum accuracy for model approval"
)

print("Pipeline parameters defined")
```

### 1.2 ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã‚¹ãƒ†ãƒƒãƒ—

```python
from sagemaker.sklearn.processing import SKLearnProcessor
from sagemaker.processing import ProcessingInput, ProcessingOutput

# å‰å‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
preprocessing_script = """
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import joblib
import argparse
import os

def preprocess_data():
    parser = argparse.ArgumentParser()
    parser.add_argument('--input-data', type=str, required=True)
    args = parser.parse_args()
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    print(f"Loading data from {args.input_data}")
    df = pd.read_csv(f"{args.input_data}/data.csv")
    
    # ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
    df = df.dropna()
    df = df[df['target'].notnull()]
    
    # ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®åˆ†é›¢
    X = df.drop(['target'], axis=1)
    y = df['target']
    
    # è¨“ç·´ãƒ»æ¤œè¨¼ãƒ»ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«åˆ†å‰²
    X_temp, X_test, y_temp, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    X_train, X_val, y_train, y_val = train_test_split(
        X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp
    )
    
    # ç‰¹å¾´é‡æ¨™æº–åŒ–
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_val_scaled = scaler.transform(X_val)
    X_test_scaled = scaler.transform(X_test)
    
    # ãƒ‡ãƒ¼ã‚¿ä¿å­˜
    output_dir = "/opt/ml/processing/output"
    os.makedirs(output_dir, exist_ok=True)
    
    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿
    train_df = pd.DataFrame(X_train_scaled)
    train_df['target'] = y_train.values
    train_df.to_csv(f"{output_dir}/train.csv", index=False)
    
    # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿
    val_df = pd.DataFrame(X_val_scaled)
    val_df['target'] = y_val.values
    val_df.to_csv(f"{output_dir}/validation.csv", index=False)
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
    test_df = pd.DataFrame(X_test_scaled)
    test_df['target'] = y_test.values
    test_df.to_csv(f"{output_dir}/test.csv", index=False)
    
    # ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ä¿å­˜
    joblib.dump(scaler, f"{output_dir}/scaler.pkl")
    
    print("Data preprocessing completed")

if __name__ == "__main__":
    preprocess_data()
"""

# å‰å‡¦ç†ã‚¹ãƒ†ãƒƒãƒ—ã®å®šç¾©
sklearn_processor = SKLearnProcessor(
    framework_version="0.23-1",
    instance_type="ml.m5.large",
    instance_count=1,
    base_job_name="ml-preprocessing",
    role=sagemaker.get_execution_role()
)

processing_step = ProcessingStep(
    name="DataPreprocessing",
    processor=sklearn_processor,
    inputs=[
        ProcessingInput(
            source=input_data,
            destination="/opt/ml/processing/input"
        )
    ],
    outputs=[
        ProcessingOutput(
            output_name="train_data",
            source="/opt/ml/processing/output/train.csv"
        ),
        ProcessingOutput(
            output_name="validation_data", 
            source="/opt/ml/processing/output/validation.csv"
        ),
        ProcessingOutput(
            output_name="test_data",
            source="/opt/ml/processing/output/test.csv"
        ),
        ProcessingOutput(
            output_name="scaler",
            source="/opt/ml/processing/output/scaler.pkl"
        )
    ],
    code="preprocessing.py"
)

print("Data preprocessing step defined")
```

### 1.3 ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—

```python
from sagemaker.sklearn.estimator import SKLearn
from sagemaker.inputs import TrainingInput

# å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
training_script = """
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
import joblib
import json
import argparse
import os

def train_model():
    parser = argparse.ArgumentParser()
    parser.add_argument('--n-estimators', type=int, default=100)
    parser.add_argument('--max-depth', type=int, default=10)
    parser.add_argument('--min-samples-split', type=int, default=2)
    args = parser.parse_args()
    
    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    train_df = pd.read_csv('/opt/ml/input/data/train/train.csv')
    val_df = pd.read_csv('/opt/ml/input/data/validation/validation.csv')
    
    # ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®åˆ†é›¢
    X_train = train_df.drop(['target'], axis=1)
    y_train = train_df['target']
    X_val = val_df.drop(['target'], axis=1)
    y_val = val_df['target']
    
    # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
    model = RandomForestClassifier(
        n_estimators=args.n_estimators,
        max_depth=args.max_depth,
        min_samples_split=args.min_samples_split,
        random_state=42
    )
    
    model.fit(X_train, y_train)
    
    # ãƒ¢ãƒ‡ãƒ«è©•ä¾¡
    train_pred = model.predict(X_train)
    val_pred = model.predict(X_val)
    
    train_accuracy = accuracy_score(y_train, train_pred)
    val_accuracy = accuracy_score(y_val, val_pred)
    
    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä¿å­˜
    metrics = {
        'train_accuracy': float(train_accuracy),
        'validation_accuracy': float(val_accuracy),
        'model_parameters': {
            'n_estimators': args.n_estimators,
            'max_depth': args.max_depth,
            'min_samples_split': args.min_samples_split
        }
    }
    
    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
    model_dir = '/opt/ml/model'
    os.makedirs(model_dir, exist_ok=True)
    joblib.dump(model, f'{model_dir}/model.pkl')
    
    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä¿å­˜
    with open(f'{model_dir}/metrics.json', 'w') as f:
        json.dump(metrics, f)
    
    print(f"Training completed. Validation accuracy: {val_accuracy}")

if __name__ == "__main__":
    train_model()
"""

# å­¦ç¿’ç”¨ Estimator
sklearn_estimator = SKLearn(
    entry_point="training.py",
    framework_version="0.23-1",
    instance_type="ml.m5.large",
    role=sagemaker.get_execution_role(),
    hyperparameters={
        'n-estimators': 100,
        'max-depth': 10
    }
)

# å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—ã®å®šç¾©
training_step = TrainingStep(
    name="ModelTraining",
    estimator=sklearn_estimator,
    inputs={
        "train": TrainingInput(
            s3_data=processing_step.properties.ProcessingOutputConfig.Outputs[
                "train_data"
            ].S3Output.S3Uri
        ),
        "validation": TrainingInput(
            s3_data=processing_step.properties.ProcessingOutputConfig.Outputs[
                "validation_data"
            ].S3Output.S3Uri
        )
    }
)

print("Model training step defined")
```

### 1.4 ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã‚¹ãƒ†ãƒƒãƒ—

```python
# è©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
evaluation_script = """
import pandas as pd
import numpy as np
import joblib
import json
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import argparse

def evaluate_model():
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    test_df = pd.read_csv('/opt/ml/processing/input/test.csv')
    X_test = test_df.drop(['target'], axis=1)
    y_test = test_df['target']
    
    # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
    model = joblib.load('/opt/ml/processing/model/model.pkl')
    
    # äºˆæ¸¬å®Ÿè¡Œ
    y_pred = model.predict(X_test)
    
    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted')
    recall = recall_score(y_test, y_pred, average='weighted')
    f1 = f1_score(y_test, y_pred, average='weighted')
    
    # è©•ä¾¡çµæžœ
    evaluation_result = {
        'classification_metrics': {
            'accuracy': {'value': float(accuracy)},
            'precision': {'value': float(precision)},
            'recall': {'value': float(recall)},
            'f1_score': {'value': float(f1)}
        }
    }
    
    # çµæžœä¿å­˜
    output_dir = "/opt/ml/processing/evaluation"
    os.makedirs(output_dir, exist_ok=True)
    
    with open(f"{output_dir}/evaluation.json", "w") as f:
        json.dump(evaluation_result, f)
    
    print(f"Model evaluation completed. Accuracy: {accuracy}")

if __name__ == "__main__":
    evaluate_model()
"""

# è©•ä¾¡ã‚¹ãƒ†ãƒƒãƒ—ã®å®šç¾©
evaluation_step = ProcessingStep(
    name="ModelEvaluation",
    processor=sklearn_processor,
    inputs=[
        ProcessingInput(
            source=processing_step.properties.ProcessingOutputConfig.Outputs[
                "test_data"
            ].S3Output.S3Uri,
            destination="/opt/ml/processing/input"
        ),
        ProcessingInput(
            source=training_step.properties.ModelArtifacts.S3ModelArtifacts,
            destination="/opt/ml/processing/model"
        )
    ],
    outputs=[
        ProcessingOutput(
            output_name="evaluation",
            source="/opt/ml/processing/evaluation"
        )
    ],
    code="evaluation.py"
)

print("Model evaluation step defined")
```

### 1.5 æ¡ä»¶ä»˜ããƒ¢ãƒ‡ãƒ«ç™»éŒ²

```python
from sagemaker.workflow.step_collections import RegisterModel

# ãƒ¢ãƒ‡ãƒ«ç™»éŒ²ã‚¹ãƒ†ãƒƒãƒ—
register_step = RegisterModel(
    name="RegisterModel",
    estimator=sklearn_estimator,
    model_data=training_step.properties.ModelArtifacts.S3ModelArtifacts,
    content_types=["text/csv"],
    response_types=["text/csv"],
    inference_instances=["ml.t2.medium", "ml.m5.large"],
    transform_instances=["ml.m5.large"],
    model_package_group_name="ml-model-package-group",
    approval_status=model_approval_status
)

# æ¡ä»¶å®šç¾©ï¼ˆç²¾åº¦ã—ãã„å€¤ï¼‰
accuracy_condition = ConditionGreaterThanOrEqualTo(
    left=JsonGet(
        step_name=evaluation_step.name,
        property_file=PropertyFile(
            name="EvaluationReport",
            output_name="evaluation",
            path="evaluation.json"
        ),
        json_path="classification_metrics.accuracy.value"
    ),
    right=accuracy_threshold
)

# æ¡ä»¶ä»˜ãã‚¹ãƒ†ãƒƒãƒ—
condition_step = ConditionStep(
    name="CheckAccuracy",
    conditions=[accuracy_condition],
    if_steps=[register_step],
    else_steps=[]
)

print("Conditional model registration step defined")
```

### 1.6 Pipeline ã®ä½œæˆã¨å®Ÿè¡Œ

```python
# Pipeline ã®å®šç¾©
pipeline = Pipeline(
    name="ml-training-pipeline",
    parameters=[
        input_data,
        model_approval_status,
        accuracy_threshold
    ],
    steps=[
        processing_step,
        training_step,
        evaluation_step,
        condition_step
    ],
    sagemaker_session=sagemaker.Session()
)

# Pipeline ã®ä½œæˆ/æ›´æ–°
pipeline.upsert(role_arn=sagemaker.get_execution_role())

# Pipeline ã®å®Ÿè¡Œ
execution = pipeline.start(
    parameters={
        "InputData": "s3://your-bucket/input-data/",
        "AccuracyThreshold": 0.85
    }
)

print(f"Pipeline execution started: {execution.arn}")

# å®Ÿè¡ŒçŠ¶æ³ã®ç›£è¦–
execution.wait()
print(f"Pipeline execution completed with status: {execution.describe()['PipelineExecutionStatus']}")
```

## ðŸ”„ Step 2: CI/CD ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®æ§‹ç¯‰

### 2.1 CodeCommit ãƒªãƒã‚¸ãƒˆãƒªã®è¨­å®š

```python
import boto3

# CodeCommit ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
codecommit = boto3.client('codecommit')

# ãƒªãƒã‚¸ãƒˆãƒªä½œæˆ
try:
    response = codecommit.create_repository(
        repositoryName='ml-ops-repository',
        repositoryDescription='MLOps pipeline source code'
    )
    repo_url = response['repositoryMetadata']['cloneUrlHttp']
    print(f"Repository created: {repo_url}")
except codecommit.exceptions.RepositoryNameExistsException:
    print("Repository already exists")
```

### 2.2 buildspec.yml ã®ä½œæˆ

```yaml
# buildspec.yml - CodeBuild è¨­å®š
version: 0.2

phases:
  install:
    runtime-versions:
      python: 3.8
    commands:
      - echo Installing dependencies...
      - pip install --upgrade pip
      - pip install boto3 sagemaker scikit-learn pandas numpy

  pre_build:
    commands:
      - echo Logging in to Amazon ECR...
      - aws ecr get-login-password --region $AWS_DEFAULT_REGION | docker login --username AWS --password-stdin $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com

  build:
    commands:
      - echo Build started on `date`
      - echo Running tests...
      - python -m pytest tests/ -v
      - echo Building and running SageMaker Pipeline...
      - python scripts/run_pipeline.py

  post_build:
    commands:
      - echo Build completed on `date`
      - echo Checking pipeline execution status...
      - python scripts/check_pipeline_status.py

artifacts:
  files:
    - '**/*'
  name: ml-ops-artifacts
```

### 2.3 è‡ªå‹•ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ

```python
# tests/test_model.py - ãƒ¢ãƒ‡ãƒ«ã®ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆ
import pytest
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

class TestMLModel:
    def setup_method(self):
        """ãƒ†ã‚¹ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™"""
        np.random.seed(42)
        n_samples = 1000
        n_features = 4
        
        # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        X = np.random.randn(n_samples, n_features)
        y = (X[:, 0] + X[:, 1] > 0).astype(int)
        
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )
    
    def test_model_training(self):
        """ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã®ãƒ†ã‚¹ãƒˆ"""
        model = RandomForestClassifier(n_estimators=10, random_state=42)
        model.fit(self.X_train, self.y_train)
        
        # å­¦ç¿’å¾Œã®ãƒ¢ãƒ‡ãƒ«ãŒäºˆæ¸¬å¯èƒ½ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
        predictions = model.predict(self.X_test)
        assert len(predictions) == len(self.y_test)
        assert all(pred in [0, 1] for pred in predictions)
    
    def test_model_accuracy(self):
        """ãƒ¢ãƒ‡ãƒ«ç²¾åº¦ã®ãƒ†ã‚¹ãƒˆ"""
        model = RandomForestClassifier(n_estimators=50, random_state=42)
        model.fit(self.X_train, self.y_train)
        
        predictions = model.predict(self.X_test)
        accuracy = accuracy_score(self.y_test, predictions)
        
        # æœ€ä½Žé™ã®ç²¾åº¦ã‚’ç¢ºä¿
        assert accuracy > 0.7, f"Model accuracy {accuracy} is below threshold"
    
    def test_data_validation(self):
        """ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã®ãƒ†ã‚¹ãƒˆ"""
        # ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶ã®ç¢ºèª
        assert self.X_train.shape[1] == 4, "Feature count mismatch"
        assert len(self.X_train) > 0, "Training data is empty"
        assert len(self.y_train) == len(self.X_train), "Target length mismatch"
        
        # ãƒ‡ãƒ¼ã‚¿å€¤ã®ç¢ºèª
        assert not np.isnan(self.X_train).any(), "Training data contains NaN"
        assert not np.isnan(self.y_train).any(), "Target data contains NaN"

# tests/test_pipeline.py - ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ãƒ†ã‚¹ãƒˆ
import boto3
import pytest
from moto import mock_s3

class TestPipelineComponents:
    @mock_s3
    def test_s3_data_access(self):
        """S3ãƒ‡ãƒ¼ã‚¿ã‚¢ã‚¯ã‚»ã‚¹ã®ãƒ†ã‚¹ãƒˆ"""
        # Mock S3 setup
        s3 = boto3.client('s3', region_name='us-east-1')
        bucket_name = 'test-ml-bucket'
        s3.create_bucket(Bucket=bucket_name)
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        test_data = "feature1,feature2,target\n1,2,0\n3,4,1\n"
        s3.put_object(Bucket=bucket_name, Key='data/test.csv', Body=test_data)
        
        # ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ†ã‚¹ãƒˆ
        response = s3.get_object(Bucket=bucket_name, Key='data/test.csv')
        content = response['Body'].read().decode('utf-8')
        
        assert 'feature1,feature2,target' in content
        assert len(content.split('\n')) >= 3
```

### 2.4 Pipeline å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ

```python
# scripts/run_pipeline.py - Pipelineå®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ
import boto3
import os
import json
from sagemaker.workflow.pipeline import Pipeline
from sagemaker import get_execution_role

def run_pipeline():
    """SageMaker Pipeline ã‚’å®Ÿè¡Œ"""
    
    # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®šå–å¾—
    bucket_name = os.environ.get('ML_BUCKET', 'default-ml-bucket')
    pipeline_name = os.environ.get('PIPELINE_NAME', 'ml-training-pipeline')
    
    try:
        # SageMaker ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        sagemaker_client = boto3.client('sagemaker')
        
        # Pipeline ã®å­˜åœ¨ç¢ºèª
        try:
            pipeline_desc = sagemaker_client.describe_pipeline(PipelineName=pipeline_name)
            print(f"Pipeline {pipeline_name} found")
        except sagemaker_client.exceptions.ResourceNotFound:
            print(f"Pipeline {pipeline_name} not found. Creating new pipeline...")
            # æ–°ã—ã„ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ä½œæˆãƒ­ã‚¸ãƒƒã‚¯ã‚’ã“ã“ã«è¿½åŠ 
            return
        
        # Pipeline å®Ÿè¡Œ
        response = sagemaker_client.start_pipeline_execution(
            PipelineName=pipeline_name,
            PipelineParameters=[
                {
                    'Name': 'InputData',
                    'Value': f's3://{bucket_name}/input-data/'
                },
                {
                    'Name': 'AccuracyThreshold',
                    'Value': '0.85'
                }
            ]
        )
        
        execution_arn = response['PipelineExecutionArn']
        print(f"Pipeline execution started: {execution_arn}")
        
        # å®Ÿè¡ŒARNã‚’å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        with open('pipeline_execution.json', 'w') as f:
            json.dump({'execution_arn': execution_arn}, f)
        
        return execution_arn
        
    except Exception as e:
        print(f"Error running pipeline: {str(e)}")
        raise

if __name__ == "__main__":
    run_pipeline()
```

### 2.5 CodePipeline ã®è¨­å®š

```python
# Pipeline å®šç¾©ï¼ˆCloudFormation Templateï¼‰
pipeline_template = {
    "AWSTemplateFormatVersion": "2010-09-09",
    "Description": "MLOps CI/CD Pipeline",
    "Resources": {
        "CodePipelineServiceRole": {
            "Type": "AWS::IAM::Role",
            "Properties": {
                "AssumeRolePolicyDocument": {
                    "Version": "2012-10-17",
                    "Statement": [
                        {
                            "Effect": "Allow",
                            "Principal": {"Service": "codepipeline.amazonaws.com"},
                            "Action": "sts:AssumeRole"
                        }
                    ]
                },
                "Policies": [
                    {
                        "PolicyName": "CodePipelinePolicy",
                        "PolicyDocument": {
                            "Version": "2012-10-17",
                            "Statement": [
                                {
                                    "Effect": "Allow",
                                    "Action": [
                                        "codecommit:GetBranch",
                                        "codecommit:GetCommit",
                                        "codebuild:BatchGetBuilds",
                                        "codebuild:StartBuild",
                                        "s3:GetObject",
                                        "s3:PutObject",
                                        "sagemaker:*"
                                    ],
                                    "Resource": "*"
                                }
                            ]
                        }
                    }
                ]
            }
        },
        "MLOpsPipeline": {
            "Type": "AWS::CodePipeline::Pipeline",
            "Properties": {
                "Name": "MLOpsPipeline",
                "RoleArn": {"Fn::GetAtt": ["CodePipelineServiceRole", "Arn"]},
                "ArtifactStore": {
                    "Type": "S3",
                    "Location": {"Ref": "ArtifactBucket"}
                },
                "Stages": [
                    {
                        "Name": "Source",
                        "Actions": [
                            {
                                "Name": "Source",
                                "ActionTypeId": {
                                    "Category": "Source",
                                    "Owner": "AWS",
                                    "Provider": "CodeCommit",
                                    "Version": "1"
                                },
                                "Configuration": {
                                    "RepositoryName": "ml-ops-repository",
                                    "BranchName": "main"
                                },
                                "OutputArtifacts": [{"Name": "SourceOutput"}]
                            }
                        ]
                    },
                    {
                        "Name": "Build",
                        "Actions": [
                            {
                                "Name": "Build",
                                "ActionTypeId": {
                                    "Category": "Build",
                                    "Owner": "AWS", 
                                    "Provider": "CodeBuild",
                                    "Version": "1"
                                },
                                "Configuration": {
                                    "ProjectName": {"Ref": "CodeBuildProject"}
                                },
                                "InputArtifacts": [{"Name": "SourceOutput"}],
                                "OutputArtifacts": [{"Name": "BuildOutput"}]
                            }
                        ]
                    }
                ]
            }
        }
    }
}
```

## ðŸ“Š Step 3: ãƒ¢ãƒ‡ãƒ«ç›£è¦–ã¨è‡ªå‹•å†å­¦ç¿’

### 3.1 Model Monitor ã®è¨­å®š

```python
from sagemaker.model_monitor import DefaultModelMonitor
from sagemaker.model_monitor.dataset_format import DatasetFormat

def setup_model_monitoring(endpoint_name, baseline_data_uri):
    """ãƒ¢ãƒ‡ãƒ«ç›£è¦–ã®è¨­å®š"""
    
    # Model Monitor ã®ä½œæˆ
    model_monitor = DefaultModelMonitor(
        role=get_execution_role(),
        instance_count=1,
        instance_type='ml.m5.xlarge',
        volume_size_in_gb=20,
        max_runtime_in_seconds=3600
    )
    
    # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ä½œæˆ
    baseline_job = model_monitor.suggest_baseline(
        baseline_dataset=baseline_data_uri,
        dataset_format=DatasetFormat.csv(header=True),
        output_s3_uri='s3://your-bucket/baseline-results/'
    )
    
    # ç›£è¦–ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ä½œæˆ
    monitor_schedule = model_monitor.create_monitoring_schedule(
        monitor_schedule_name=f'{endpoint_name}-monitor',
        endpoint_input=endpoint_name,
        output_s3_uri='s3://your-bucket/monitoring-results/',
        statistics=baseline_job.baseline_statistics(),
        constraints=baseline_job.suggested_constraints(),
        schedule_cron_expression='cron(0 * * * * ?)',  # æ¯Žæ™‚å®Ÿè¡Œ
        enable_cloudwatch_metrics=True
    )
    
    return monitor_schedule

# ç›£è¦–è¨­å®šã®å®Ÿè¡Œ
endpoint_name = 'ml-production-endpoint'
baseline_data = 's3://your-bucket/baseline/baseline.csv'
monitor_schedule = setup_model_monitoring(endpoint_name, baseline_data)
print(f"Model monitoring configured: {monitor_schedule.monitor_schedule_name}")
```

### 3.2 ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡ºã¨ã‚¢ãƒ©ãƒ¼ãƒˆ

```python
import boto3

def setup_drift_alerts(monitor_schedule_name):
    """ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡ºã‚¢ãƒ©ãƒ¼ãƒˆã®è¨­å®š"""
    
    cloudwatch = boto3.client('cloudwatch')
    sns = boto3.client('sns')
    
    # SNS ãƒˆãƒ”ãƒƒã‚¯ä½œæˆ
    topic_response = sns.create_topic(Name='model-drift-alerts')
    topic_arn = topic_response['TopicArn']
    
    # CloudWatch ã‚¢ãƒ©ãƒ¼ãƒ ä½œæˆ
    cloudwatch.put_metric_alarm(
        AlarmName='ModelDriftAlert',
        ComparisonOperator='GreaterThanThreshold',
        EvaluationPeriods=1,
        MetricName='feature_baseline_drift_distance',
        Namespace='aws/sagemaker/Endpoints/data-metrics',
        Period=3600,  # 1æ™‚é–“
        Statistic='Average',
        Threshold=0.1,  # ãƒ‰ãƒªãƒ•ãƒˆé–¾å€¤
        ActionsEnabled=True,
        AlarmActions=[topic_arn],
        AlarmDescription='Model data drift detected',
        Dimensions=[
            {
                'Name': 'MonitoringSchedule',
                'Value': monitor_schedule_name
            }
        ]
    )
    
    print(f"Drift alert configured with topic: {topic_arn}")
    return topic_arn

# ã‚¢ãƒ©ãƒ¼ãƒˆè¨­å®š
topic_arn = setup_drift_alerts(monitor_schedule.monitor_schedule_name)
```

### 3.3 è‡ªå‹•å†å­¦ç¿’ã®å®Ÿè£…

```python
import json

def create_retraining_lambda():
    """è‡ªå‹•å†å­¦ç¿’ Lambda é–¢æ•°"""
    
    lambda_code = """
import json
import boto3
import os

def lambda_handler(event, context):
    try:
        # SNS ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰ã‚¢ãƒ©ãƒ¼ãƒˆæƒ…å ±ã‚’å–å¾—
        message = json.loads(event['Records'][0]['Sns']['Message'])
        alarm_name = message['AlarmName']
        
        if 'ModelDriftAlert' in alarm_name:
            # SageMaker Pipeline ã‚’å®Ÿè¡Œã—ã¦å†å­¦ç¿’
            sagemaker_client = boto3.client('sagemaker')
            
            pipeline_name = os.environ['PIPELINE_NAME']
            
            response = sagemaker_client.start_pipeline_execution(
                PipelineName=pipeline_name,
                PipelineParameters=[
                    {
                        'Name': 'InputData',
                        'Value': os.environ['RETRAIN_DATA_PATH']
                    },
                    {
                        'Name': 'AccuracyThreshold',
                        'Value': '0.85'
                    }
                ]
            )
            
            execution_arn = response['PipelineExecutionArn']
            
            # å®Ÿè¡Œé€šçŸ¥
            sns = boto3.client('sns')
            sns.publish(
                TopicArn=os.environ['NOTIFICATION_TOPIC'],
                Subject='Model Retraining Started',
                Message=f'Automatic retraining triggered due to drift detection.\\nExecution ARN: {execution_arn}'
            )
            
            return {
                'statusCode': 200,
                'body': json.dumps({
                    'message': 'Retraining pipeline started',
                    'execution_arn': execution_arn
                })
            }
        
    except Exception as e:
        print(f"Error: {str(e)}")
        return {
            'statusCode': 500,
            'body': json.dumps({'error': str(e)})
        }
    """
    
    return lambda_code

# Lambda é–¢æ•°ãƒ‡ãƒ—ãƒ­ã‚¤ï¼ˆCloudFormation/CDKä½¿ç”¨æŽ¨å¥¨ï¼‰
print("Lambda function code generated for automatic retraining")
```

## ðŸ—ï¸ Step 4: Infrastructure as Code

### 4.1 CloudFormation ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ

```yaml
# mlops-infrastructure.yaml
AWSTemplateFormatVersion: '2010-09-09'
Description: 'MLOps Infrastructure Stack'

Parameters:
  ProjectName:
    Type: String
    Default: 'mlops-project'
    Description: 'Name of the MLOps project'

Resources:
  # S3 Bucket for ML artifacts
  MLArtifactsBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Sub '${ProjectName}-ml-artifacts-${AWS::AccountId}'
      VersioningConfiguration:
        Status: Enabled
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true

  # IAM Role for SageMaker
  SageMakerExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub '${ProjectName}-SageMakerRole'
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: sagemaker.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
      Policies:
        - PolicyName: S3Access
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - s3:GetObject
                  - s3:PutObject
                  - s3:DeleteObject
                  - s3:ListBucket
                Resource:
                  - !Sub '${MLArtifactsBucket}/*'
                  - !GetAtt MLArtifactsBucket.Arn

  # CodeCommit Repository
  MLOpsRepository:
    Type: AWS::CodeCommit::Repository
    Properties:
      RepositoryName: !Sub '${ProjectName}-repository'
      RepositoryDescription: 'MLOps source code repository'

  # CodeBuild Project
  MLOpsBuildProject:
    Type: AWS::CodeBuild::Project
    Properties:
      Name: !Sub '${ProjectName}-build'
      ServiceRole: !GetAtt CodeBuildRole.Arn
      Artifacts:
        Type: CODEPIPELINE
      Environment:
        Type: LINUX_CONTAINER
        ComputeType: BUILD_GENERAL1_MEDIUM
        Image: aws/codebuild/amazonlinux2-x86_64-standard:3.0
        EnvironmentVariables:
          - Name: ML_BUCKET
            Value: !Ref MLArtifactsBucket
          - Name: SAGEMAKER_ROLE
            Value: !GetAtt SageMakerExecutionRole.Arn
      Source:
        Type: CODEPIPELINE
        BuildSpec: |
          version: 0.2
          phases:
            install:
              runtime-versions:
                python: 3.8
              commands:
                - pip install boto3 sagemaker scikit-learn pandas numpy pytest
            build:
              commands:
                - python -m pytest tests/ -v
                - python scripts/run_pipeline.py
          artifacts:
            files:
              - '**/*'

Outputs:
  ArtifactsBucket:
    Description: 'S3 bucket for ML artifacts'
    Value: !Ref MLArtifactsBucket
    Export:
      Name: !Sub '${ProjectName}-ArtifactsBucket'
  
  SageMakerRole:
    Description: 'SageMaker execution role ARN'
    Value: !GetAtt SageMakerExecutionRole.Arn
    Export:
      Name: !Sub '${ProjectName}-SageMakerRole'
```

### 4.2 ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆè‡ªå‹•åŒ–

```bash
#!/bin/bash
# deploy.sh - ã‚¤ãƒ³ãƒ•ãƒ©ã‚¹ãƒˆãƒ©ã‚¯ãƒãƒ£ãƒ‡ãƒ—ãƒ­ã‚¤ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

PROJECT_NAME="mlops-project"
REGION="us-east-1"
STACK_NAME="${PROJECT_NAME}-infrastructure"

echo "Deploying MLOps infrastructure..."

# CloudFormation ã‚¹ã‚¿ãƒƒã‚¯ã®ãƒ‡ãƒ—ãƒ­ã‚¤
aws cloudformation deploy \
  --template-file mlops-infrastructure.yaml \
  --stack-name $STACK_NAME \
  --parameter-overrides ProjectName=$PROJECT_NAME \
  --capabilities CAPABILITY_NAMED_IAM \
  --region $REGION

if [ $? -eq 0 ]; then
    echo "Infrastructure deployment completed successfully"
    
    # å‡ºåŠ›å€¤ã‚’å–å¾—
    BUCKET_NAME=$(aws cloudformation describe-stacks \
      --stack-name $STACK_NAME \
      --region $REGION \
      --query 'Stacks[0].Outputs[?OutputKey==`ArtifactsBucket`].OutputValue' \
      --output text)
    
    SAGEMAKER_ROLE=$(aws cloudformation describe-stacks \
      --stack-name $STACK_NAME \
      --region $REGION \
      --query 'Stacks[0].Outputs[?OutputKey==`SageMakerRole`].OutputValue' \
      --output text)
    
    echo "Artifacts bucket: $BUCKET_NAME"
    echo "SageMaker role: $SAGEMAKER_ROLE"
    
    # ç’°å¢ƒå¤‰æ•°ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
    cat > .env << EOF
ML_BUCKET=$BUCKET_NAME
SAGEMAKER_ROLE=$SAGEMAKER_ROLE
REGION=$REGION
EOF
    
    echo "Environment variables saved to .env file"
else
    echo "Infrastructure deployment failed"
    exit 1
fi
```

## ðŸ§¹ Step 5: ãƒªã‚½ãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—

### 5.1 Pipeline ã¨ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—

```python
def cleanup_mlops_resources():
    """MLOps ãƒªã‚½ãƒ¼ã‚¹ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
    
    sagemaker_client = boto3.client('sagemaker')
    
    try:
        # Pipeline å‰Šé™¤
        pipeline_name = 'ml-training-pipeline'
        sagemaker_client.delete_pipeline(PipelineName=pipeline_name)
        print(f"Pipeline {pipeline_name} deleted")
        
        # ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å‰Šé™¤
        monitor_schedule_name = 'ml-production-endpoint-monitor'
        sagemaker_client.delete_monitoring_schedule(
            MonitoringScheduleName=monitor_schedule_name
        )
        print(f"Monitoring schedule {monitor_schedule_name} deleted")
        
        # ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆå‰Šé™¤
        endpoint_name = 'ml-production-endpoint'
        sagemaker_client.delete_endpoint(EndpointName=endpoint_name)
        print(f"Endpoint {endpoint_name} deleted")
        
    except Exception as e:
        print(f"Error during cleanup: {str(e)}")

# ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Ÿè¡Œ
cleanup_mlops_resources()
```

### 5.2 CloudFormation ã‚¹ã‚¿ãƒƒã‚¯å‰Šé™¤

```bash
#!/bin/bash
# cleanup.sh - ãƒªã‚½ãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

PROJECT_NAME="mlops-project"
REGION="us-east-1"
STACK_NAME="${PROJECT_NAME}-infrastructure"

echo "Cleaning up MLOps infrastructure..."

# S3 ãƒã‚±ãƒƒãƒˆã®å†…å®¹ã‚’å‰Šé™¤
BUCKET_NAME=$(aws cloudformation describe-stacks \
  --stack-name $STACK_NAME \
  --region $REGION \
  --query 'Stacks[0].Outputs[?OutputKey==`ArtifactsBucket`].OutputValue' \
  --output text)

if [ ! -z "$BUCKET_NAME" ]; then
    echo "Emptying S3 bucket: $BUCKET_NAME"
    aws s3 rm s3://$BUCKET_NAME --recursive
fi

# CloudFormation ã‚¹ã‚¿ãƒƒã‚¯å‰Šé™¤
aws cloudformation delete-stack \
  --stack-name $STACK_NAME \
  --region $REGION

echo "CloudFormation stack deletion initiated"
echo "Cleanup completed"
```

## ðŸ’° ã‚³ã‚¹ãƒˆè¨ˆç®—

### æŽ¨å®šã‚³ã‚¹ãƒˆï¼ˆ1æ—¥ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œï¼‰
- **SageMaker Pipelineå®Ÿè¡Œ**: $2.00
- **Model Monitor**: $1.50
- **CodeBuild**: $0.50
- **S3ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸**: $0.30
- **ãã®ä»–AWSã‚µãƒ¼ãƒ“ã‚¹**: $0.70
- **åˆè¨ˆ**: ç´„ $5.00/æ—¥

## ðŸ“š å­¦ç¿’ãƒã‚¤ãƒ³ãƒˆ

### é‡è¦ãªæ¦‚å¿µ
1. **MLOps Pipeline**: ãƒ‡ãƒ¼ã‚¿å‡¦ç†â†’å­¦ç¿’â†’è©•ä¾¡â†’ãƒ‡ãƒ—ãƒ­ã‚¤ã®è‡ªå‹•åŒ–
2. **CI/CD Integration**: ã‚³ãƒ¼ãƒ‰å¤‰æ›´ã‹ã‚‰ãƒ‡ãƒ—ãƒ­ã‚¤ã¾ã§ã®è‡ªå‹•åŒ–
3. **Model Monitoring**: ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡ºã¨è‡ªå‹•å†å­¦ç¿’
4. **Infrastructure as Code**: å†ç¾å¯èƒ½ãªã‚¤ãƒ³ãƒ•ãƒ©ç®¡ç†
5. **Cost Optimization**: ãƒªã‚½ãƒ¼ã‚¹åŠ¹çŽ‡åŒ–ã¨ã‚³ã‚¹ãƒˆç®¡ç†

### å®Ÿè·µçš„ãªã‚¹ã‚­ãƒ«
- SageMaker Pipelines ã®è¨­è¨ˆãƒ»å®Ÿè£…
- CI/CD ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®æ§‹ç¯‰
- ãƒ¢ãƒ‡ãƒ«ç›£è¦–ã¨ã‚¢ãƒ©ãƒ¼ãƒˆè¨­å®š
- è‡ªå‹•åŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ä½œæˆ
- Infrastructure as Code ã®å®Ÿè£…

---

**æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—**: [Lab 5: ãƒ¢ãƒ‡ãƒ«ç›£è¦–](./lab05-monitoring.md) ã§ã¯ã€ã‚ˆã‚Šè©³ç´°ãªãƒ¢ãƒ‡ãƒ«ç›£è¦–ã¨é‹ç”¨ã«ã¤ã„ã¦å­¦ç¿’ã—ã¾ã™ã€‚