# Lab 5: ãƒ¢ãƒ‡ãƒ«ç›£è¦–ã¨é‹ç”¨

## ğŸ¯ å­¦ç¿’ç›®æ¨™

ã“ã®ãƒ©ãƒœã§ã¯ã€æœ¬ç•ªç’°å¢ƒã§ã®MLãƒ¢ãƒ‡ãƒ«ã®åŒ…æ‹¬çš„ãªç›£è¦–ã¨é‹ç”¨ç®¡ç†ã‚’å­¦ç¿’ã—ã¾ã™ï¼š

- SageMaker Model Monitor ã«ã‚ˆã‚‹è©³ç´°ç›£è¦–
- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç›£è¦–ã¨ã‚¢ãƒ©ãƒ¼ãƒˆ
- ãƒ‡ãƒ¼ã‚¿å“è³ªã¨ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã®ç¶™ç¶šçš„è©•ä¾¡
- A/B ãƒ†ã‚¹ãƒˆã¨æ®µéšçš„ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ
- ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆå¯¾å¿œã¨è‡ªå‹•å¾©æ—§

## ğŸ“‹ å‰ææ¡ä»¶

- AWS CLI ãŒè¨­å®šæ¸ˆã¿
- CloudWatchã€SNS ã®åŸºæœ¬çŸ¥è­˜
- [Lab 4: MLOpsãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³](./lab04-mlops-pipeline.md) ã®å®Œäº†æ¨å¥¨

## ğŸ—ï¸ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ¦‚è¦

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      ãƒ¢ãƒ‡ãƒ«ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚   Data      â”‚    â”‚   Model     â”‚    â”‚ Performance â”‚     â”‚
â”‚  â”‚  Quality    â”‚    â”‚  Monitor    â”‚    â”‚   Monitor   â”‚     â”‚
â”‚  â”‚  Monitor    â”‚    â”‚             â”‚    â”‚             â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚        â”‚                  â”‚                  â”‚             â”‚
â”‚        â–¼                  â–¼                  â–¼             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                 CloudWatch                              â”‚ â”‚
â”‚  â”‚            Metrics & Alarms                             â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚        â”‚                       â”‚                             â”‚
â”‚        â–¼                       â–¼                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚  â”‚  Real-time  â”‚         â”‚  Automated  â”‚                     â”‚
â”‚  â”‚   Alerts    â”‚         â”‚  Response   â”‚                     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Step 1: åŒ…æ‹¬çš„ãªãƒ¢ãƒ‡ãƒ«ç›£è¦–è¨­å®š

### 1.1 ãƒ‡ãƒ¼ã‚¿å“è³ªç›£è¦–ã®å®Ÿè£…

```python
import boto3
import sagemaker
from sagemaker.model_monitor import DefaultModelMonitor, ModelQualityMonitor
from sagemaker.model_monitor.dataset_format import DatasetFormat
import pandas as pd
import numpy as np

# SageMaker ã‚»ãƒƒã‚·ãƒ§ãƒ³è¨­å®š
sagemaker_session = sagemaker.Session()
role = sagemaker.get_execution_role()

# ãƒ‡ãƒ¼ã‚¿å“è³ªç›£è¦–ã®è¨­å®š
def setup_data_quality_monitoring(endpoint_name, baseline_data_uri):
    """ãƒ‡ãƒ¼ã‚¿å“è³ªç›£è¦–ã®è©³ç´°è¨­å®š"""
    
    # Data Quality Monitor ã®ä½œæˆ
    data_quality_monitor = DefaultModelMonitor(
        role=role,
        instance_count=1,
        instance_type='ml.m5.xlarge',
        volume_size_in_gb=20,
        max_runtime_in_seconds=3600,
        env={
            'publish_cloudwatch_metrics': 'Enabled',
            'dataset_format': 'json'
        }
    )
    
    # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ä½œæˆ
    baseline_job = data_quality_monitor.suggest_baseline(
        baseline_dataset=baseline_data_uri,
        dataset_format=DatasetFormat.csv(header=True),
        output_s3_uri=f's3://your-bucket/baseline-results/{endpoint_name}/',
        wait=True
    )
    
    print(f"Baseline job completed: {baseline_job.baseline_job_name}")
    
    # ã‚«ã‚¹ã‚¿ãƒ åˆ¶ç´„ã®è¿½åŠ 
    custom_constraints = {
        "features": [
            {
                "name": "feature_1",
                "inferred_type": "Fractional",
                "completeness": {"must_be_present": True},
                "numerical_statistics": {
                    "min": {"must_be_less_than": 10.0},
                    "max": {"must_be_greater_than": -10.0}
                }
            },
            {
                "name": "feature_2", 
                "inferred_type": "Fractional",
                "completeness": {"must_be_present": True},
                "numerical_statistics": {
                    "std_dev": {"must_be_less_than": 5.0}
                }
            }
        ],
        "dataset_statistics": {
            "num_rows": {"must_be_greater_than": 100}
        }
    }
    
    # ç›£è¦–ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ä½œæˆ
    monitor_schedule = data_quality_monitor.create_monitoring_schedule(
        monitor_schedule_name=f'{endpoint_name}-data-quality',
        endpoint_input=endpoint_name,
        output_s3_uri=f's3://your-bucket/monitoring-results/{endpoint_name}/',
        statistics=baseline_job.baseline_statistics(),
        constraints=baseline_job.suggested_constraints(),
        schedule_cron_expression='cron(0 */6 * * * ?)',  # 6æ™‚é–“æ¯
        enable_cloudwatch_metrics=True
    )
    
    return monitor_schedule, baseline_job

# å®Ÿè¡Œ
endpoint_name = 'ml-production-endpoint'
baseline_data = 's3://your-bucket/baseline/baseline.csv'
data_monitor, baseline = setup_data_quality_monitoring(endpoint_name, baseline_data)
print(f"Data quality monitoring configured: {data_monitor.monitor_schedule_name}")
```

### 1.2 ãƒ¢ãƒ‡ãƒ«å“è³ªç›£è¦–ã®å®Ÿè£…

```python
def setup_model_quality_monitoring(endpoint_name, ground_truth_data_uri):
    """ãƒ¢ãƒ‡ãƒ«å“è³ªç›£è¦–ã®è¨­å®š"""
    
    # Model Quality Monitor ã®ä½œæˆ
    model_quality_monitor = ModelQualityMonitor(
        role=role,
        instance_count=1,
        instance_type='ml.m5.xlarge',
        volume_size_in_gb=20,
        max_runtime_in_seconds=3600,
        env={
            'publish_cloudwatch_metrics': 'Enabled'
        }
    )
    
    # å•é¡Œã‚¿ã‚¤ãƒ—è¨­å®šï¼ˆåˆ†é¡ã®å ´åˆï¼‰
    problem_type = "BinaryClassification"  # ã¾ãŸã¯ "MulticlassClassification", "Regression"
    
    # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ä½œæˆ
    baseline_job = model_quality_monitor.suggest_baseline(
        baseline_dataset=ground_truth_data_uri,
        dataset_format=DatasetFormat.csv(header=True),
        problem_type=problem_type,
        inference_attribute="prediction",
        probability_attribute="probability",
        ground_truth_attribute="ground_truth",
        output_s3_uri=f's3://your-bucket/model-quality-baseline/{endpoint_name}/',
        wait=True
    )
    
    # ç›£è¦–ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ä½œæˆ
    monitor_schedule = model_quality_monitor.create_monitoring_schedule(
        monitor_schedule_name=f'{endpoint_name}-model-quality',
        endpoint_input=endpoint_name,
        ground_truth_input=ground_truth_data_uri,
        problem_type=problem_type,
        inference_attribute="prediction",
        probability_attribute="probability", 
        ground_truth_attribute="ground_truth",
        output_s3_uri=f's3://your-bucket/model-quality-results/{endpoint_name}/',
        statistics=baseline_job.baseline_statistics(),
        constraints=baseline_job.suggested_constraints(),
        schedule_cron_expression='cron(0 0 * * * ?)',  # æ¯æ—¥å®Ÿè¡Œ
        enable_cloudwatch_metrics=True
    )
    
    return monitor_schedule, baseline_job

# Ground Truthãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ï¼ˆå®Ÿéš›ã®äºˆæ¸¬çµæœã¨æ­£è§£ãƒ©ãƒ™ãƒ«ï¼‰
ground_truth_data = 's3://your-bucket/ground-truth/ground_truth.csv'
model_quality_monitor, quality_baseline = setup_model_quality_monitoring(
    endpoint_name, ground_truth_data
)
print(f"Model quality monitoring configured: {model_quality_monitor.monitor_schedule_name}")
```

### 1.3 ãƒã‚¤ã‚¢ã‚¹ç›£è¦–ã®å®Ÿè£…

```python
from sagemaker.clarify import BiasConfig, DataConfig, ModelConfig
from sagemaker.model_monitor import BiasMonitor

def setup_bias_monitoring(endpoint_name, bias_analysis_config):
    """ãƒã‚¤ã‚¢ã‚¹ç›£è¦–ã®è¨­å®š"""
    
    # Bias Monitor ã®ä½œæˆ
    bias_monitor = BiasMonitor(
        role=role,
        instance_count=1,
        instance_type='ml.m5.xlarge',
        volume_size_in_gb=20,
        max_runtime_in_seconds=3600
    )
    
    # ãƒ‡ãƒ¼ã‚¿è¨­å®š
    data_config = DataConfig(
        s3_data_input_path=bias_analysis_config['data_path'],
        s3_output_path=f's3://your-bucket/bias-analysis/{endpoint_name}/',
        label=bias_analysis_config['label_column'],
        headers=bias_analysis_config['headers'],
        dataset_type='text/csv'
    )
    
    # ãƒã‚¤ã‚¢ã‚¹è¨­å®š
    bias_config = BiasConfig(
        label_values_or_threshold=bias_analysis_config['positive_label'],
        facet_name=bias_analysis_config['sensitive_feature'],
        facet_values_or_threshold=bias_analysis_config['sensitive_values']
    )
    
    # ãƒ¢ãƒ‡ãƒ«è¨­å®š
    model_config = ModelConfig(
        model_name=bias_analysis_config['model_name'],
        instance_type='ml.m5.large',
        instance_count=1,
        accept_type='text/csv'
    )
    
    # ãƒã‚¤ã‚¢ã‚¹åˆ†æå®Ÿè¡Œ
    bias_job = bias_monitor.run_bias_analysis(
        data_config=data_config,
        bias_config=bias_config,
        model_config=model_config,
        job_name=f'{endpoint_name}-bias-analysis',
        wait=True
    )
    
    # ç¶™ç¶šçš„ãƒã‚¤ã‚¢ã‚¹ç›£è¦–ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«
    monitor_schedule = bias_monitor.create_monitoring_schedule(
        monitor_schedule_name=f'{endpoint_name}-bias-monitor',
        endpoint_input=endpoint_name,
        ground_truth_input=bias_analysis_config['data_path'],
        analysis_config=bias_job.latest_job.describe()['ProcessingInputs'][0],
        output_s3_uri=f's3://your-bucket/bias-monitoring/{endpoint_name}/',
        schedule_cron_expression='cron(0 0 * * MON ?)',  # æ¯é€±æœˆæ›œæ—¥
        enable_cloudwatch_metrics=True
    )
    
    return monitor_schedule, bias_job

# ãƒã‚¤ã‚¢ã‚¹åˆ†æè¨­å®š
bias_config = {
    'data_path': 's3://your-bucket/bias-analysis-data/data.csv',
    'label_column': 'approved',
    'headers': ['age', 'income', 'education', 'gender', 'approved'],
    'positive_label': [1],
    'sensitive_feature': 'gender',
    'sensitive_values': ['female'],
    'model_name': 'ml-production-model'
}

bias_monitor, bias_analysis = setup_bias_monitoring(endpoint_name, bias_config)
print(f"Bias monitoring configured: {bias_monitor.monitor_schedule_name}")
```

## ğŸ“Š Step 2: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç›£è¦–

### 2.1 ã‚«ã‚¹ã‚¿ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä½œæˆ

```python
import boto3
from datetime import datetime, timedelta

def create_custom_metrics(endpoint_name):
    """ã‚«ã‚¹ã‚¿ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ä½œæˆã¨è¨­å®š"""
    
    cloudwatch = boto3.client('cloudwatch')
    
    # ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®å–å¾—
    def get_endpoint_metrics(metric_name, start_time, end_time):
        response = cloudwatch.get_metric_statistics(
            Namespace='AWS/SageMaker',
            MetricName=metric_name,
            Dimensions=[
                {
                    'Name': 'EndpointName',
                    'Value': endpoint_name
                }
            ],
            StartTime=start_time,
            EndTime=end_time,
            Period=300,  # 5åˆ†é–“éš”
            Statistics=['Average', 'Maximum', 'Sum']
        )
        return response['Datapoints']
    
    # ã‚«ã‚¹ã‚¿ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¨ˆç®—ã¨é€ä¿¡
    def calculate_and_send_custom_metrics():
        end_time = datetime.utcnow()
        start_time = end_time - timedelta(minutes=5)
        
        # ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—
        model_latency = get_endpoint_metrics('ModelLatency', start_time, end_time)
        overhead_latency = get_endpoint_metrics('OverheadLatency', start_time, end_time)
        
        if model_latency and overhead_latency:
            # ç·ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·è¨ˆç®—
            total_latency = model_latency[-1]['Average'] + overhead_latency[-1]['Average']
            
            # ã‚«ã‚¹ã‚¿ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹é€ä¿¡
            cloudwatch.put_metric_data(
                Namespace='Custom/SageMaker',
                MetricData=[
                    {
                        'MetricName': 'TotalLatency',
                        'Dimensions': [
                            {
                                'Name': 'EndpointName',
                                'Value': endpoint_name
                            }
                        ],
                        'Value': total_latency,
                        'Unit': 'Milliseconds',
                        'Timestamp': datetime.utcnow()
                    }
                ]
            )
        
        # ã‚¨ãƒ©ãƒ¼ç‡ã®è¨ˆç®—
        invocations = get_endpoint_metrics('Invocations', start_time, end_time)
        invocation_errors = get_endpoint_metrics('InvocationErrors', start_time, end_time)
        
        if invocations and invocation_errors:
            error_rate = (invocation_errors[-1]['Sum'] / invocations[-1]['Sum']) * 100
            
            cloudwatch.put_metric_data(
                Namespace='Custom/SageMaker',
                MetricData=[
                    {
                        'MetricName': 'ErrorRate',
                        'Dimensions': [
                            {
                                'Name': 'EndpointName',
                                'Value': endpoint_name
                            }
                        ],
                        'Value': error_rate,
                        'Unit': 'Percent',
                        'Timestamp': datetime.utcnow()
                    }
                ]
            )
        
        print(f"Custom metrics updated for {endpoint_name}")
    
    return calculate_and_send_custom_metrics

# ã‚«ã‚¹ã‚¿ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨­å®š
custom_metrics_func = create_custom_metrics(endpoint_name)
custom_metrics_func()
```

### 2.2 ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ä½œæˆ

```python
def create_monitoring_dashboard(endpoint_name):
    """CloudWatch ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã®ä½œæˆ"""
    
    cloudwatch = boto3.client('cloudwatch')
    
    dashboard_body = {
        "widgets": [
            {
                "type": "metric",
                "x": 0,
                "y": 0,
                "width": 12,
                "height": 6,
                "properties": {
                    "metrics": [
                        ["AWS/SageMaker", "Invocations", "EndpointName", endpoint_name],
                        [".", "InvocationErrors", ".", "."],
                        ["Custom/SageMaker", "ErrorRate", ".", "."]
                    ],
                    "period": 300,
                    "stat": "Sum",
                    "region": "us-east-1",
                    "title": "Endpoint Invocations and Errors",
                    "yAxis": {
                        "left": {
                            "min": 0
                        }
                    }
                }
            },
            {
                "type": "metric",
                "x": 12,
                "y": 0,
                "width": 12,
                "height": 6,
                "properties": {
                    "metrics": [
                        ["AWS/SageMaker", "ModelLatency", "EndpointName", endpoint_name],
                        [".", "OverheadLatency", ".", "."],
                        ["Custom/SageMaker", "TotalLatency", ".", "."]
                    ],
                    "period": 300,
                    "stat": "Average",
                    "region": "us-east-1",
                    "title": "Latency Metrics",
                    "yAxis": {
                        "left": {
                            "min": 0
                        }
                    }
                }
            },
            {
                "type": "metric",
                "x": 0,
                "y": 6,
                "width": 24,
                "height": 6,
                "properties": {
                    "metrics": [
                        ["aws/sagemaker/Endpoints/data-metrics", "feature_baseline_drift_distance", "MonitoringSchedule", f"{endpoint_name}-data-quality"],
                        ["aws/sagemaker/Endpoints/model-metrics", "accuracy", "MonitoringSchedule", f"{endpoint_name}-model-quality"]
                    ],
                    "period": 3600,
                    "stat": "Average",
                    "region": "us-east-1",
                    "title": "Model and Data Quality Metrics"
                }
            }
        ]
    }
    
    # ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ä½œæˆ
    response = cloudwatch.put_dashboard(
        DashboardName=f'{endpoint_name}-monitoring',
        DashboardBody=json.dumps(dashboard_body)
    )
    
    print(f"Dashboard created: {endpoint_name}-monitoring")
    return response

# ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ä½œæˆ
import json
dashboard_response = create_monitoring_dashboard(endpoint_name)
```

## ğŸš¨ Step 3: ã‚¢ãƒ©ãƒ¼ãƒˆã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰

### 3.1 å¤šæ®µéšã‚¢ãƒ©ãƒ¼ãƒˆè¨­å®š

```python
def setup_multi_tier_alerts(endpoint_name):
    """å¤šæ®µéšã‚¢ãƒ©ãƒ¼ãƒˆã‚·ã‚¹ãƒ†ãƒ ã®è¨­å®š"""
    
    cloudwatch = boto3.client('cloudwatch')
    sns = boto3.client('sns')
    
    # SNS ãƒˆãƒ”ãƒƒã‚¯ä½œæˆ
    topics = {
        'critical': sns.create_topic(Name=f'{endpoint_name}-critical-alerts')['TopicArn'],
        'warning': sns.create_topic(Name=f'{endpoint_name}-warning-alerts')['TopicArn'],
        'info': sns.create_topic(Name=f'{endpoint_name}-info-alerts')['TopicArn']
    }
    
    # ã‚¢ãƒ©ãƒ¼ãƒ è¨­å®šãƒªã‚¹ãƒˆ
    alarms = [
        {
            'name': 'HighErrorRate',
            'metric_name': 'ErrorRate',
            'namespace': 'Custom/SageMaker',
            'threshold': 5.0,  # 5%
            'comparison': 'GreaterThanThreshold',
            'severity': 'critical',
            'description': 'High error rate detected'
        },
        {
            'name': 'HighLatency',
            'metric_name': 'TotalLatency',
            'namespace': 'Custom/SageMaker',
            'threshold': 1000.0,  # 1ç§’
            'comparison': 'GreaterThanThreshold',
            'severity': 'warning',
            'description': 'High latency detected'
        },
        {
            'name': 'DataDrift',
            'metric_name': 'feature_baseline_drift_distance',
            'namespace': 'aws/sagemaker/Endpoints/data-metrics',
            'threshold': 0.1,
            'comparison': 'GreaterThanThreshold',
            'severity': 'warning',
            'description': 'Data drift detected'
        },
        {
            'name': 'ModelAccuracyDrop',
            'metric_name': 'accuracy',
            'namespace': 'aws/sagemaker/Endpoints/model-metrics',
            'threshold': 0.8,
            'comparison': 'LessThanThreshold',
            'severity': 'critical',
            'description': 'Model accuracy below threshold'
        }
    ]
    
    # ã‚¢ãƒ©ãƒ¼ãƒ ä½œæˆ
    for alarm in alarms:
        dimensions = [{'Name': 'EndpointName', 'Value': endpoint_name}]
        if 'MonitoringSchedule' in alarm['namespace']:
            dimensions = [{'Name': 'MonitoringSchedule', 'Value': f"{endpoint_name}-data-quality"}]
        
        cloudwatch.put_metric_alarm(
            AlarmName=f"{endpoint_name}-{alarm['name']}",
            ComparisonOperator=alarm['comparison'],
            EvaluationPeriods=2,
            MetricName=alarm['metric_name'],
            Namespace=alarm['namespace'],
            Period=300,
            Statistic='Average',
            Threshold=alarm['threshold'],
            ActionsEnabled=True,
            AlarmActions=[topics[alarm['severity']]],
            AlarmDescription=alarm['description'],
            Dimensions=dimensions,
            TreatMissingData='notBreaching'
        )
    
    print(f"Multi-tier alerts configured for {endpoint_name}")
    return topics

# ã‚¢ãƒ©ãƒ¼ãƒˆè¨­å®š
alert_topics = setup_multi_tier_alerts(endpoint_name)
```

### 3.2 è‡ªå‹•å¯¾å¿œã‚·ã‚¹ãƒ†ãƒ 

```python
def create_auto_response_system(endpoint_name, alert_topics):
    """è‡ªå‹•å¯¾å¿œã‚·ã‚¹ãƒ†ãƒ ã®ä½œæˆ"""
    
    # Lambda é–¢æ•°ã‚³ãƒ¼ãƒ‰
    lambda_code = f"""
import json
import boto3
import os
from datetime import datetime

def lambda_handler(event, context):
    try:
        # SNS ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰æƒ…å ±å–å¾—
        message = json.loads(event['Records'][0]['Sns']['Message'])
        alarm_name = message['AlarmName']
        alarm_state = message['NewStateValue']
        
        if alarm_state != 'ALARM':
            return {{'statusCode': 200, 'body': 'Alarm not in ALARM state'}}
        
        # å¯¾å¿œã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ±ºå®š
        response_action = determine_response_action(alarm_name)
        
        if response_action:
            result = execute_response_action(response_action, alarm_name)
            
            # å¯¾å¿œçµæœã‚’é€šçŸ¥
            notify_response_result(alarm_name, response_action, result)
            
            return {{
                'statusCode': 200,
                'body': json.dumps({{
                    'alarm': alarm_name,
                    'action': response_action,
                    'result': result
                }})
            }}
        
    except Exception as e:
        print(f"Error in auto-response: {{str(e)}}")
        return {{'statusCode': 500, 'body': str(e)}}

def determine_response_action(alarm_name):
    '''ã‚¢ãƒ©ãƒ¼ãƒ ã«å¿œã˜ãŸå¯¾å¿œã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ±ºå®š'''
    actions = {{
        'HighErrorRate': 'scale_up',
        'HighLatency': 'scale_up', 
        'DataDrift': 'trigger_retraining',
        'ModelAccuracyDrop': 'rollback_model'
    }}
    
    for alarm_type, action in actions.items():
        if alarm_type in alarm_name:
            return action
    return None

def execute_response_action(action, alarm_name):
    '''å¯¾å¿œã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œ'''
    sagemaker = boto3.client('sagemaker')
    autoscaling = boto3.client('application-autoscaling')
    
    if action == 'scale_up':
        # Auto Scaling è¨­å®šæ›´æ–°
        resource_id = f"endpoint/{endpoint_name}/variant/primary"
        try:
            autoscaling.put_scaling_policy(
                PolicyName=f'{{endpoint_name}}-emergency-scale',
                ServiceNamespace='sagemaker',
                ResourceId=resource_id,
                ScalableDimension='sagemaker:variant:DesiredInstanceCount',
                PolicyType='StepScaling',
                StepScalingPolicyConfiguration={{
                    'AdjustmentType': 'ChangeInCapacity',
                    'StepAdjustments': [
                        {{
                            'MetricIntervalLowerBound': 0,
                            'ScalingAdjustment': 2
                        }}
                    ]
                }}
            )
            return 'Scaling policy updated'
        except Exception as e:
            return f'Scaling failed: {{str(e)}}'
    
    elif action == 'trigger_retraining':
        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
        try:
            response = sagemaker.start_pipeline_execution(
                PipelineName='ml-training-pipeline',
                PipelineParameters=[
                    {{'Name': 'InputData', 'Value': 's3://your-bucket/retrain-data/'}},
                    {{'Name': 'AccuracyThreshold', 'Value': '0.85'}}
                ]
            )
            return f'Retraining started: {{response["PipelineExecutionArn"]}}'
        except Exception as e:
            return f'Retraining failed: {{str(e)}}'
    
    elif action == 'rollback_model':
        # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆå‰ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã«æˆ»ã™ï¼‰
        try:
            # å®Ÿè£…: å‰ã®ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’å–å¾—ã—ã¦ãƒ‡ãƒ—ãƒ­ã‚¤
            return 'Model rollback initiated'
        except Exception as e:
            return f'Rollback failed: {{str(e)}}'
    
    return 'No action taken'

def notify_response_result(alarm_name, action, result):
    '''å¯¾å¿œçµæœã‚’é€šçŸ¥'''
    sns = boto3.client('sns')
    
    message = f'''
    è‡ªå‹•å¯¾å¿œã‚·ã‚¹ãƒ†ãƒ ãŒä½œå‹•ã—ã¾ã—ãŸ
    
    ã‚¢ãƒ©ãƒ¼ãƒ : {{alarm_name}}
    å®Ÿè¡Œã‚¢ã‚¯ã‚·ãƒ§ãƒ³: {{action}}
    çµæœ: {{result}}
    æ™‚åˆ»: {{datetime.now().isoformat()}}
    '''
    
    sns.publish(
        TopicArn='{alert_topics["info"]}',
        Subject=f'è‡ªå‹•å¯¾å¿œå®Ÿè¡Œ - {{alarm_name}}',
        Message=message
    )
"""
    
    # Lambda é–¢æ•°ä½œæˆï¼ˆå®Ÿéš›ã«ã¯CloudFormation/CDKã‚’ä½¿ç”¨æ¨å¥¨ï¼‰
    print("Auto-response Lambda function code generated")
    print("Deploy this using CloudFormation or CDK for production use")
    
    return lambda_code

# è‡ªå‹•å¯¾å¿œã‚·ã‚¹ãƒ†ãƒ ä½œæˆ
auto_response_code = create_auto_response_system(endpoint_name, alert_topics)
```

## ğŸ”„ Step 4: A/B ãƒ†ã‚¹ãƒˆã¨æ®µéšçš„ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ

### 4.1 A/B ãƒ†ã‚¹ãƒˆç›£è¦–

```python
def setup_ab_test_monitoring(endpoint_name, variant_names):
    """A/B ãƒ†ã‚¹ãƒˆç”¨ã®ç›£è¦–è¨­å®š"""
    
    cloudwatch = boto3.client('cloudwatch')
    
    # å„ãƒãƒªã‚¢ãƒ³ãƒˆç”¨ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç›£è¦–
    for variant in variant_names:
        # ãƒãƒªã‚¢ãƒ³ãƒˆå›ºæœ‰ã®ã‚¢ãƒ©ãƒ¼ãƒ ä½œæˆ
        cloudwatch.put_metric_alarm(
            AlarmName=f"{endpoint_name}-{variant}-ErrorRate",
            ComparisonOperator='GreaterThanThreshold',
            EvaluationPeriods=3,
            MetricName='Invocation4XXErrors',
            Namespace='AWS/SageMaker',
            Period=300,
            Statistic='Sum',
            Threshold=10.0,
            ActionsEnabled=True,
            AlarmActions=[alert_topics['warning']],
            AlarmDescription=f'High error rate for variant {variant}',
            Dimensions=[
                {'Name': 'EndpointName', 'Value': endpoint_name},
                {'Name': 'VariantName', 'Value': variant}
            ]
        )
        
        # ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã‚¢ãƒ©ãƒ¼ãƒ 
        cloudwatch.put_metric_alarm(
            AlarmName=f"{endpoint_name}-{variant}-Latency",
            ComparisonOperator='GreaterThanThreshold',
            EvaluationPeriods=2,
            MetricName='ModelLatency',
            Namespace='AWS/SageMaker',
            Period=300,
            Statistic='Average',
            Threshold=500.0,  # 500ms
            ActionsEnabled=True,
            AlarmActions=[alert_topics['warning']],
            AlarmDescription=f'High latency for variant {variant}',
            Dimensions=[
                {'Name': 'EndpointName', 'Value': endpoint_name},
                {'Name': 'VariantName', 'Value': variant}
            ]
        )
    
    print(f"A/B test monitoring configured for variants: {variant_names}")

# A/B ãƒ†ã‚¹ãƒˆç›£è¦–è¨­å®š
variant_names = ['model-a', 'model-b']
setup_ab_test_monitoring(endpoint_name, variant_names)
```

### 4.2 æ®µéšçš„ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆç›£è¦–

```python
def create_canary_deployment_monitor(endpoint_name):
    """ã‚«ãƒŠãƒªã‚¢ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆç›£è¦–ã‚·ã‚¹ãƒ†ãƒ """
    
    def monitor_canary_metrics(canary_variant, primary_variant, duration_minutes=30):
        """ã‚«ãƒŠãƒªã‚¢ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ç›£è¦–"""
        
        cloudwatch = boto3.client('cloudwatch')
        end_time = datetime.utcnow()
        start_time = end_time - timedelta(minutes=duration_minutes)
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—
        def get_variant_metrics(variant_name, metric_name):
            response = cloudwatch.get_metric_statistics(
                Namespace='AWS/SageMaker',
                MetricName=metric_name,
                Dimensions=[
                    {'Name': 'EndpointName', 'Value': endpoint_name},
                    {'Name': 'VariantName', 'Value': variant_name}
                ],
                StartTime=start_time,
                EndTime=end_time,
                Period=300,
                Statistics=['Average', 'Sum']
            )
            return response['Datapoints']
        
        # ã‚«ãƒŠãƒªã‚¢ã¨ãƒ—ãƒ©ã‚¤ãƒãƒªã®æ¯”è¼ƒ
        canary_errors = get_variant_metrics(canary_variant, 'Invocation4XXErrors')
        primary_errors = get_variant_metrics(primary_variant, 'Invocation4XXErrors')
        
        canary_latency = get_variant_metrics(canary_variant, 'ModelLatency')
        primary_latency = get_variant_metrics(primary_variant, 'ModelLatency')
        
        # æ¯”è¼ƒåˆ†æ
        analysis_result = {
            'canary_healthy': True,
            'error_rate_comparison': 'unknown',
            'latency_comparison': 'unknown',
            'recommendation': 'continue'
        }
        
        if canary_errors and primary_errors:
            canary_error_rate = sum([d['Sum'] for d in canary_errors]) / len(canary_errors)
            primary_error_rate = sum([d['Sum'] for d in primary_errors]) / len(primary_errors)
            
            if canary_error_rate > primary_error_rate * 1.5:  # 50%ä»¥ä¸Šæ‚ªã„
                analysis_result['canary_healthy'] = False
                analysis_result['error_rate_comparison'] = 'worse'
                analysis_result['recommendation'] = 'rollback'
        
        if canary_latency and primary_latency:
            canary_avg_latency = sum([d['Average'] for d in canary_latency]) / len(canary_latency)
            primary_avg_latency = sum([d['Average'] for d in primary_latency]) / len(primary_latency)
            
            if canary_avg_latency > primary_avg_latency * 1.2:  # 20%ä»¥ä¸Šé…ã„
                analysis_result['latency_comparison'] = 'worse'
                if analysis_result['recommendation'] != 'rollback':
                    analysis_result['recommendation'] = 'hold'
        
        return analysis_result
    
    def execute_canary_decision(analysis_result, canary_variant):
        """ã‚«ãƒŠãƒªã‚¢åˆ†æçµæœã«åŸºã¥ãã‚¢ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œ"""
        
        sagemaker = boto3.client('sagemaker')
        
        if analysis_result['recommendation'] == 'rollback':
            # ã‚«ãƒŠãƒªã‚¢ã‚’0%ã«æˆ»ã™
            sagemaker.update_endpoint_weights_and_capacities(
                EndpointName=endpoint_name,
                DesiredWeightsAndCapacities=[
                    {
                        'VariantName': canary_variant,
                        'CurrentWeight': 0
                    }
                ]
            )
            print(f"Canary rolled back: {canary_variant}")
            
        elif analysis_result['recommendation'] == 'continue':
            # ã‚«ãƒŠãƒªã‚¢ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯ã‚’æ®µéšçš„ã«å¢—åŠ 
            current_weight = 10  # ç¾åœ¨ã®é‡ã¿ï¼ˆå®Ÿéš›ã¯å–å¾—ï¼‰
            new_weight = min(current_weight + 10, 50)  # 10%ãšã¤å¢—åŠ ã€æœ€å¤§50%
            
            sagemaker.update_endpoint_weights_and_capacities(
                EndpointName=endpoint_name,
                DesiredWeightsAndCapacities=[
                    {
                        'VariantName': canary_variant,
                        'CurrentWeight': new_weight
                    }
                ]
            )
            print(f"Canary traffic increased to {new_weight}%")
        
        return analysis_result['recommendation']
    
    return monitor_canary_metrics, execute_canary_decision

# ã‚«ãƒŠãƒªã‚¢ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆç›£è¦–
canary_monitor, canary_executor = create_canary_deployment_monitor(endpoint_name)

# å®Ÿè¡Œä¾‹
canary_analysis = canary_monitor('model-v2', 'model-v1', 30)
action_taken = canary_executor(canary_analysis, 'model-v2')
print(f"Canary analysis: {canary_analysis}")
print(f"Action taken: {action_taken}")
```

## ğŸ“ˆ Step 5: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æã¨ãƒ¬ãƒãƒ¼ãƒˆ

### 5.1 è‡ªå‹•ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ

```python
def create_monitoring_report(endpoint_name, report_period_hours=24):
    """ç›£è¦–ãƒ¬ãƒãƒ¼ãƒˆã®è‡ªå‹•ç”Ÿæˆ"""
    
    cloudwatch = boto3.client('cloudwatch')
    end_time = datetime.utcnow()
    start_time = end_time - timedelta(hours=report_period_hours)
    
    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†
    metrics_to_collect = [
        'Invocations',
        'InvocationErrors', 
        'ModelLatency',
        'OverheadLatency'
    ]
    
    report_data = {}
    
    for metric in metrics_to_collect:
        response = cloudwatch.get_metric_statistics(
            Namespace='AWS/SageMaker',
            MetricName=metric,
            Dimensions=[
                {'Name': 'EndpointName', 'Value': endpoint_name}
            ],
            StartTime=start_time,
            EndTime=end_time,
            Period=3600,  # 1æ™‚é–“æ¯
            Statistics=['Average', 'Maximum', 'Sum']
        )
        report_data[metric] = response['Datapoints']
    
    # ãƒ¬ãƒãƒ¼ãƒˆåˆ†æ
    def analyze_metrics(data):
        analysis = {}
        
        # ç·å‘¼ã³å‡ºã—æ•°
        if 'Invocations' in data:
            total_invocations = sum([d['Sum'] for d in data['Invocations']])
            analysis['total_invocations'] = total_invocations
        
        # ã‚¨ãƒ©ãƒ¼ç‡
        if 'InvocationErrors' in data and 'Invocations' in data:
            total_errors = sum([d['Sum'] for d in data['InvocationErrors']])
            error_rate = (total_errors / total_invocations * 100) if total_invocations > 0 else 0
            analysis['error_rate_percent'] = error_rate
        
        # å¹³å‡ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·
        if 'ModelLatency' in data:
            avg_latency = sum([d['Average'] for d in data['ModelLatency']]) / len(data['ModelLatency'])
            analysis['average_latency_ms'] = avg_latency
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡
        analysis['performance_grade'] = 'A'
        if analysis.get('error_rate_percent', 0) > 1:
            analysis['performance_grade'] = 'B'
        if analysis.get('error_rate_percent', 0) > 5:
            analysis['performance_grade'] = 'C'
        if analysis.get('average_latency_ms', 0) > 1000:
            analysis['performance_grade'] = 'C'
        
        return analysis
    
    analysis = analyze_metrics(report_data)
    
    # HTML ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    html_report = f"""
    <!DOCTYPE html>
    <html>
    <head>
        <title>Model Monitoring Report - {endpoint_name}</title>
        <style>
            body {{ font-family: Arial, sans-serif; margin: 40px; }}
            .header {{ background-color: #f0f0f0; padding: 20px; }}
            .metric {{ margin: 20px 0; padding: 15px; border-left: 4px solid #007cba; }}
            .grade-A {{ color: green; }}
            .grade-B {{ color: orange; }}
            .grade-C {{ color: red; }}
        </style>
    </head>
    <body>
        <div class="header">
            <h1>Model Monitoring Report</h1>
            <p>Endpoint: {endpoint_name}</p>
            <p>Period: {start_time.strftime('%Y-%m-%d %H:%M')} - {end_time.strftime('%Y-%m-%d %H:%M')}</p>
        </div>
        
        <div class="metric">
            <h2>Performance Summary</h2>
            <p><strong>Overall Grade:</strong> 
            <span class="grade-{analysis['performance_grade']}">{analysis['performance_grade']}</span></p>
        </div>
        
        <div class="metric">
            <h3>Key Metrics</h3>
            <ul>
                <li>Total Invocations: {analysis.get('total_invocations', 'N/A'):,}</li>
                <li>Error Rate: {analysis.get('error_rate_percent', 'N/A'):.2f}%</li>
                <li>Average Latency: {analysis.get('average_latency_ms', 'N/A'):.2f} ms</li>
            </ul>
        </div>
        
        <div class="metric">
            <h3>Recommendations</h3>
            <ul>
    """
    
    # æ¨å¥¨äº‹é …è¿½åŠ 
    if analysis.get('error_rate_percent', 0) > 1:
        html_report += "<li>Error rate is elevated. Consider investigating root causes.</li>"
    
    if analysis.get('average_latency_ms', 0) > 500:
        html_report += "<li>Latency is high. Consider scaling up or optimizing the model.</li>"
    
    if analysis.get('total_invocations', 0) < 100:
        html_report += "<li>Low traffic volume. Monitor for sufficient load testing.</li>"
    
    html_report += """
            </ul>
        </div>
    </body>
    </html>
    """
    
    # S3ã«ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
    s3 = boto3.client('s3')
    report_key = f"monitoring-reports/{endpoint_name}/{end_time.strftime('%Y-%m-%d-%H')}-report.html"
    
    s3.put_object(
        Bucket='your-monitoring-bucket',
        Key=report_key,
        Body=html_report,
        ContentType='text/html'
    )
    
    print(f"Monitoring report generated: s3://your-monitoring-bucket/{report_key}")
    return analysis, f"s3://your-monitoring-bucket/{report_key}"

# ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Ÿè¡Œ
report_analysis, report_url = create_monitoring_report(endpoint_name, 24)
print(f"Report analysis: {report_analysis}")
```

## ğŸ§¹ Step 6: ãƒªã‚½ãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—

### 6.1 ç›£è¦–ãƒªã‚½ãƒ¼ã‚¹ã®å‰Šé™¤

```python
def cleanup_monitoring_resources(endpoint_name):
    """ç›£è¦–é–¢é€£ãƒªã‚½ãƒ¼ã‚¹ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
    
    sagemaker_client = boto3.client('sagemaker')
    cloudwatch = boto3.client('cloudwatch')
    sns = boto3.client('sns')
    
    try:
        # ç›£è¦–ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã®å‰Šé™¤
        schedules = [
            f'{endpoint_name}-data-quality',
            f'{endpoint_name}-model-quality',
            f'{endpoint_name}-bias-monitor'
        ]
        
        for schedule in schedules:
            try:
                sagemaker_client.delete_monitoring_schedule(
                    MonitoringScheduleName=schedule
                )
                print(f"Monitoring schedule deleted: {schedule}")
            except Exception as e:
                print(f"Error deleting schedule {schedule}: {e}")
        
        # CloudWatch ã‚¢ãƒ©ãƒ¼ãƒ ã®å‰Šé™¤
        alarm_names = [
            f'{endpoint_name}-HighErrorRate',
            f'{endpoint_name}-HighLatency',
            f'{endpoint_name}-DataDrift',
            f'{endpoint_name}-ModelAccuracyDrop'
        ]
        
        for alarm_name in alarm_names:
            try:
                cloudwatch.delete_alarms(AlarmNames=[alarm_name])
                print(f"Alarm deleted: {alarm_name}")
            except Exception as e:
                print(f"Error deleting alarm {alarm_name}: {e}")
        
        # ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã®å‰Šé™¤
        try:
            cloudwatch.delete_dashboards(
                DashboardNames=[f'{endpoint_name}-monitoring']
            )
            print(f"Dashboard deleted: {endpoint_name}-monitoring")
        except Exception as e:
            print(f"Error deleting dashboard: {e}")
        
        # SNS ãƒˆãƒ”ãƒƒã‚¯ã®å‰Šé™¤
        topic_names = [
            f'{endpoint_name}-critical-alerts',
            f'{endpoint_name}-warning-alerts',
            f'{endpoint_name}-info-alerts'
        ]
        
        for topic_name in topic_names:
            try:
                # ãƒˆãƒ”ãƒƒã‚¯ARNã‚’å–å¾—ã—ã¦å‰Šé™¤
                topics = sns.list_topics()
                for topic in topics['Topics']:
                    if topic_name in topic['TopicArn']:
                        sns.delete_topic(TopicArn=topic['TopicArn'])
                        print(f"SNS topic deleted: {topic_name}")
                        break
            except Exception as e:
                print(f"Error deleting topic {topic_name}: {e}")
        
    except Exception as e:
        print(f"Error during cleanup: {str(e)}")

# ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Ÿè¡Œ
cleanup_monitoring_resources(endpoint_name)
```

## ğŸ’° ã‚³ã‚¹ãƒˆè¨ˆç®—

### æ¨å®šã‚³ã‚¹ãƒˆï¼ˆ1æ—¥ã‚ãŸã‚Šï¼‰
- **Model Monitor**: $5.00/æ—¥
- **CloudWatch ãƒ¡ãƒˆãƒªã‚¯ã‚¹**: $1.50/æ—¥
- **CloudWatch ã‚¢ãƒ©ãƒ¼ãƒ **: $0.30/æ—¥
- **SNSé€šçŸ¥**: $0.10/æ—¥
- **Lambdaå®Ÿè¡Œ**: $0.20/æ—¥
- **S3ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸**: $0.50/æ—¥
- **åˆè¨ˆ**: ç´„ $7.60/æ—¥

## ğŸ“š å­¦ç¿’ãƒã‚¤ãƒ³ãƒˆ

### é‡è¦ãªæ¦‚å¿µ
1. **åŒ…æ‹¬çš„ç›£è¦–**: ãƒ‡ãƒ¼ã‚¿å“è³ªãƒ»ãƒ¢ãƒ‡ãƒ«å“è³ªãƒ»ãƒã‚¤ã‚¢ã‚¹ç›£è¦–
2. **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¢ãƒ©ãƒ¼ãƒˆ**: å¤šæ®µéšã‚¢ãƒ©ãƒ¼ãƒˆã¨è‡ªå‹•å¯¾å¿œ
3. **A/B ãƒ†ã‚¹ãƒˆç›£è¦–**: ãƒãƒªã‚¢ãƒ³ãƒˆé–“ã®æ€§èƒ½æ¯”è¼ƒ
4. **è‡ªå‹•ãƒ¬ãƒãƒ¼ãƒˆ**: å®šæœŸçš„ãªæ€§èƒ½åˆ†æ
5. **ã‚³ã‚¹ãƒˆåŠ¹ç‡**: ç›£è¦–ãƒ¬ãƒ™ãƒ«ã¨ã‚³ã‚¹ãƒˆã®ãƒãƒ©ãƒ³ã‚¹

### å®Ÿè·µçš„ãªã‚¹ã‚­ãƒ«
- SageMaker Model Monitor ã®æ´»ç”¨
- CloudWatch ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã¨ã‚¢ãƒ©ãƒ¼ãƒ è¨­å®š
- è‡ªå‹•å¯¾å¿œã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰
- ã‚«ãƒŠãƒªã‚¢ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆç›£è¦–
- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æã¨ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ

---

**å®Œäº†**: ã“ã® Lab 5 ã§ã€æœ¬æ ¼çš„ãªMLãƒ¢ãƒ‡ãƒ«ç›£è¦–ã¨é‹ç”¨ã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰ã‚¹ã‚­ãƒ«ã‚’ç¿’å¾—ã—ã¾ã—ãŸã€‚æ¬¡ã¯å®Ÿéš›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã“ã‚Œã‚‰ã®æŠ€è¡“ã‚’æ´»ç”¨ã—ã¦ãã ã•ã„ã€‚