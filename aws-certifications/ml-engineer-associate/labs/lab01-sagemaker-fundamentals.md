# Lab 1: SageMaker åŸºç¤ã¨ãƒ‡ãƒ¼ã‚¿æº–å‚™

## ğŸ¯ å­¦ç¿’ç›®æ¨™

ã“ã®ãƒ©ãƒœã§ã¯ã€AWS SageMaker ã®åŸºæœ¬æ“ä½œã¨ãƒ‡ãƒ¼ã‚¿æº–å‚™ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã‚’å®Ÿè·µçš„ã«å­¦ç¿’ã—ã¾ã™ã€‚

**ç¿’å¾—ã‚¹ã‚­ãƒ«**:
- SageMaker Studio ã®æ“ä½œ
- Feature Store ã®æ§‹ç¯‰ã¨æ´»ç”¨
- ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å®Ÿè£…
- S3 ã¨ã®é€£æº

**æ‰€è¦æ™‚é–“**: 4-6æ™‚é–“  
**æ¨å®šã‚³ã‚¹ãƒˆ**: $15-25

## ğŸ“‹ å‰ææ¡ä»¶

### å¿…è¦ãªæ¨©é™
```json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "sagemaker:*",
                "s3:GetObject",
                "s3:PutObject",
                "s3:DeleteObject",
                "s3:ListBucket",
                "iam:ListRoles",
                "iam:PassRole"
            ],
            "Resource": "*"
        }
    ]
}
```

### äº‹å‰æº–å‚™
```bash
# Python ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install sagemaker pandas scikit-learn matplotlib seaborn

# AWS CLI è¨­å®šç¢ºèª
aws sts get-caller-identity
```

## Phase 1: SageMaker Studio ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

### 1.1 SageMaker Studio ãƒ‰ãƒ¡ã‚¤ãƒ³ä½œæˆ

```bash
#!/bin/bash
# ã‚¹ã‚¯ãƒªãƒ—ãƒˆ: setup-sagemaker-studio.sh

set -e

echo "=== SageMaker Studio ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—é–‹å§‹ ==="

# å¤‰æ•°å®šç¾©
REGION="ap-northeast-1"
DOMAIN_NAME="ml-engineer-lab-domain"
EXECUTION_ROLE_NAME="SageMaker-ExecutionRole"

# IAM ãƒ­ãƒ¼ãƒ«ä½œæˆ
echo "1. IAM ãƒ­ãƒ¼ãƒ«ä½œæˆä¸­..."

# ä¿¡é ¼ãƒãƒªã‚·ãƒ¼ä½œæˆ
cat > trust-policy.json << 'EOF'
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": {
                "Service": "sagemaker.amazonaws.com"
            },
            "Action": "sts:AssumeRole"
        }
    ]
}
EOF

# IAM ãƒ­ãƒ¼ãƒ«ä½œæˆ
ROLE_ARN=$(aws iam create-role \
    --role-name $EXECUTION_ROLE_NAME \
    --assume-role-policy-document file://trust-policy.json \
    --query 'Role.Arn' \
    --output text 2>/dev/null || \
    aws iam get-role --role-name $EXECUTION_ROLE_NAME --query 'Role.Arn' --output text)

echo "   IAM ãƒ­ãƒ¼ãƒ«: $ROLE_ARN"

# å¿…è¦ãªãƒãƒªã‚·ãƒ¼ã‚’ã‚¢ã‚¿ãƒƒãƒ
aws iam attach-role-policy \
    --role-name $EXECUTION_ROLE_NAME \
    --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess

aws iam attach-role-policy \
    --role-name $EXECUTION_ROLE_NAME \
    --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess

# ã‚«ã‚¹ã‚¿ãƒ ãƒãƒªã‚·ãƒ¼ä½œæˆ
cat > custom-policy.json << 'EOF'
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "logs:CreateLogGroup",
                "logs:CreateLogStream",
                "logs:PutLogEvents"
            ],
            "Resource": "*"
        },
        {
            "Effect": "Allow",
            "Action": [
                "ecr:GetAuthorizationToken",
                "ecr:BatchCheckLayerAvailability",
                "ecr:GetDownloadUrlForLayer",
                "ecr:BatchGetImage"
            ],
            "Resource": "*"
        }
    ]
}
EOF

aws iam put-role-policy \
    --role-name $EXECUTION_ROLE_NAME \
    --policy-name "SageMakerCustomPolicy" \
    --policy-document file://custom-policy.json

echo "   IAM ãƒãƒªã‚·ãƒ¼è¨­å®šå®Œäº†"

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ VPC å–å¾—
echo "2. ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è¨­å®šå–å¾—ä¸­..."
VPC_ID=$(aws ec2 describe-vpcs \
    --filters "Name=is-default,Values=true" \
    --query 'Vpcs[0].VpcId' \
    --output text \
    --region $REGION)

SUBNET_IDS=$(aws ec2 describe-subnets \
    --filters "Name=vpc-id,Values=$VPC_ID" \
    --query 'Subnets[*].SubnetId' \
    --output text \
    --region $REGION | tr '\t' ',')

echo "   VPC ID: $VPC_ID"
echo "   Subnet IDs: $SUBNET_IDS"

# SageMaker Studio ãƒ‰ãƒ¡ã‚¤ãƒ³ä½œæˆ
echo "3. SageMaker Studio ãƒ‰ãƒ¡ã‚¤ãƒ³ä½œæˆä¸­..."

DOMAIN_ID=$(aws sagemaker create-domain \
    --domain-name $DOMAIN_NAME \
    --auth-mode IAM \
    --default-user-settings '{
        "ExecutionRole": "'$ROLE_ARN'",
        "SecurityGroups": [],
        "SharingSettings": {
            "NotebookOutputOption": "Disabled"
        }
    }' \
    --subnet-ids $(echo $SUBNET_IDS | tr ',' ' ') \
    --vpc-id $VPC_ID \
    --region $REGION \
    --query 'DomainArn' \
    --output text 2>/dev/null || echo "EXISTS")

if [ "$DOMAIN_ID" = "EXISTS" ]; then
    echo "   SageMaker Studio ãƒ‰ãƒ¡ã‚¤ãƒ³ã¯æ—¢ã«å­˜åœ¨ã—ã¾ã™"
    DOMAIN_ID=$(aws sagemaker list-domains \
        --query 'Domains[0].DomainId' \
        --output text \
        --region $REGION)
else
    # ãƒ‰ãƒ¡ã‚¤ãƒ³ ID ã®æŠ½å‡º
    DOMAIN_ID=$(echo $DOMAIN_ID | sed 's/.*domain\///')
    echo "   SageMaker Studio ãƒ‰ãƒ¡ã‚¤ãƒ³ä½œæˆå®Œäº†: $DOMAIN_ID"
    
    echo "   ãƒ‰ãƒ¡ã‚¤ãƒ³ãŒåˆ©ç”¨å¯èƒ½ã«ãªã‚‹ã¾ã§å¾…æ©Ÿä¸­..."
    aws sagemaker wait domain-in-service \
        --domain-id $DOMAIN_ID \
        --region $REGION
fi

# ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
echo "4. ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆä¸­..."
USER_PROFILE_NAME="ml-engineer-user"

aws sagemaker create-user-profile \
    --domain-id $DOMAIN_ID \
    --user-profile-name $USER_PROFILE_NAME \
    --user-settings '{
        "ExecutionRole": "'$ROLE_ARN'"
    }' \
    --region $REGION 2>/dev/null || echo "   ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã¯æ—¢ã«å­˜åœ¨ã—ã¾ã™"

echo "   ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆå®Œäº†: $USER_PROFILE_NAME"

# è¨­å®šæƒ…å ±ä¿å­˜
cat > sagemaker-config.json << EOF
{
    "region": "$REGION",
    "domain_id": "$DOMAIN_ID",
    "user_profile_name": "$USER_PROFILE_NAME",
    "execution_role_arn": "$ROLE_ARN",
    "vpc_id": "$VPC_ID"
}
EOF

echo "=== SageMaker Studio ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº† ==="
echo "è¨­å®šæƒ…å ±ãŒ sagemaker-config.json ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ"
echo ""
echo "SageMaker Studio ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹:"
echo "1. AWS Console > SageMaker > Studio"
echo "2. ãƒ‰ãƒ¡ã‚¤ãƒ³: $DOMAIN_ID"
echo "3. ãƒ¦ãƒ¼ã‚¶ãƒ¼: $USER_PROFILE_NAME ã§ãƒ­ã‚°ã‚¤ãƒ³"
echo ""
echo "æ¨å®šã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ™‚é–“: 10-15åˆ†"
echo "æ¨å®šæœˆé¡ã‚³ã‚¹ãƒˆ: ãƒ‰ãƒ¡ã‚¤ãƒ³ç¶­æŒè²» $0 (ä½¿ç”¨æ™‚ã®ã¿èª²é‡‘)"
```

### 1.2 S3 ãƒã‚±ãƒƒãƒˆæº–å‚™

```python
#!/usr/bin/env python3
"""
S3 ãƒã‚±ãƒƒãƒˆã¨ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
"""

import boto3
import pandas as pd
import numpy as np
import json
from datetime import datetime, timedelta
import os

class S3DataPreparation:
    def __init__(self, region='ap-northeast-1'):
        self.region = region
        self.s3 = boto3.client('s3', region_name=region)
        self.bucket_name = f"sagemaker-lab-{boto3.session.Session().region_name}-{boto3.sts.StsClient().get_caller_identity()['Account']}"
        
    def create_bucket(self):
        """S3ãƒã‚±ãƒƒãƒˆä½œæˆ"""
        print("1. S3ãƒã‚±ãƒƒãƒˆä½œæˆä¸­...")
        
        try:
            if self.region == 'us-east-1':
                self.s3.create_bucket(Bucket=self.bucket_name)
            else:
                self.s3.create_bucket(
                    Bucket=self.bucket_name,
                    CreateBucketConfiguration={'LocationConstraint': self.region}
                )
            print(f"   S3ãƒã‚±ãƒƒãƒˆä½œæˆå®Œäº†: {self.bucket_name}")
        except self.s3.exceptions.BucketAlreadyOwnedByYou:
            print(f"   S3ãƒã‚±ãƒƒãƒˆã¯æ—¢ã«å­˜åœ¨ã—ã¾ã™: {self.bucket_name}")
        except Exception as e:
            print(f"   ã‚¨ãƒ©ãƒ¼: S3ãƒã‚±ãƒƒãƒˆä½œæˆå¤±æ•— - {str(e)}")
            return False
            
        # ãƒã‚±ãƒƒãƒˆãƒãƒªã‚·ãƒ¼è¨­å®šï¼ˆSageMaker ã‚¢ã‚¯ã‚»ã‚¹ç”¨ï¼‰
        bucket_policy = {
            "Version": "2012-10-17",
            "Statement": [
                {
                    "Effect": "Allow",
                    "Principal": {
                        "Service": "sagemaker.amazonaws.com"
                    },
                    "Action": [
                        "s3:GetObject",
                        "s3:PutObject",
                        "s3:DeleteObject",
                        "s3:ListBucket"
                    ],
                    "Resource": [
                        f"arn:aws:s3:::{self.bucket_name}",
                        f"arn:aws:s3:::{self.bucket_name}/*"
                    ]
                }
            ]
        }
        
        try:
            self.s3.put_bucket_policy(
                Bucket=self.bucket_name,
                Policy=json.dumps(bucket_policy)
            )
            print("   ãƒã‚±ãƒƒãƒˆãƒãƒªã‚·ãƒ¼è¨­å®šå®Œäº†")
        except Exception as e:
            print(f"   è­¦å‘Š: ãƒã‚±ãƒƒãƒˆãƒãƒªã‚·ãƒ¼è¨­å®šå¤±æ•— - {str(e)}")
            
        return True
    
    def generate_sample_data(self):
        """ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ"""
        print("2. ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆä¸­...")
        
        # é¡§å®¢ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        np.random.seed(42)
        n_customers = 10000
        
        customers = pd.DataFrame({
            'customer_id': range(1, n_customers + 1),
            'age': np.random.normal(35, 12, n_customers).astype(int),
            'income': np.random.lognormal(10.5, 0.5, n_customers).astype(int),
            'credit_score': np.random.normal(650, 100, n_customers).astype(int),
            'account_balance': np.random.exponential(5000, n_customers),
            'years_with_bank': np.random.exponential(3, n_customers),
            'num_products': np.random.poisson(2, n_customers) + 1,
            'has_cr_card': np.random.choice([0, 1], n_customers, p=[0.3, 0.7]),
            'is_active_member': np.random.choice([0, 1], n_customers, p=[0.2, 0.8]),
            'geography': np.random.choice(['France', 'Germany', 'Spain'], n_customers, p=[0.5, 0.25, 0.25])
        })
        
        # ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚«ãƒ¼ãƒ‰é€€ä¼šäºˆæ¸¬ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°
        # ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°çš„ãªé–¢ä¿‚æ€§ã‚’ä½œæˆ
        logit = (
            -2.0 + 
            -0.01 * customers['age'] +
            -0.00001 * customers['income'] +
            -0.005 * customers['credit_score'] +
            -0.0001 * customers['account_balance'] +
            -0.3 * customers['years_with_bank'] +
            -0.2 * customers['num_products'] +
            -0.5 * customers['has_cr_card'] +
            -1.0 * customers['is_active_member'] +
            0.2 * (customers['geography'] == 'Germany').astype(int)
        )
        
        probability = 1 / (1 + np.exp(-logit))
        customers['churned'] = np.random.binomial(1, probability, n_customers)
        
        print(f"   é¡§å®¢ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆå®Œäº†: {len(customers):,} ä»¶")
        print(f"   è§£ç´„ç‡: {customers['churned'].mean():.2%}")
        
        return customers
    
    def generate_time_series_data(self):
        """æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆå•†å“å£²ä¸Šäºˆæ¸¬ç”¨ï¼‰"""
        print("3. æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆä¸­...")
        
        # 2å¹´é–“ã®æ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿
        start_date = datetime.now() - timedelta(days=730)
        dates = [start_date + timedelta(days=x) for x in range(730)]
        
        # è¤‡æ•°å•†å“ã®å£²ä¸Šãƒ‡ãƒ¼ã‚¿
        products = ['Product_A', 'Product_B', 'Product_C', 'Product_D', 'Product_E']
        
        sales_data = []
        for product in products:
            # å•†å“ã”ã¨ã«ç•°ãªã‚‹ãƒˆãƒ¬ãƒ³ãƒ‰ã¨å­£ç¯€æ€§
            base_sales = np.random.normal(1000, 200, 730)
            
            # ãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆç·šå½¢ + ãƒã‚¤ã‚ºï¼‰
            trend = np.linspace(0, 300, 730) + np.random.normal(0, 50, 730)
            
            # å­£ç¯€æ€§ï¼ˆå¹´æ¬¡ + é€±æ¬¡ï¼‰
            days_from_start = np.arange(730)
            yearly_seasonal = 200 * np.sin(2 * np.pi * days_from_start / 365)
            weekly_seasonal = 50 * np.sin(2 * np.pi * days_from_start / 7)
            
            # ç‰¹åˆ¥ã‚¤ãƒ™ãƒ³ãƒˆï¼ˆãƒ–ãƒ©ãƒƒã‚¯ãƒ•ãƒ©ã‚¤ãƒ‡ãƒ¼ã€å¹´æœ«å¹´å§‹ãªã©ï¼‰
            event_boost = np.zeros(730)
            for i, date in enumerate(dates):
                if date.month == 11 and date.day >= 25:  # ãƒ–ãƒ©ãƒƒã‚¯ãƒ•ãƒ©ã‚¤ãƒ‡ãƒ¼æœŸé–“
                    event_boost[i] = 500
                elif date.month == 12 and date.day >= 20:  # å¹´æœ«å•†æˆ¦
                    event_boost[i] = 300
                elif date.month == 1 and date.day <= 5:   # æ–°å¹´ã‚»ãƒ¼ãƒ«
                    event_boost[i] = 200
            
            # æœ€çµ‚å£²ä¸Šè¨ˆç®—
            sales = np.maximum(0, base_sales + trend + yearly_seasonal + weekly_seasonal + event_boost)
            
            for i, (date, sale) in enumerate(zip(dates, sales)):
                sales_data.append({
                    'date': date.strftime('%Y-%m-%d'),
                    'product': product,
                    'sales': int(sale),
                    'day_of_week': date.weekday(),
                    'month': date.month,
                    'is_weekend': 1 if date.weekday() >= 5 else 0,
                    'is_holiday': 1 if (date.month == 12 and date.day >= 24) or (date.month == 1 and date.day <= 2) else 0
                })
        
        sales_df = pd.DataFrame(sales_data)
        print(f"   å£²ä¸Šãƒ‡ãƒ¼ã‚¿ç”Ÿæˆå®Œäº†: {len(sales_df):,} ä»¶")
        
        return sales_df
    
    def upload_datasets(self, customers_df, sales_df):
        """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"""
        print("4. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆS3ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­...")
        
        # é¡§å®¢ãƒ‡ãƒ¼ã‚¿ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        customers_csv = customers_df.to_csv(index=False)
        self.s3.put_object(
            Bucket=self.bucket_name,
            Key='datasets/customers/customer_churn.csv',
            Body=customers_csv
        )
        
        # è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆåˆ†å‰²ç‰ˆã‚‚ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        train_customers = customers_df.sample(frac=0.8, random_state=42)
        test_customers = customers_df.drop(train_customers.index)
        
        self.s3.put_object(
            Bucket=self.bucket_name,
            Key='datasets/customers/train/customer_churn_train.csv',
            Body=train_customers.to_csv(index=False)
        )
        
        self.s3.put_object(
            Bucket=self.bucket_name,
            Key='datasets/customers/test/customer_churn_test.csv',
            Body=test_customers.to_csv(index=False)
        )
        
        # å£²ä¸Šãƒ‡ãƒ¼ã‚¿ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        sales_csv = sales_df.to_csv(index=False)
        self.s3.put_object(
            Bucket=self.bucket_name,
            Key='datasets/sales/sales_forecast.csv',
            Body=sales_csv
        )
        
        # æ™‚ç³»åˆ—åˆ†å‰²ç‰ˆ
        train_sales = sales_df[sales_df['date'] < '2024-06-01']
        test_sales = sales_df[sales_df['date'] >= '2024-06-01']
        
        self.s3.put_object(
            Bucket=self.bucket_name,
            Key='datasets/sales/train/sales_train.csv',
            Body=train_sales.to_csv(index=False)
        )
        
        self.s3.put_object(
            Bucket=self.bucket_name,
            Key='datasets/sales/test/sales_test.csv',
            Body=test_sales.to_csv(index=False)
        )
        
        print("   ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†")
        print(f"   - é¡§å®¢ãƒ‡ãƒ¼ã‚¿: {len(customers_df):,} ä»¶")
        print(f"   - å£²ä¸Šãƒ‡ãƒ¼ã‚¿: {len(sales_df):,} ä»¶")
        
        # ãƒ‡ãƒ¼ã‚¿ã‚«ã‚¿ãƒ­ã‚°ä½œæˆ
        catalog = {
            "datasets": {
                "customer_churn": {
                    "description": "éŠ€è¡Œé¡§å®¢ã®é€€ä¼šäºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ",
                    "path": f"s3://{self.bucket_name}/datasets/customers/",
                    "target": "churned",
                    "features": list(customers_df.columns[:-1]),
                    "task_type": "binary_classification",
                    "rows": len(customers_df)
                },
                "sales_forecast": {
                    "description": "å•†å“å£²ä¸Šäºˆæ¸¬ç”¨æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿",
                    "path": f"s3://{self.bucket_name}/datasets/sales/",
                    "target": "sales",
                    "features": ["product", "day_of_week", "month", "is_weekend", "is_holiday"],
                    "task_type": "regression",
                    "rows": len(sales_df)
                }
            },
            "bucket_name": self.bucket_name,
            "region": self.region,
            "created_at": datetime.now().isoformat()
        }
        
        # ã‚«ã‚¿ãƒ­ã‚°ã‚’S3ã«ä¿å­˜
        self.s3.put_object(
            Bucket=self.bucket_name,
            Key='metadata/data_catalog.json',
            Body=json.dumps(catalog, indent=2)
        )
        
        # ãƒ­ãƒ¼ã‚«ãƒ«ã«ã‚‚ä¿å­˜
        with open('data_catalog.json', 'w') as f:
            json.dump(catalog, f, indent=2)
        
        print("   ãƒ‡ãƒ¼ã‚¿ã‚«ã‚¿ãƒ­ã‚°ä½œæˆå®Œäº†")
        return catalog
    
    def run_setup(self):
        """å®Œå…¨ãªãƒ‡ãƒ¼ã‚¿æº–å‚™ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        print("=== S3 ãƒ‡ãƒ¼ã‚¿æº–å‚™ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—é–‹å§‹ ===")
        
        # S3ãƒã‚±ãƒƒãƒˆä½œæˆ
        if not self.create_bucket():
            return False
        
        # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        customers_df = self.generate_sample_data()
        sales_df = self.generate_time_series_data()
        
        # ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        catalog = self.upload_datasets(customers_df, sales_df)
        
        print("=== S3 ãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº† ===")
        print(f"S3ãƒã‚±ãƒƒãƒˆ: {self.bucket_name}")
        print("æ¨å®šå®Ÿè¡Œæ™‚é–“: 5-10åˆ†")
        print("æ¨å®šã‚³ã‚¹ãƒˆ: $0.10-0.50 (ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸è²»ç”¨)")
        
        return True

if __name__ == "__main__":
    data_prep = S3DataPreparation()
    success = data_prep.run_setup()
    
    if success:
        print("\nãƒ‡ãƒ¼ã‚¿æº–å‚™ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
        print("æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—: SageMaker Studio ã§ãƒ‡ãƒ¼ã‚¿æ¢ç´¢ã‚’é–‹å§‹ã—ã¦ãã ã•ã„ã€‚")
    else:
        print("\nãƒ‡ãƒ¼ã‚¿æº–å‚™ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚")
```

## Phase 2: ãƒ‡ãƒ¼ã‚¿æ¢ç´¢ã¨ Feature Store æ§‹ç¯‰

### 2.1 SageMaker Studio ã§ã®ãƒ‡ãƒ¼ã‚¿æ¢ç´¢

SageMaker Studio ã§ä»¥ä¸‹ã®Jupyterãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚’ä½œæˆã—ã¾ã™ï¼š

```python
# ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯: data_exploration.ipynb

import sagemaker
import boto3
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sagemaker.feature_store.feature_group import FeatureGroup
from sagemaker.feature_store.feature_definition import FeatureDefinition, FeatureTypeEnum
import json
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# SageMaker ã‚»ãƒƒã‚·ãƒ§ãƒ³åˆæœŸåŒ–
sagemaker_session = sagemaker.Session()
role = sagemaker.get_execution_role()
region = sagemaker_session.boto_region_name
s3_client = boto3.client('s3')

print(f"SageMaker Role: {role}")
print(f"Region: {region}")

# ãƒ‡ãƒ¼ã‚¿ã‚«ã‚¿ãƒ­ã‚°èª­ã¿è¾¼ã¿
with open('data_catalog.json', 'r') as f:
    catalog = json.load(f)

bucket_name = catalog['bucket_name']
print(f"S3 Bucket: {bucket_name}")
```

```python
# ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 1: é¡§å®¢ãƒ‡ãƒ¼ã‚¿ã®æ¢ç´¢

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
customers_df = pd.read_csv(f's3://{bucket_name}/datasets/customers/customer_churn.csv')

print("=== é¡§å®¢ãƒ‡ãƒ¼ã‚¿æ¦‚è¦ ===")
print(f"ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {customers_df.shape}")
print(f"è§£ç´„ç‡: {customers_df['churned'].mean():.2%}")
print()

# åŸºæœ¬çµ±è¨ˆ
print("=== åŸºæœ¬çµ±è¨ˆé‡ ===")
print(customers_df.describe())
print()

# ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯
print("=== ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ ===")
print("æ¬ æå€¤:")
print(customers_df.isnull().sum())
print()

print("é‡è¤‡ãƒ‡ãƒ¼ã‚¿:")
print(f"é‡è¤‡è¡Œæ•°: {customers_df.duplicated().sum()}")
print()

# ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã®åˆ†å¸ƒ
print("=== ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã®åˆ†å¸ƒ ===")
categorical_cols = ['geography', 'has_cr_card', 'is_active_member', 'churned']
for col in categorical_cols:
    print(f"\n{col}:")
    print(customers_df[col].value_counts())
```

```python
# ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 2: å¯è¦–åŒ–ã«ã‚ˆã‚‹ãƒ‡ãƒ¼ã‚¿ç†è§£

# å›³ã®ã‚µã‚¤ã‚ºè¨­å®š
plt.style.use('seaborn-v0_8')
fig, axes = plt.subplots(2, 3, figsize=(18, 12))

# å¹´é½¢åˆ†å¸ƒ
axes[0, 0].hist(customers_df['age'], bins=30, alpha=0.7, color='skyblue')
axes[0, 0].set_title('å¹´é½¢åˆ†å¸ƒ')
axes[0, 0].set_xlabel('å¹´é½¢')
axes[0, 0].set_ylabel('é »åº¦')

# åå…¥åˆ†å¸ƒï¼ˆå¯¾æ•°ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰
axes[0, 1].hist(np.log10(customers_df['income']), bins=30, alpha=0.7, color='lightgreen')
axes[0, 1].set_title('åå…¥åˆ†å¸ƒï¼ˆlog10ï¼‰')
axes[0, 1].set_xlabel('log10(åå…¥)')
axes[0, 1].set_ylabel('é »åº¦')

# ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚¹ã‚³ã‚¢åˆ†å¸ƒ
axes[0, 2].hist(customers_df['credit_score'], bins=30, alpha=0.7, color='orange')
axes[0, 2].set_title('ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚¹ã‚³ã‚¢åˆ†å¸ƒ')
axes[0, 2].set_xlabel('ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚¹ã‚³ã‚¢')
axes[0, 2].set_ylabel('é »åº¦')

# åœ°åŸŸåˆ¥è§£ç´„ç‡
churn_by_geo = customers_df.groupby('geography')['churned'].mean()
axes[1, 0].bar(churn_by_geo.index, churn_by_geo.values, color='lightcoral')
axes[1, 0].set_title('åœ°åŸŸåˆ¥è§£ç´„ç‡')
axes[1, 0].set_xlabel('åœ°åŸŸ')
axes[1, 0].set_ylabel('è§£ç´„ç‡')
axes[1, 0].tick_params(axis='x', rotation=45)

# ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ¡ãƒ³ãƒãƒ¼åˆ¥è§£ç´„ç‡
churn_by_active = customers_df.groupby('is_active_member')['churned'].mean()
axes[1, 1].bar(['éã‚¢ã‚¯ãƒ†ã‚£ãƒ–', 'ã‚¢ã‚¯ãƒ†ã‚£ãƒ–'], churn_by_active.values, color='gold')
axes[1, 1].set_title('ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ¡ãƒ³ãƒãƒ¼åˆ¥è§£ç´„ç‡')
axes[1, 1].set_xlabel('ã‚¢ã‚¯ãƒ†ã‚£ãƒ–çŠ¶æ…‹')
axes[1, 1].set_ylabel('è§£ç´„ç‡')

# å•†å“æ•°åˆ¥è§£ç´„ç‡
churn_by_products = customers_df.groupby('num_products')['churned'].mean()
axes[1, 2].bar(churn_by_products.index, churn_by_products.values, color='mediumpurple')
axes[1, 2].set_title('å•†å“æ•°åˆ¥è§£ç´„ç‡')
axes[1, 2].set_xlabel('ä¿æœ‰å•†å“æ•°')
axes[1, 2].set_ylabel('è§£ç´„ç‡')

plt.tight_layout()
plt.show()
```

```python
# ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 3: ç›¸é–¢åˆ†æ

# æ•°å€¤å¤‰æ•°ã®ç›¸é–¢è¡Œåˆ—
numeric_cols = ['age', 'income', 'credit_score', 'account_balance', 'years_with_bank', 'num_products']
correlation_matrix = customers_df[numeric_cols + ['churned']].corr()

plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, 
            square=True, linewidths=0.5)
plt.title('ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã®ç›¸é–¢è¡Œåˆ—')
plt.tight_layout()
plt.show()

# è§£ç´„ã«æœ€ã‚‚å½±éŸ¿ã™ã‚‹è¦å› 
churn_correlation = correlation_matrix['churned'].drop('churned').sort_values(key=abs, ascending=False)
print("=== è§£ç´„ã¨ã®ç›¸é–¢ãŒé«˜ã„ç‰¹å¾´é‡ ===")
for feature, corr in churn_correlation.items():
    print(f"{feature}: {corr:.3f}")
```

### 2.2 Feature Store ã®æ§‹ç¯‰

```python
# ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 4: Feature Store æ§‹ç¯‰

def create_feature_group(df, feature_group_name, record_identifier, event_time_feature):
    """
    Feature Group ã‚’ä½œæˆã™ã‚‹é–¢æ•°
    """
    
    # Feature Definition ã®ä½œæˆ
    feature_definitions = []
    
    for column in df.columns:
        if column in [record_identifier, event_time_feature]:
            feature_type = FeatureTypeEnum.STRING
        elif df[column].dtype == 'object':
            feature_type = FeatureTypeEnum.STRING
        elif df[column].dtype in ['int64', 'int32']:
            feature_type = FeatureTypeEnum.INTEGRAL
        else:
            feature_type = FeatureTypeEnum.FRACTIONAL
        
        feature_definitions.append(
            FeatureDefinition(
                feature_name=column, 
                feature_type=feature_type
            )
        )
    
    # Feature Group ã®è¨­å®š
    feature_group = FeatureGroup(
        name=feature_group_name,
        sagemaker_session=sagemaker_session
    )
    
    return feature_group, feature_definitions

# é¡§å®¢ç‰¹å¾´é‡ç”¨ã® Feature Group ä½œæˆ
print("=== Feature Store ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ— ===")

# ã‚¤ãƒ™ãƒ³ãƒˆæ™‚é–“ã®è¿½åŠ ï¼ˆç¾åœ¨æ™‚åˆ»ï¼‰
customers_with_time = customers_df.copy()
customers_with_time['event_time'] = datetime.now().strftime('%Y-%m-%dT%H:%M:%SZ')
customers_with_time['customer_id'] = customers_with_time['customer_id'].astype(str)

# Feature Group ä½œæˆ
customer_feature_group, customer_feature_definitions = create_feature_group(
    customers_with_time,
    'customer-features-lab1',
    'customer_id',
    'event_time'
)

print(f"Feature Groupå: customer-features-lab1")
print(f"ç‰¹å¾´é‡æ•°: {len(customer_feature_definitions)}")
```

```python
# Feature Group ã®å®Ÿéš›ã®ä½œæˆã¨è¨­å®š

try:
    customer_feature_group.create(
        s3_uri=f's3://{bucket_name}/feature-store/customer-features',
        record_identifier_name='customer_id',
        event_time_feature_name='event_time',
        role_arn=role,
        enable_online_store=True,
        feature_definitions=customer_feature_definitions,
        description='é¡§å®¢ã®åŸºæœ¬æƒ…å ±ã¨è¡Œå‹•ãƒ‡ãƒ¼ã‚¿'
    )
    
    print("Feature Group ä½œæˆè¦æ±‚é€ä¿¡å®Œäº†")
    print("ä½œæˆå®Œäº†ã¾ã§ç´„10-15åˆ†ã‹ã‹ã‚Šã¾ã™...")
    
except Exception as e:
    print(f"Feature Group ä½œæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
    if "already exists" in str(e):
        print("Feature Group ã¯æ—¢ã«å­˜åœ¨ã—ã¾ã™")

# ä½œæˆçŠ¶æ³ã®ç¢ºèª
import time

def wait_for_feature_group_creation(feature_group, max_wait_time=900):
    """Feature Group ã®ä½œæˆå®Œäº†ã‚’å¾…æ©Ÿ"""
    start_time = time.time()
    
    while time.time() - start_time < max_wait_time:
        try:
            status = feature_group.describe()['FeatureGroupStatus']
            print(f"Feature Group ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {status}")
            
            if status == 'Created':
                print("âœ… Feature Group ä½œæˆå®Œäº†!")
                return True
            elif status == 'CreateFailed':
                print("âŒ Feature Group ä½œæˆå¤±æ•—")
                return False
                
        except Exception as e:
            print(f"ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ç¢ºèªã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        print("å¾…æ©Ÿä¸­... (30ç§’å¾Œã«å†ç¢ºèª)")
        time.sleep(30)
    
    print("â° ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: Feature Group ä½œæˆã«æ™‚é–“ãŒã‹ã‹ã£ã¦ã„ã¾ã™")
    return False

# ä½œæˆå®Œäº†å¾…æ©Ÿï¼ˆãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§å®Ÿè¡Œã•ã‚Œã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—å¯èƒ½ï¼‰
print("Feature Group ä½œæˆçŠ¶æ³ã‚’ç¢ºèªã—ã¾ã™...")
creation_success = wait_for_feature_group_creation(customer_feature_group)
```

```python
# ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 5: ãƒ‡ãƒ¼ã‚¿ã®å–ã‚Šè¾¼ã¿

if creation_success:
    print("=== Feature Store ãƒ‡ãƒ¼ã‚¿å–ã‚Šè¾¼ã¿ ===")
    
    # ãƒãƒƒãƒã§ã®ãƒ‡ãƒ¼ã‚¿å–ã‚Šè¾¼ã¿
    batch_size = 1000
    total_rows = len(customers_with_time)
    
    for i in range(0, total_rows, batch_size):
        batch_df = customers_with_time.iloc[i:i+batch_size]
        
        try:
            customer_feature_group.ingest(
                data_frame=batch_df,
                max_workers=3,
                wait=True
            )
            
            print(f"ãƒãƒƒãƒ {i//batch_size + 1}/{(total_rows + batch_size - 1)//batch_size} å®Œäº† "
                  f"({len(batch_df)} ä»¶)")
            
        except Exception as e:
            print(f"ãƒãƒƒãƒ {i//batch_size + 1} ã§ã‚¨ãƒ©ãƒ¼: {str(e)}")
            break
    
    print("âœ… å…¨ãƒ‡ãƒ¼ã‚¿ã®å–ã‚Šè¾¼ã¿å®Œäº†!")
    
else:
    print("âš ï¸ Feature Group ã®ä½œæˆãŒå®Œäº†ã—ã¦ã„ãªã„ãŸã‚ã€ãƒ‡ãƒ¼ã‚¿å–ã‚Šè¾¼ã¿ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
    print("Feature Group ä½œæˆå®Œäº†å¾Œã«ã€å†åº¦ãƒ‡ãƒ¼ã‚¿å–ã‚Šè¾¼ã¿ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
```

### 2.3 Feature Store ã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿å–å¾—

```python
# ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 6: Feature Store ã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ†ã‚¹ãƒˆ

from sagemaker.feature_store.feature_store import FeatureStore

def test_feature_store_queries():
    """Feature Store ã®ã‚¯ã‚¨ãƒªæ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ"""
    
    print("=== Feature Store ã‚¯ã‚¨ãƒªãƒ†ã‚¹ãƒˆ ===")
    
    # Athena ã‚¯ã‚¨ãƒªã«ã‚ˆã‚‹ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ã‚¹ãƒˆã‚¢æ¤œç´¢
    feature_store = FeatureStore(sagemaker_session=sagemaker_session)
    
    try:
        # ãƒ†ãƒ¼ãƒ–ãƒ«åå–å¾—
        athena_query = f"""
        SELECT customer_id, age, income, credit_score, churned, event_time
        FROM "sagemaker_featurestore"."customer_features_lab1_{int(time.time())}"
        WHERE age > 40 AND income > 50000
        LIMIT 10
        """
        
        print("Athena ã‚¯ã‚¨ãƒªå®Ÿè¡Œä¸­...")
        query_results = feature_store.athena_query(
            query=athena_query,
            output_location=f's3://{bucket_name}/athena-results/'
        )
        
        print("âœ… Athena ã‚¯ã‚¨ãƒªæˆåŠŸ")
        print("çµæœã®ã‚µãƒ³ãƒ—ãƒ«:")
        print(query_results.head())
        
    except Exception as e:
        print(f"âš ï¸ Athena ã‚¯ã‚¨ãƒªã‚¨ãƒ©ãƒ¼: {str(e)}")
        print("ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ã‚¹ãƒˆã‚¢ã®æº–å‚™ã«æ™‚é–“ãŒã‹ã‹ã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™")
    
    # ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã‚¹ãƒˆã‚¢ã‹ã‚‰ã®ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å–å¾—ãƒ†ã‚¹ãƒˆ
    try:
        print("\n=== ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã‚¹ãƒˆã‚¢å–å¾—ãƒ†ã‚¹ãƒˆ ===")
        
        # ç‰¹å®šã®é¡§å®¢ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        test_customer_id = "1"
        
        online_response = customer_feature_group.get_record(
            record_identifier_value_as_string=test_customer_id
        )
        
        print(f"âœ… é¡§å®¢ID {test_customer_id} ã®ãƒ‡ãƒ¼ã‚¿å–å¾—æˆåŠŸ:")
        for record in online_response.record:
            print(f"  {record.feature_name}: {record.value_as_string}")
            
    except Exception as e:
        print(f"âš ï¸ ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã‚¹ãƒˆã‚¢å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
        print("ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã‚¹ãƒˆã‚¢ã®åŒæœŸã«æ™‚é–“ãŒã‹ã‹ã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™")

# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
if creation_success:
    test_feature_store_queries()
else:
    print("Feature Group ã®ä½œæˆãŒå®Œäº†ã—ã¦ã‹ã‚‰ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
```

## Phase 3: ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

### 3.1 SageMaker Processing ã«ã‚ˆã‚‹å‰å‡¦ç†

```python
# ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 7: SageMaker Processing ã‚¸ãƒ§ãƒ–

from sagemaker.processing import ProcessingInput, ProcessingOutput
from sagemaker.sklearn.processing import SKLearnProcessor
import os

# å‰å‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ä½œæˆ
preprocessing_script = """
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder
from sklearn.model_selection import train_test_split
import argparse
import joblib
import os

def preprocess_data(input_path, output_path):
    '''ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ãƒ¡ã‚¤ãƒ³é–¢æ•°'''
    
    print("=== ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†é–‹å§‹ ===")
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    df = pd.read_csv(os.path.join(input_path, 'customer_churn.csv'))
    print(f"å…ƒãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {df.shape}")
    
    # ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
    print("ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°å®Ÿè¡Œä¸­...")
    
    # 1. æ–°ã—ã„ç‰¹å¾´é‡ã®ä½œæˆ
    df['balance_per_product'] = df['account_balance'] / (df['num_products'] + 1)
    df['age_group'] = pd.cut(df['age'], bins=[0, 30, 40, 50, 60, 100], 
                            labels=['<30', '30-40', '40-50', '50-60', '60+'])
    df['income_category'] = pd.cut(df['income'], 
                                 bins=[0, 30000, 50000, 80000, 150000, float('inf')],
                                 labels=['Low', 'Medium', 'High', 'Very High', 'Premium'])
    
    # 2. ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
    # One-hot encoding for geography
    geography_encoded = pd.get_dummies(df['geography'], prefix='geo')
    df = pd.concat([df, geography_encoded], axis=1)
    df.drop('geography', axis=1, inplace=True)
    
    # Age group encoding
    age_group_encoded = pd.get_dummies(df['age_group'], prefix='age_group')
    df = pd.concat([df, age_group_encoded], axis=1)
    df.drop('age_group', axis=1, inplace=True)
    
    # Income category encoding
    income_cat_encoded = pd.get_dummies(df['income_category'], prefix='income_cat')
    df = pd.concat([df, income_cat_encoded], axis=1)
    df.drop('income_category', axis=1, inplace=True)
    
    # 3. æ•°å€¤ç‰¹å¾´é‡ã®æ­£è¦åŒ–
    numeric_features = ['age', 'income', 'credit_score', 'account_balance', 
                       'years_with_bank', 'balance_per_product']
    
    scaler = StandardScaler()
    df[numeric_features] = scaler.fit_transform(df[numeric_features])
    
    # 4. ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®åˆ†é›¢
    X = df.drop(['customer_id', 'churned'], axis=1)
    y = df['churned']
    
    # 5. è¨“ç·´ãƒ»æ¤œè¨¼ãƒ»ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆåˆ†å‰²
    X_temp, X_test, y_temp, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    X_train, X_val, y_train, y_val = train_test_split(
        X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp
    )
    
    print(f"è¨“ç·´ã‚»ãƒƒãƒˆ: {X_train.shape[0]} ä»¶")
    print(f"æ¤œè¨¼ã‚»ãƒƒãƒˆ: {X_val.shape[0]} ä»¶") 
    print(f"ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆ: {X_test.shape[0]} ä»¶")
    
    # 6. ãƒ‡ãƒ¼ã‚¿ä¿å­˜
    os.makedirs(output_path, exist_ok=True)
    
    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿
    train_data = pd.concat([X_train, y_train], axis=1)
    train_data.to_csv(os.path.join(output_path, 'train.csv'), index=False)
    
    # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿
    val_data = pd.concat([X_val, y_val], axis=1)
    val_data.to_csv(os.path.join(output_path, 'validation.csv'), index=False)
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
    test_data = pd.concat([X_test, y_test], axis=1)
    test_data.to_csv(os.path.join(output_path, 'test.csv'), index=False)
    
    # ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã®ä¿å­˜
    joblib.dump(scaler, os.path.join(output_path, 'scaler.pkl'))
    
    # ç‰¹å¾´é‡åã®ä¿å­˜
    feature_names = list(X.columns)
    with open(os.path.join(output_path, 'feature_names.txt'), 'w') as f:
        for name in feature_names:
            f.write(f"{name}\\n")
    
    print("=== ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†å®Œäº† ===")
    print(f"ç‰¹å¾´é‡æ•°: {len(feature_names)}")
    print("ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«:")
    print("- train.csv: è¨“ç·´ãƒ‡ãƒ¼ã‚¿")
    print("- validation.csv: æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿") 
    print("- test.csv: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿")
    print("- scaler.pkl: æ¨™æº–åŒ–ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼")
    print("- feature_names.txt: ç‰¹å¾´é‡åãƒªã‚¹ãƒˆ")

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--input-path', type=str, default='/opt/ml/processing/input')
    parser.add_argument('--output-path', type=str, default='/opt/ml/processing/output')
    
    args = parser.parse_args()
    preprocess_data(args.input_path, args.output_path)
"""

# ã‚¹ã‚¯ãƒªãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
with open('preprocessing.py', 'w') as f:
    f.write(preprocessing_script)

print("å‰å‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆä½œæˆå®Œäº†")
```

```python
# SageMaker Processing ã‚¸ãƒ§ãƒ–ã®å®Ÿè¡Œ

# SKLearn Processor ã®è¨­å®š
sklearn_processor = SKLearnProcessor(
    framework_version='1.0-1',
    instance_type='ml.m5.xlarge',
    instance_count=1,
    base_job_name='customer-churn-preprocessing',
    role=role,
    sagemaker_session=sagemaker_session
)

print("=== SageMaker Processing ã‚¸ãƒ§ãƒ–å®Ÿè¡Œ ===")

# Processing ã‚¸ãƒ§ãƒ–å®Ÿè¡Œ
try:
    sklearn_processor.run(
        code='preprocessing.py',
        inputs=[
            ProcessingInput(
                source=f's3://{bucket_name}/datasets/customers/',
                destination='/opt/ml/processing/input',
                input_name='customer-data'
            )
        ],
        outputs=[
            ProcessingOutput(
                source='/opt/ml/processing/output',
                destination=f's3://{bucket_name}/processed-data/customer-churn/',
                output_name='processed-data'
            )
        ],
        arguments=[
            '--input-path', '/opt/ml/processing/input',
            '--output-path', '/opt/ml/processing/output'
        ]
    )
    
    print("âœ… Processing ã‚¸ãƒ§ãƒ–é€ä¿¡å®Œäº†")
    print(f"ã‚¸ãƒ§ãƒ–å: {sklearn_processor.latest_job.job_name}")
    print("å‡¦ç†å®Œäº†ã¾ã§ç´„10-15åˆ†ã‹ã‹ã‚Šã¾ã™...")
    
except Exception as e:
    print(f"âŒ Processing ã‚¸ãƒ§ãƒ–ã‚¨ãƒ©ãƒ¼: {str(e)}")
```

## ğŸ“Š ãƒ©ãƒœã®æ¤œè¨¼ã¨ã¾ã¨ã‚

### æ¤œè¨¼æ‰‹é †

```python
# ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 8: ãƒ©ãƒœçµæœã®æ¤œè¨¼

def verify_lab_completion():
    """ãƒ©ãƒœå®Œäº†ã®æ¤œè¨¼"""
    
    print("=== Lab 1 å®Œäº†æ¤œè¨¼ ===")
    
    verification_results = {
        's3_bucket': False,
        'sample_data': False,
        'feature_store': False,
        'processing_job': False
    }
    
    # 1. S3ãƒã‚±ãƒƒãƒˆã¨ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª
    try:
        response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix='datasets/')
        if response.get('Contents'):
            verification_results['s3_bucket'] = True
            verification_results['sample_data'] = True
            print("âœ… S3ãƒã‚±ãƒƒãƒˆã¨ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿: æ­£å¸¸")
        else:
            print("âŒ S3ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    except Exception as e:
        print(f"âŒ S3ç¢ºèªã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    # 2. Feature Store ã®ç¢ºèª
    try:
        feature_group_status = customer_feature_group.describe()
        if feature_group_status['FeatureGroupStatus'] == 'Created':
            verification_results['feature_store'] = True
            print("âœ… Feature Store: æ­£å¸¸")
        else:
            print(f"âš ï¸ Feature Store ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {feature_group_status['FeatureGroupStatus']}")
    except Exception as e:
        print(f"âŒ Feature Storeç¢ºèªã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    # 3. Processing ã‚¸ãƒ§ãƒ–ã®ç¢ºèª
    try:
        processing_job_name = sklearn_processor.latest_job.job_name
        processing_status = sklearn_processor.latest_job.describe()
        
        if processing_status['ProcessingJobStatus'] == 'Completed':
            verification_results['processing_job'] = True
            print("âœ… Processing ã‚¸ãƒ§ãƒ–: å®Œäº†")
        else:
            print(f"âš ï¸ Processing ã‚¸ãƒ§ãƒ– ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {processing_status['ProcessingJobStatus']}")
    except Exception as e:
        print(f"âŒ Processing ã‚¸ãƒ§ãƒ–ç¢ºèªã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    # çµæœã‚µãƒãƒªãƒ¼
    completed_tasks = sum(verification_results.values())
    total_tasks = len(verification_results)
    
    print(f"\n=== å®Œäº†çŠ¶æ³: {completed_tasks}/{total_tasks} ===")
    
    if completed_tasks == total_tasks:
        print("ğŸ‰ Lab 1 å®Œå…¨å®Œäº†!")
        print("æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—: Lab 2 - ãƒ¢ãƒ‡ãƒ«é–‹ç™ºã«é€²ã‚“ã§ãã ã•ã„")
    else:
        print("âš ï¸ ä¸€éƒ¨ã®ã‚¿ã‚¹ã‚¯ãŒæœªå®Œäº†ã§ã™")
        print("æœªå®Œäº†ã®ã‚¿ã‚¹ã‚¯ã‚’ç¢ºèªã—ã¦å†å®Ÿè¡Œã—ã¦ãã ã•ã„")
    
    return verification_results

# æ¤œè¨¼å®Ÿè¡Œ
verification_results = verify_lab_completion()
```

### ã‚³ã‚¹ãƒˆ ã‚µãƒãƒªãƒ¼

```python
# ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 9: ã‚³ã‚¹ãƒˆã‚µãƒãƒªãƒ¼ã¨æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

def generate_cost_summary():
    """ã‚³ã‚¹ãƒˆæ¦‚ç®—ã®è¡¨ç¤º"""
    
    print("=== Lab 1 ã‚³ã‚¹ãƒˆã‚µãƒãƒªãƒ¼ ===")
    
    costs = {
        'SageMaker Studio': '$0 (ä½¿ç”¨æ™‚ã®ã¿èª²é‡‘)',
        'S3 ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸': '$0.10-0.50 (ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºæ¬¡ç¬¬)',
        'Feature Store': '$0.05-0.20 (ãƒ‡ãƒ¼ã‚¿å–ã‚Šè¾¼ã¿)',
        'Processing ã‚¸ãƒ§ãƒ–': '$8-12 (ml.m5.xlarge Ã— å®Ÿè¡Œæ™‚é–“)',
        'Athena ã‚¯ã‚¨ãƒª': '$0.01-0.05 (ã‚¹ã‚­ãƒ£ãƒ³ãƒ‡ãƒ¼ã‚¿é‡æ¬¡ç¬¬)'
    }
    
    total_min = 8.16
    total_max = 12.75
    
    for service, cost in costs.items():
        print(f"  {service}: {cost}")
    
    print(f"\nç·è¨ˆæ¦‚ç®—: ${total_min:.2f} - ${total_max:.2f}")
    print("æ³¨æ„: å®Ÿéš›ã®ã‚³ã‚¹ãƒˆã¯ä½¿ç”¨æ™‚é–“ã¨ãƒªã‚½ãƒ¼ã‚¹ã‚µã‚¤ã‚ºã«ä¾å­˜ã—ã¾ã™")

generate_cost_summary()

print("\n=== æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ— ===")
print("Lab 1 å®Œäº†å¾Œã¯ä»¥ä¸‹ã®ãƒ©ãƒœã«é€²ã‚“ã§ãã ã•ã„:")
print("1. Lab 2: SageMaker ã§ã®ãƒ¢ãƒ‡ãƒ«é–‹ç™º")
print("2. Lab 3: ãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ")
print("3. Lab 4: MLOps ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰")
print("4. Lab 5: ãƒ¢ãƒ‡ãƒ«ç›£è¦–ã¨ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹")
```

## ğŸ§¹ ãƒªã‚½ãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—

æœ€å¾Œã«ã€ã‚³ã‚¹ãƒˆå‰Šæ¸›ã®ãŸã‚ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚‚æä¾›ã—ã¾ã™ï¼š

```bash
#!/bin/bash
# ã‚¹ã‚¯ãƒªãƒ—ãƒˆ: cleanup-lab1.sh

echo "=== Lab 1 ãƒªã‚½ãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—é–‹å§‹ ==="

# è¨­å®šèª­ã¿è¾¼ã¿
if [ -f "sagemaker-config.json" ]; then
    DOMAIN_ID=$(jq -r '.domain_id' sagemaker-config.json)
    USER_PROFILE_NAME=$(jq -r '.user_profile_name' sagemaker-config.json)
    EXECUTION_ROLE_NAME=$(jq -r '.execution_role_arn' sagemaker-config.json | sed 's/.*role\///')
    REGION=$(jq -r '.region' sagemaker-config.json)
else
    echo "è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚æ‰‹å‹•ã§ãƒªã‚½ãƒ¼ã‚¹ã‚’å‰Šé™¤ã—ã¦ãã ã•ã„ã€‚"
    exit 1
fi

# Feature Store å‰Šé™¤ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
read -p "Feature Store ã‚’å‰Šé™¤ã—ã¾ã™ã‹ï¼Ÿ (y/N): " delete_fs
if [ "$delete_fs" = "y" ] || [ "$delete_fs" = "Y" ]; then
    echo "Feature Store å‰Šé™¤ä¸­..."
    aws sagemaker delete-feature-group \
        --feature-group-name customer-features-lab1 \
        --region $REGION 2>/dev/null || echo "Feature Store ã¯æ—¢ã«å‰Šé™¤ã•ã‚Œã¦ã„ã¾ã™"
fi

# S3 ãƒã‚±ãƒƒãƒˆå‰Šé™¤ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
if [ -f "data_catalog.json" ]; then
    BUCKET_NAME=$(jq -r '.bucket_name' data_catalog.json)
    read -p "S3 ãƒã‚±ãƒƒãƒˆ '$BUCKET_NAME' ã‚’å‰Šé™¤ã—ã¾ã™ã‹ï¼Ÿ (y/N): " delete_s3
    if [ "$delete_s3" = "y" ] || [ "$delete_s3" = "Y" ]; then
        echo "S3 ãƒã‚±ãƒƒãƒˆå‰Šé™¤ä¸­..."
        aws s3 rm s3://$BUCKET_NAME --recursive
        aws s3 rb s3://$BUCKET_NAME
    fi
fi

# SageMaker Studio ãƒªã‚½ãƒ¼ã‚¹å‰Šé™¤ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
read -p "SageMaker Studio ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’å‰Šé™¤ã—ã¾ã™ã‹ï¼Ÿ (y/N): " delete_studio
if [ "$delete_studio" = "y" ] || [ "$delete_studio" = "Y" ]; then
    echo "SageMaker Studio ãƒ‰ãƒ¡ã‚¤ãƒ³å‰Šé™¤ä¸­..."
    
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
    aws sagemaker delete-user-profile \
        --domain-id $DOMAIN_ID \
        --user-profile-name $USER_PROFILE_NAME \
        --region $REGION
    
    # ãƒ‰ãƒ¡ã‚¤ãƒ³å‰Šé™¤
    aws sagemaker delete-domain \
        --domain-id $DOMAIN_ID \
        --region $REGION
fi

echo "=== ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº† ==="
echo "æ³¨æ„: ä¸€éƒ¨ã®ãƒªã‚½ãƒ¼ã‚¹å‰Šé™¤ã«ã¯æ™‚é–“ãŒã‹ã‹ã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™"
```

ã“ã®Lab 1ã§ã¯ã€SageMakerã®åŸºæœ¬çš„ãªæ“ä½œã‹ã‚‰Feature Storeã®æ§‹ç¯‰ã€ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¾ã§ã‚’å®Ÿè·µçš„ã«å­¦ç¿’ã§ãã¾ã™ã€‚æ¬¡ã®Lab 2ã§ã¯ã€ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã®é–‹ç™ºã‚’è¡Œã„ã¾ã™ã€‚

<function_calls>
<invoke name="TodoWrite">
<parameter name="todos">[{"id": "19", "content": "MLE-A\u8a66\u9a13\u306e\u6700\u65b0\u60c5\u5831\u3068\u51fa\u984c\u7bc4\u56f2\u3092\u8abf\u67fb", "status": "completed", "priority": "high"}, {"id": "20", "content": "AWS ML\u30b5\u30fc\u30d3\u30b9\u306e\u6700\u65b0\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3092MCP\u30b5\u30fc\u30d0\u3067\u53c2\u7167", "status": "completed", "priority": "high"}, {"id": "21", "content": "ML\u30a8\u30f3\u30b8\u30cb\u30a2\u5411\u3051\u30cf\u30f3\u30ba\u30aa\u30f3\u6559\u6750\u306e\u69cb\u6210\u8a2d\u8a08", "status": "completed", "priority": "high"}, {"id": "22", "content": "\u5b9f\u8df5\u7684\u306aML\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u30e9\u30dc\u306e\u4f5c\u6210", "status": "in_progress", "priority": "medium"}, {"id": "23", "content": "SageMaker\u3068\u95a2\u9023\u30b5\u30fc\u30d3\u30b9\u306e\u5b9f\u8df5\u30e9\u30dc", "status": "pending", "priority": "medium"}, {"id": "24", "content": "MLOps\u3068\u30e2\u30c7\u30eb\u30c7\u30d7\u30ed\u30a4\u306e\u30e9\u30dc", "status": "pending", "priority": "medium"}, {"id": "25", "content": "MLE-A\u8a66\u9a13\u60f3\u5b9a\u554f\u984c\u96c6\u306e\u4f5c\u6210", "status": "pending", "priority": "medium"}, {"id": "26", "content": "\u6559\u6750\u306e\u691c\u8a3c\u3068\u6700\u7d42\u8abf\u6574", "status": "pending", "priority": "low"}, {"id": "27", "content": "DevOps Pro\u8a66\u9a13\u306e\u6700\u65b0\u60c5\u5831\u3068\u51fa\u984c\u7bc4\u56f2\u3092\u8abf\u67fb", "status": "pending", "priority": "medium"}, {"id": "28", "content": "DevOps\u5411\u3051\u30cf\u30f3\u30ba\u30aa\u30f3\u6559\u6750\u306e\u4f5c\u6210", "status": "pending", "priority": "medium"}, {"id": "29", "content": "\u6559\u6750\u7528\u30d5\u30a9\u30eb\u30c0\u69cb\u9020\u306e\u6574\u7406\u3068\u4f5c\u6210", "status": "completed", "priority": "high"}]